## pg_dump 并行备份failed 的原因分析 - This usually means that someone requested an ACCESS EXCLUSIVE lock on the table after the pg_dump parent process had gotten the initial ACCESS SHARE lock on the table   
                                                                                                      
### 作者                                                                                                      
digoal                                                                                                      
                                                                                                      
### 日期                                                                                                      
2019-04-17                                                                                                      
                                                                                                      
### 标签                                                                                                      
PostgreSQL , pg_dump , 并行备份 , shared lock , master process , worker process , access exclusive lock , shared lock , nowait      
                     
----                                                                                                
                                                                                                  
## 背景   
  
启动并行逻辑备份，parent进行对所有要备份的对象加shared lock，worker processes开始挨个目标表（备份前，worker process copy目标表前，先对表加shared lock nowait，防止死锁的问题），如果worker process获得shared lock失败，导致整个pg_dump失败。  
  
```  
-j njobs  
--jobs=njobs  
    Run the dump in parallel by dumping njobs tables simultaneously. This option reduces the time of the dump but it also increases the load on the database server. You can only use this option with the directory output  
    format because this is the only output format where multiple processes can write their data at the same time.  
  
    pg_dump will open njobs + 1 connections to the database, so make sure your max_connections setting is high enough to accommodate all connections.  
  
注意这一段：  
  
    Requesting exclusive locks on database objects while running a parallel dump could cause the dump to fail.   
    The reason is that the pg_dump master process requests shared locks on the objects that the worker processes are  
    going to dump later in order to make sure that nobody deletes them and makes them go away while the dump is running.   
    If another client then requests an exclusive lock on a table, that lock will not be granted but will be  
    queued waiting for the shared lock of the master process to be released. Consequently any other access   
    to the table will not be granted either and will queue after the exclusive lock request. This includes the worker  
    process trying to dump the table. Without any precautions this would be a classic deadlock situation.   
    To detect this conflict, the pg_dump worker process requests another shared lock using the NOWAIT option.   
    If the  
    worker process is not granted this shared lock, somebody else must have requested an exclusive lock in the   
    meantime and there is no way to continue with the dump, so pg_dump has no choice but to abort the dump.  
  
    For a consistent backup, the database server needs to support synchronized snapshots, a feature that was introduced in PostgreSQL 9.2 for primary servers and 10 for standbys. With this feature, database clients can  
    ensure they see the same data set even though they use different connections.  pg_dump -j uses multiple database connections; it connects to the database once with the master process and once again for each worker job.  
    Without the synchronized snapshot feature, the different worker jobs wouldn't be guaranteed to see the same data in each connection, which could lead to an inconsistent backup.  
  
    If you want to run a parallel dump of a pre-9.2 server, you need to make sure that the database content doesn't change from between the time the master connects to the database until the last worker job has connected to  
    the database. The easiest way to do this is to halt any data modifying processes (DDL and DML) accessing the database before starting the backup. You also need to specify the --no-synchronized-snapshots parameter when  
    running pg_dump -j against a pre-9.2 PostgreSQL server.  
```  
  
## 复现  
1、创建一堆测试表  
  
2、写入测试数据  
  
3、并行备份，master (parent)进程对所有要备份的目标对象加shared lock。    
  
```  
pg_dump -F d -j 2 -f /data01/digoal/test.tar  
```  
  
4、对还没有备份的表加排他锁  
  
```  
postgres=#        begin;  
BEGIN  
postgres=# lock table tmp1 in access exclusive mode ;  
由于这个表已经被parent加了shared lock，所以请求access exclusive锁会进入等待中  
  
这个等待中的锁也在队列中，会堵塞后面work process备份这个表之前请求的shared lock nowait.  
```  
  
5、备份的worker process对这个表加shared lock nowait时，报错  
  
整个备份FAILED  
  
```  
digoal@pg11-test-> pg_dump -F d -j 2 -f /data01/digoal/test.tar  
pg_dump: [parallel archiver] could not obtain lock on relation "public.tmp1"  
This usually means that someone requested an ACCESS EXCLUSIVE lock on the table after the pg_dump parent process had gotten the initial ACCESS SHARE lock on the table.  
pg_dump: [parallel archiver] a worker process died unexpectedly  
```  
  
6、等待中的access lock申请成功  
  
```  
postgres=#        begin;  
BEGIN  
postgres=# lock table tmp1 in access exclusive mode ;  
LOCK TABLE  
```  
  
  
## 解决问题  
明确知道问题在哪里，就很好解决。方法任选：  
  
1、在备份期间不要执行DDL  
  
2、在备份期间，设置所有用户请求的lock_timeout。  
     
## 参考  
man pg_dump  
  
         
  
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"  ><img src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"  alt="Flag Counter"  border="0"  ></a>  
  
  
## [digoal's 大量PostgreSQL文章入口](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
## [免费领取阿里云RDS PostgreSQL实例、ECS虚拟机](https://free.aliyun.com/ "57258f76c37864c6e6d23383d05714ea")
  
