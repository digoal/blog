## PostgreSQL 14 GetSnapshotData 高并发优化
    
### 作者    
digoal    
    
### 日期    
2020-08-12    
    
### 标签    
PostgreSQL , GetSnapshotData , 高并发 
    
----    
    
## 背景    
PostgreSQL将通过优化GetSnapshotData, 在连接数超过5000时依旧可以保持超过100万tps.    
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=1f51c17c68d05c28d5b9294d8013cb9e7e653160  
  
https://www.citusdata.com/blog/2020/10/25/improving-postgres-connection-scalability-snapshots/  
  
```
snapshot scalability: Move PGXACT->xmin back to PGPROC.
author	Andres Freund <andres@anarazel.de>	
Fri, 14 Aug 2020 07:25:21 +0800 (16:25 -0700)
committer	Andres Freund <andres@anarazel.de>	
Fri, 14 Aug 2020 07:25:21 +0800 (16:25 -0700)
commit	1f51c17c68d05c28d5b9294d8013cb9e7e653160
tree	bb2dc0ab3009ced3ccfe819aba406f1f5810d4e4	tree | snapshot
parent	a811ea5bde2fbf450095994b5726dcbf64d68668	commit | diff
snapshot scalability: Move PGXACT->xmin back to PGPROC.

Now that xmin isn't needed for GetSnapshotData() anymore, it leads to
unnecessary cacheline ping-pong to have it in PGXACT, as it is updated
considerably more frequently than the other PGXACT members.

After the changes in dc7420c2c92, this is a very straight-forward change.

For highly concurrent, snapshot acquisition heavy, workloads this change alone
can significantly increase scalability. E.g. plain pgbench on a smaller 2
socket machine gains 1.07x for read-only pgbench, 1.22x for read-only pgbench
when submitting queries in batches of 100, and 2.85x for batches of 100
'SELECT';.  The latter numbers are obviously not to be expected in the
real-world, but micro-benchmark the snapshot computation
scalability (previously spending ~80% of the time in GetSnapshotData()).

Author: Andres Freund <andres@anarazel.de>
Reviewed-By: Robert Haas <robertmhaas@gmail.com>
Reviewed-By: Thomas Munro <thomas.munro@gmail.com>
Reviewed-By: David Rowley <dgrowleyml@gmail.com>
Discussion: https://postgr.es/m/20200301083601.ews6hz5dduc3w2se@alap3.anarazel.de
```
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=dc7420c2c9274a283779ec19718d2d16323640c0   
  
```
snapshot scalability: Don't compute global horizons while building snapshots.
author	Andres Freund <andres@anarazel.de>	
Thu, 13 Aug 2020 07:03:49 +0800 (16:03 -0700)
committer	Andres Freund <andres@anarazel.de>	
Thu, 13 Aug 2020 07:03:49 +0800 (16:03 -0700)
commit	dc7420c2c9274a283779ec19718d2d16323640c0
tree	1ec40b9eebbf7913780ac6a7d6193605c25f1aa2	tree | snapshot
parent	1f42d35a1d6144a23602b2c0bc7f97f3046cf890	commit | diff
snapshot scalability: Don't compute global horizons while building snapshots.

To make GetSnapshotData() more scalable, it cannot not look at at each proc's
xmin: While snapshot contents do not need to change whenever a read-only
transaction commits or a snapshot is released, a proc's xmin is modified in
those cases. The frequency of xmin modifications leads to, particularly on
higher core count systems, many cache misses inside GetSnapshotData(), despite
the data underlying a snapshot not changing. That is the most
significant source of GetSnapshotData() scaling poorly on larger systems.

Without accessing xmins, GetSnapshotData() cannot calculate accurate horizons /
thresholds as it has so far. But we don't really have to: The horizons don't
actually change that much between GetSnapshotData() calls. Nor are the horizons
actually used every time a snapshot is built.

The trick this commit introduces is to delay computation of accurate horizons
until there use and using horizon boundaries to determine whether accurate
horizons need to be computed.

The use of RecentGlobal[Data]Xmin to decide whether a row version could be
removed has been replaces with new GlobalVisTest* functions.  These use two
thresholds to determine whether a row can be pruned:
1) definitely_needed, indicating that rows deleted by XIDs >= definitely_needed
   are definitely still visible.
2) maybe_needed, indicating that rows deleted by XIDs < maybe_needed can
   definitely be removed
GetSnapshotData() updates definitely_needed to be the xmin of the computed
snapshot.

When testing whether a row can be removed (with GlobalVisTestIsRemovableXid())
and the tested XID falls in between the two (i.e. XID >= maybe_needed && XID <
definitely_needed) the boundaries can be recomputed to be more accurate. As it
is not cheap to compute accurate boundaries, we limit the number of times that
happens in short succession.  As the boundaries used by
GlobalVisTestIsRemovableXid() are never reset (with maybe_needed updated by
GetSnapshotData()), it is likely that further test can benefit from an earlier
computation of accurate horizons.

To avoid regressing performance when old_snapshot_threshold is set (as that
requires an accurate horizon to be computed), heap_page_prune_opt() doesn't
unconditionally call TransactionIdLimitedForOldSnapshots() anymore. Both the
computation of the limited horizon, and the triggering of errors (with
SetOldSnapshotThresholdTimestamp()) is now only done when necessary to remove
tuples.

This commit just removes the accesses to PGXACT->xmin from
GetSnapshotData(), but other members of PGXACT residing in the same
cache line are accessed. Therefore this in itself does not result in a
significant improvement. Subsequent commits will take advantage of the
fact that GetSnapshotData() now does not need to access xmins anymore.

Note: This contains a workaround in heap_page_prune_opt() to keep the
snapshot_too_old tests working. While that workaround is ugly, the tests
currently are not meaningful, and it seems best to address them separately.

Author: Andres Freund <andres@anarazel.de>
Reviewed-By: Robert Haas <robertmhaas@gmail.com>
Reviewed-By: Thomas Munro <thomas.munro@gmail.com>
Reviewed-By: David Rowley <dgrowleyml@gmail.com>
Discussion: https://postgr.es/m/20200301083601.ews6hz5dduc3w2se@alap3.anarazel.de
```
    
```    
Track latest completed xid as a FullTransactionId. master github/master    
author	Andres Freund <andres@anarazel.de>	    
Wed, 12 Aug 2020 08:41:18 +0800 (17:41 -0700)    
committer	Andres Freund <andres@anarazel.de>	    
Wed, 12 Aug 2020 08:41:18 +0800 (17:41 -0700)    
commit	3bd7f9969a240827bc2effa399170b7565238fd2    
tree	c3a1db34b0730dfda425c98ce4ef18d71f953108	tree | snapshot    
parent	fea10a64340e529805609126740a540c8f9daab4	commit | diff    
Track latest completed xid as a FullTransactionId.    
    
The reason for doing so is that a subsequent commit will need that to    
avoid wraparound issues. As the subsequent change is large this was    
split out for easier review.    
    
The reason this is not a perfect straight-forward change is that we do    
not want track 64bit xids in the procarray or the WAL. Therefore we    
need to advance lastestCompletedXid in relation to 32 bit xids. The    
code for that is now centralized in MaintainLatestCompletedXid*.    
    
Author: Andres Freund    
Reviewed-By: Thomas Munro, Robert Haas, David Rowley    
Discussion: https://postgr.es/m/20200301083601.ews6hz5dduc3w2se@alap3.anarazel.de    
```    
    
https://www.postgresql.org/message-id/20200301083601.ews6hz5dduc3w2se@alap3.anarazel.de    
    
https://github.com/anarazel/postgres/tree/pgxact-split    
    
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=3bd7f9969a240827bc2effa399170b7565238fd2    
    
```    
Hi,    
    
    
    
I think postgres' issues with scaling to larger numbers of connections    
is a serious problem in the field. While poolers can address some of    
that, given the issues around prepared statements, transaction state,    
etc, I don't think that's sufficient in many cases. It also adds    
latency.    
    
    
    
Nor do I think the argument that one shouldn't have more than a few    
dozen connection holds particularly much water. As clients have think    
time, and database results have to be sent/received (most clients don't    
use pipelining), and as many applications have many application servers    
with individual connection pools, it's very common to need more    
connections than postgres can easily deal with.    
    
    
    
The largest reason for that is GetSnapshotData(). It scales poorly to    
larger connection counts. Part of that is obviously it's O(connections)    
nature, but I always thought it had to be more.  I've seen production    
workloads spending > 98% of the cpu time n GetSnapshotData().    
    
    
    
After a lot of analysis and experimentation I figured out that the    
primary reason for this is PGXACT->xmin. Even the simplest transaction    
modifies MyPgXact->xmin several times during its lifetime (IIRC twice    
(snapshot & release) for exec_bind_message(), same for    
exec_exec_message(), then again as part of EOXact processing). Which    
means that a backend doing GetSnapshotData() on a system with a number    
of other connections active, is very likely to hit PGXACT cachelines    
that are owned by another cpu / set of cpus / socket. The larger the    
system is, the worse the consequences of this are.    
    
    
    
This problem is most prominent (and harder to fix) for xmin, but also    
exists for the other fields in PGXACT. We rarely have xid, nxids,    
overflow, or vacuumFlags set, yet constantly set them, leading to    
cross-node traffic.    
    
    
    
The second biggest problem is that the indirection through pgprocnos    
that GetSnapshotData() has to do to go through to get each backend's    
xmin is very unfriendly for a pipelined CPU (i.e. all that postgres runs    
on). There's basically a stall at the end of every loop iteration -    
which is exascerbated by there being so many cache misses.    
    
    
    
It's fairly easy to avoid unnecessarily dirtying cachelines for all the    
PGXACT fields except xmin. Because that actually needs to be visible to    
other backends.    
    
    
    
While it sounds almost trivial in hindsight, it took me a long while to    
grasp a solution to a big part of this problem: We don't actually need    
to look at PGXACT->xmin to compute a snapshot. The only reason that    
GetSnapshotData() does so, is because it also computes    
RecentGlobal[Data]Xmin.    
    
    
    
But we don't actually need them all that frequently. They're primarily    
used as a horizons for heap_page_prune_opt() etc. But for one, while    
pruning is really important, it doesn't happen *all* the time. But more    
importantly a RecentGlobalXmin from an earlier transaction is actually    
sufficient for most pruning requests, especially when there is a larger    
percentage of reading than updating transaction (very common).    
    
    
    
By having GetSnapshotData() compute an accurate upper bound after which    
we are certain not to be able to prune (basically the transaction's    
xmin, slots horizons, etc), and a conservative lower bound below which    
we are definitely able to prune, we can allow some pruning actions to    
happen. If a pruning request (or something similar) encounters an xid    
between those, an accurate lower bound can be computed.    
    
    
    
That allows to avoid looking at PGXACT->xmin.    
    
    
    
To address the second big problem (the indirection), we can instead pack    
the contents of PGXACT tightly, just like we do for pgprocnos. In the    
attached series, I introduced separate arrays for xids, vacuumFlags,    
nsubxids.    
    
    
    
The reason for splitting them is that they change at different rates,    
and different sizes.  In a read-mostly workload, most backends are not    
going to have an xid, therefore making the xids array almost    
constant. As long as all xids are unassigned, GetSnapshotData() doesn't    
need to look at anything else, therefore making it sensible to check the    
xid first.    
    
    
    
Here are some numbers for the submitted patch series. I'd to cull some    
further improvements to make it more manageable, but I think the numbers    
still are quite convincing.    
    
The workload is a pgbench readonly, with pgbench -M prepared -c $conns    
-j $conns -S -n for each client count.  This is on a machine with 2    
Intel(R) Xeon(R) Platinum 8168, but virtualized.    
     
conns   tps master		tps pgxact-split    
    
1       26842.492845            26524.194821    
10      246923.158682           249224.782661    
50      695956.539704           709833.746374    
100     1054727.043139          1903616.306028    
200     964795.282957           1949200.338012    
300     906029.377539           1927881.231478    
400     845696.690912           1911065.369776    
500     812295.222497           1926237.255856    
600     888030.104213           1903047.236273    
700     866896.532490           1886537.202142    
800     863407.341506           1883768.592610    
900     871386.608563           1874638.012128    
1000    887668.277133           1876402.391502    
1500    860051.361395           1815103.564241    
2000    890900.098657           1775435.271018    
3000    874184.980039           1653953.817997    
4000    845023.080703           1582582.316043    
5000    817100.195728           1512260.802371    
```    
    
    
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
  
  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
