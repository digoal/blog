## DB吐槽大会,第46期 - PG copy不能跳过错误行  
  
### 作者  
digoal  
  
### 日期  
2021-09-15  
  
### 标签  
PostgreSQL , copy   
  
----  
  
## 背景  
[视频回放](https://www.bilibili.com/video/BV1mL41137yU/)  
  
1、产品的问题点  
- PG copy不能跳过错误行  
  
2、问题点背后涉及的技术原理  
- PG 支持copy语法, 由于其高效的写入性能(功能单一, 协议精简, 节省了冗长的query parser,rewrite,plan,execute过程), 通常被用于高速数据导入导出.  
- PG copy被作为1个事务来处理, 如果一次copy过程中出现了异常的行, 将导致整个copy任务回滚.  
  
3、这个问题将影响哪些行业以及业务场景  
- 导入、迁移、恢复数据等场景  
  
4、会导致什么问题?  
- 如果一个copy的表数据量很多, 已经导入了很多数据, 然后发现异常数据, 导致整个copy任务回滚, 浪费时间, 同时产生的写操作、wal都是浪费的. 必须重来一遍.  
  
5、业务上应该如何避免这个坑  
- 使用pg_bulkload插件以及对应的导入工具, 支持多项配置, 甚至能直接写数据文件, 跳过wal, shared buffer等, 效率比普通copy还要高.       
    - http://ossc-db.github.io/pg_bulkload/pg_bulkload.html  
    - https://github.com/ossc-db/pg_bulkload/  
  
6、业务上避免这个坑牺牲了什么, 会引入什么新的问题  
- 使用pg_bulkload需要注意, 如果选择了direct模式导入, 会绕过wal, 那么以前的备份就无法使用wal来恢复新导入的数据. 必须在bulk load后做一次全量备份. 同时如果有基于流复制的逻辑订阅、物理standby, 统统都无法接受这些bulk load写入的数据.  
- pg_bulkload属于第三方工具  
  
7、数据库未来产品迭代如何修复这个坑  
- 希望PG自身能支持copy是跳过并记录下异常的行到日志、指定文件、或表里面, 导入结束后进行修复.   
    - 例如, 可以设置批量大小, 批量提交, 遇到异常时异常批次的数据跳过异常重新写入一遍. 也比全部回滚好.   
    - 例如, 可以设置错误条数上限, 如果超过上限则回退整个copy事务.  
  
  
  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
  
  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
