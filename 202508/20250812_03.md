## PolarDB存算分离共享存储 : Ceph SSD+机械盘 2层存储 典型配置实践       
                                      
### 作者                                      
digoal                                      
                                      
### 日期                                      
2025-08-12                                      
                                      
### 标签                                      
PostgreSQL , PolarDB , DuckDB , 共享存储 , 块设备 , pfs , direct io , ceph , 2层存储架构 , ssd , 机械盘 , 缓存层 , 存储层 , bcache , flashcache , zfs l2arc      
                                      
----                                      
                                      
## 背景  
没有SAN存储, 一样可以玩转存算分离数据库PolarDB.   
  
感谢Ceph提供分布式共享块设备.  
  
为了提升读写性能, Ceph也支持类似bcache、flashcache、zfs l2arc的2层架构(ssd作为读写缓存、机械盘作为持久化存储).  
  
下面就来看看Ceph的典型配置.  
  
## 环境
有4台主机, IP地址分别为:  
```
192.168.1.2
192.168.1.3
192.168.1.4
192.168.1.5
```
  
每台主机的磁盘配置都一样, 除了安装操作系统的`/dev/sda`, 另外还包含1块1TB SSD盘和10块16TB的机械盘分别对应设备名如下:  
```
/dev/nvme1n1
/dev/sd[b-k]
```
  
用这4台主机部署Ceph, SSD盘作为只读cache tier(注意根据SSD磁盘容量配置), 机械盘作为持久化tier, 采用类似raid6的冗余结构. 构建一个大的块设备, 给OLTP+OLAP混合型数据库使用, 需要考虑IO打散和条带大小(注意条带大小需匹配4台主机总机械盘的数量、冗余副本数).   
  
另外有2台主机, IP地址分别为:  
```
192.168.1.6
192.168.1.7
```
  
这两台主机作为ceph的客户端(也就是安装数据库的服务器), 共同使用Ceph提供的共享块设备. 不需要创建文件系统, 数据库将直接使用raw设备, 并使用Direct IO(决定了是否需要在ceph客户端配置缓存).  
  
请结合Ceph在数据库场景中的最佳实践, 给出详细配置示例.  
  
在deepwiki中使用以上提示词生成最佳实践(轻微人工调整)  
  
https://deepwiki.com/ceph/ceph
  
# 典型配置实践
  
使用4台主机部署Ceph集群，配置cache tier架构，为OLTP+OLAP混合数据库提供块存储服务。

## 集群架构设计

基于您的需求，这是一个典型的Ceph cache tier配置，使用SSD作为cache tier，HDD作为storage tier。  

### 硬件配置分析
- 4台存储节点：每台1块1TB SSD + 10块16TB HDD
- 总计：4TB SSD容量，640TB HDD容量
- 2台客户端节点用于数据库服务

## 详细配置步骤

### 1. 基础集群部署

首先在4台主机上部署基础Ceph集群：

```yaml
# osd_spec.yml
service_type: osd
service_id: hdd_tier
placement:
  hosts:
    - 192.168.1.2
    - 192.168.1.3
    - 192.168.1.4
    - 192.168.1.5
spec:
  data_devices:
    paths:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
      - /dev/sdf
      - /dev/sdg
      - /dev/sdh
      - /dev/sdi
      - /dev/sdj
      - /dev/sdk
  crush_device_class: hdd
---
service_type: osd
service_id: ssd_tier
placement:
  hosts:
    - 192.168.1.2
    - 192.168.1.3
    - 192.168.1.4
    - 192.168.1.5
spec:
  data_devices:
    paths:
      - /dev/nvme1n1
  crush_device_class: ssd
```  

### 2. 存储池配置

创建storage tier和cache tier存储池：

```bash
# 创建storage tier (HDD池) - 使用EC编码类似RAID6  
ceph osd pool create storage_pool 4096 4096 erasure  
ceph osd pool set storage_pool crush_rule hdd_rule  
  
# 创建cache tier (SSD池) - 使用副本模式  
ceph osd pool create cache_pool 512 512 replicated  
ceph osd pool set cache_pool crush_rule ssd_rule  
ceph osd pool set cache_pool size 3  
ceph osd pool set cache_pool min_size 2
```

备注:

PG数量的选择遵循以下原则：
- 这个计算基于Ceph的最佳实践，在代码中可以看到相关的配置参数：每个OSD建议承载100-200个PG  

每个OSD承载约100个PG是一个平衡点，既能保证良好的数据分布，又不会产生过多的元数据开销。  

这些参数的设置直接影响数据分布和性能。PG数量过少会导致数据分布不均，过多会增加元数据开销。副本数和最小副本数的设置需要在可用性和存储效率之间平衡。

根据"每个OSD建议承载100-200个PG"的建议，计算您配置中的PG数量。 

#### 当前配置分析

您的硬件配置：
- **HDD OSDs**: 4台主机 × 10块HDD = 40个HDD OSDs
- **SSD OSDs**: 4台主机 × 1块SSD = 4个SSD OSDs

#### 计算的PG数量

##### Storage Pool (HDD池)
按照每个OSD承载100-200个PG的建议：
- 40个HDD OSDs × 100-200 PG = 4000-8000个PG
- 建议使用**4096个PG**（2的幂次方，便于数据分布）

##### Cache Pool (SSD池)  
- 4个SSD OSDs × 100-200 PG = 400-800个PG
- 建议使用**512个PG**

#### 修改后的命令

```bash
# 创建storage tier (HDD池) - 使用EC编码类似RAID6  
ceph osd pool create storage_pool 4096 4096 erasure  
ceph osd pool set storage_pool crush_rule hdd_rule  
  
# 创建cache tier (SSD池) - 使用副本模式  
ceph osd pool create cache_pool 512 512 replicated  
ceph osd pool set cache_pool crush_rule ssd_rule  
ceph osd pool set cache_pool size 3  
ceph osd pool set cache_pool min_size 2
```

##### `ceph osd pool create storage_pool 4096 4096 erasure`

**4096 (第一个)**: `pg_num` - Placement Group数量  
- 这是存储池的PG（Placement Group）数量
- PG是Ceph中数据分布的基本单位，用于将对象映射到OSD上
- 4096个PG意味着数据会被分散到4096个逻辑组中

**4096 (第二个)**: `pgp_num` - PG Placement数量  
- 这是用于数据放置的PG数量，通常应该等于pg_num
- 控制数据在集群中的实际分布位置

##### `ceph osd pool create cache_pool 512 512 replicated`

**512 (两个)**: 同样是`pg_num`和`pgp_num`
- 由于SSD容量较小且作为缓存层，使用较少的PG数量（512个）
- 这样可以减少元数据开销，适合缓存场景

##### `ceph osd pool set cache_pool size 3`

**3**: 副本数量  
- 表示每个对象在集群中保存3个副本
- 这是replicated池的标准配置，提供高可用性
- 在代码中可以看到这是默认值  

##### `ceph osd pool set cache_pool min_size 2`

**2**: 最小副本数  
- 当集群处于降级状态时，仍能接受写操作的最小副本数
- 设置为2意味着即使只有2个副本可用，池仍然可以接受写入
- 这确保了在部分OSD故障时的可用性


### 3. Cache Tier配置

配置cache tier关系和缓存模式：  

```bash
# 设置cache tier关系
ceph osd tier add storage_pool cache_pool
ceph osd tier cache-mode cache_pool readonly
ceph osd tier set-overlay storage_pool cache_pool

# 配置cache参数 - 根据1TB SSD容量调整
ceph osd pool set cache_pool target_max_bytes 800000000000  # 800GB
ceph osd pool set cache_pool target_max_objects 1000000
ceph osd pool set cache_pool cache_target_dirty_ratio_micro 400000  # 40%
ceph osd pool set cache_pool cache_target_full_ratio_micro 800000   # 80%
```  

### 4. CRUSH规则配置

创建针对不同设备类型的CRUSH规则：

```bash
# HDD规则 - 确保跨主机分布
ceph osd crush rule create-erasure hdd_rule \
  --profile=default \
  --device-class=hdd

# SSD规则 - 副本分布
ceph osd crush rule create-replicated ssd_rule \
  default host ssd
```

### 5. 块设备创建

创建供数据库使用的RBD设备，配置条带参数：

```bash
# 计算条带参数：  
# 总HDD数量：40块 (4主机 × 10块)  
# EC配置：k=6, m=2 (类似RAID6)  40块盘刚好5组( 5*(6+2) )   
# 条带大小：建议4MB，条带数量：6  即每组中实际存储数据的盘数6  
  
ceph osd pool set storage_pool allow_ec_overwrites true  
  
# 创建RBD设备  
rbd create --size 500T --stripe-unit 4M --stripe-count 6 \  
  --pool storage_pool database_volume 

# 启用RBD特性
rbd feature enable storage_pool/database_volume object-map
rbd feature enable storage_pool/database_volume fast-diff
rbd feature enable storage_pool/database_volume exclusive-lock
```

备注:

#### 条带参数详解

##### EC配置：k=6, m=2 (类似RAID6)
- **k=6**: 数据块数量，表示数据被分成6个数据块
- **m=2**: 校验块数量，表示生成2个校验块用于冗余保护
- 这个配置类似RAID6，可以容忍最多2个OSD同时故障

##### `--stripe-unit 4M`
**4M**: 条带单元大小
- 这是每个对象在单个OSD上存储的数据块大小
- 4MB是针对数据库工作负载的优化选择，平衡了顺序读写性能
- 在RBD代码中可以看到条带单元的处理逻辑  

##### `--stripe-count 6`
**6**: 条带数量
- 这个数字对应EC配置中的k值（数据块数量）
- 表示数据会被分散到6个不同的OSD上
- 与EC的k=6配置保持一致，确保数据分布的最优化
- RBD条带计数的设置和验证逻辑可以在代码中看到  

##### `--size 500T`
**500T**: RBD设备总大小
- 创建一个500TB的块设备供数据库使用
- 这个大小考虑了您的总存储容量（640TB HDD）和冗余开销

#### 条带参数的计算逻辑

条带参数的选择基于以下考虑：
1. **总HDD数量40块**: 确保数据能够充分分散到所有磁盘
2. **EC k=6配置**: 条带数量必须匹配数据块数量
3. **4MB条带大小**: 适合数据库的大块顺序IO模式
4. **性能优化**: 条带配置能够实现IO并行化，提升OLTP+OLAP混合负载性能

RBD条带参数的获取和设置功能在代码中有完整实现  

#### Notes

这个条带配置实现了数据在40个HDD上的均匀分布，通过6个数据块+2个校验块的EC编码提供RAID6级别的冗余保护。4MB的条带单元大小针对数据库工作负载进行了优化，能够有效支持大块顺序读写操作。




### 6. 客户端配置

在客户端主机(192.168.1.6, 192.168.1.7)上配置：

```bash
# 安装ceph客户端
ceph-deploy install 192.168.1.6 192.168.1.7

# 复制配置文件和密钥
ceph-deploy admin 192.168.1.6 192.168.1.7

# 映射RBD设备
rbd map storage_pool/database_volume

# 由于使用direct IO，禁用客户端缓存
echo 'rbd_cache = false' >> /etc/ceph/ceph.conf
echo 'rbd_cache_writethrough_until_flush = false' >> /etc/ceph/ceph.conf
```

### 7. 性能优化配置

针对OLTP+OLAP混合负载优化：

```bash
# OSD性能调优
ceph config set osd osd_op_queue wpq
ceph config set osd osd_op_queue_cut_off high

# 客户端调优
echo 'rbd_readahead_disable_after_bytes = 0' >> /etc/ceph/ceph.conf
echo 'rbd_readahead_max_bytes = 0' >> /etc/ceph/ceph.conf
```

### 8. 监控和维护

设置cache tier监控：

```bash
# 监控cache命中率
ceph osd pool stats cache_pool

# 定期检查cache使用情况
ceph osd pool get cache_pool target_max_bytes
ceph df detail
```

## Notes

这个配置实现了您要求的架构：使用SSD作为只读cache tier提升读性能，HDD作为持久化存储，通过EC编码提供类似RAID6的冗余保护。  条带配置考虑了总磁盘数量和冗余要求，客户端配置针对direct IO进行了优化。由于数据库使用direct IO，客户端缓存被禁用以避免双重缓存。

