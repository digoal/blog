## 如何配置Ceph cache tier SSD 读写缓存?     
                                      
### 作者                                      
digoal                                      
                                      
### 日期                                      
2025-08-12                                      
                                      
### 标签                                      
PostgreSQL , PolarDB , DuckDB , 共享存储 , 块设备 , pfs , direct io , ceph , 2层存储架构 , ssd , 机械盘 , 缓存层 , 存储层 , bcache , flashcache , zfs l2arc , lvm cache      
                                      
----                                      
                                      
## 背景  
  
接上一篇:  
- [《PolarDB存算分离共享存储 : Ceph SSD+机械盘 2层存储 典型配置实践》](../202508/20250812_03.md)
  
本篇分享如何配置Ceph cache tier SSD 读写缓存?  
   
您想要将SSD从只读缓存改为读写缓存，这需要修改cache tier的缓存模式配置。    
  
## 主要修改点

### 1. 更改缓存模式

将cache tier从`readonly`模式改为`writeback`模式：

```bash
# 原配置（只读缓存）
ceph osd tier cache-mode cache_pool readonly

# 修改为读写缓存
ceph osd tier cache-mode cache_pool writeback
```

### 2. 调整缓存参数

由于现在是读写缓存，需要重新配置缓存参数：

```bash
# 配置写回缓存参数
ceph osd pool set cache_pool target_max_bytes 800000000000  # 800GB
ceph osd pool set cache_pool target_max_objects 1000000

# 脏数据比例配置 - 重要！
ceph osd pool set cache_pool cache_target_dirty_ratio_micro 400000      # 40%
ceph osd pool set cache_pool cache_target_dirty_high_ratio_micro 600000 # 60%
ceph osd pool set cache_pool cache_target_full_ratio_micro 800000       # 80%

# 刷新和驱逐年龄
ceph osd pool set cache_pool cache_min_flush_age 600    # 10分钟
ceph osd pool set cache_pool cache_min_evict_age 1800   # 30分钟
```

### 3. 配置HitSet参数

为了优化缓存效果，配置HitSet参数：

```bash
# HitSet配置
ceph osd pool set cache_pool hit_set_type bloom
ceph osd pool set cache_pool hit_set_period 3600        # 1小时
ceph osd pool set cache_pool hit_set_count 4            # 保留4个周期
ceph osd pool set cache_pool hit_set_fpp 0.05           # 布隆过滤器误报率

# 提升策略
ceph osd pool set cache_pool min_read_recency_for_promote 1
ceph osd pool set cache_pool min_write_recency_for_promote 1
```

## 缓存模式对比  

在代码中可以看到不同缓存模式的处理逻辑：

- **READONLY模式**: 只处理读请求，写请求被重定向到后端存储
- **WRITEBACK模式**: 读写都在缓存层处理，脏数据定期刷新到后端存储

### 4. 客户端配置调整

由于现在使用读写缓存，可以启用客户端缓存：

```bash
# 启用客户端缓存（与之前的direct IO配置相反）
echo 'rbd_cache = true' >> /etc/ceph/ceph.conf
echo 'rbd_cache_size = 67108864' >> /etc/ceph/ceph.conf  # 64MB
echo 'rbd_cache_max_dirty = 50331648' >> /etc/ceph/ceph.conf  # 48MB
```

### 5. 监控和调优  

监控缓存性能：

```bash
# 监控缓存统计
ceph osd pool stats cache_pool
ceph osd pool get cache_pool all

# 检查脏数据比例
ceph pg dump | grep cache_pool
```

## Notes

从只读缓存改为读写缓存的主要变化是缓存模式从`CACHEMODE_READONLY`改为`CACHEMODE_WRITEBACK`。  这种模式下，写操作会先写入SSD缓存，然后异步刷新到HDD存储层，可以显著提升写性能。但需要注意配置合适的脏数据比例和刷新策略，避免缓存层过载。  

需要注意的是，cache tiering功能已被标记为deprecated，建议在新部署中考虑其他方案。

下一篇介绍替代方案.
  
