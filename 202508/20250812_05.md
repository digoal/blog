## 作为Ceph缓存层的SSD盘故障, 如何更换?   
                                      
### 作者                                      
digoal                                      
                                      
### 日期                                      
2025-08-12                                      
                                      
### 标签                                      
PostgreSQL , PolarDB , DuckDB , 共享存储 , 块设备 , pfs , direct io , ceph , 2层存储架构 , ssd , 机械盘 , 缓存层 , 存储层 , bcache , flashcache , zfs l2arc , lvm cache      
                                      
----                                      
                                      
## 背景  
  
接上一篇:  
- [《PolarDB存算分离共享存储 : Ceph SSD+机械盘 2层存储 典型配置实践》](../202508/20250812_03.md)
  
本篇分享: 作为Ceph缓存层的SSD盘故障, 如何更换?
   
您询问的是在Ceph集群中如何更换故障的SSD盘，这涉及到cache tier中SSD OSD的替换操作。<cite/>

## SSD故障更换详细步骤

### 1. 确认故障状态

首先检查故障SSD的状态：

```bash
# 检查OSD状态
ceph osd tree
ceph osd status

# 检查具体故障OSD（假设故障OSD为osd.40）
ceph osd dump | grep osd.40
```

### 2. 标记OSD为out状态

将故障的SSD OSD标记为out，让数据迁移到其他OSD：

```bash
# 标记OSD为out（假设故障SSD OSD ID为40）
ceph osd out osd.40

# 监控数据迁移进度
ceph -w
```

### 3. 停止OSD服务

在故障主机上停止OSD服务：

```bash
# 使用cephadm停止OSD服务
ceph orch daemon stop osd.40

# 或者直接停止systemd服务
systemctl stop ceph-osd@40
```

### 4. 移除故障OSD

从集群中完全移除故障OSD：  

```bash
# 移除OSD（--replace保留OSD ID用于新设备）
ceph orch osd rm 40 --replace --zap

# 监控移除状态
ceph orch osd rm status
```

### 5. 物理更换SSD设备

- 关闭主机电源
- 更换故障的SSD设备（/dev/nvme1n1）
- 重新启动主机

### 6. 准备新SSD设备

确保新SSD设备可用：  

```bash
# 清理新设备（如果需要）
ceph orch device zap <hostname> /dev/nvme1n1

# 检查设备状态
ceph orch device ls
```

### 7. 重新部署SSD OSD

使用原有的OSD规格重新部署：  

```bash
# 应用SSD OSD规格（会自动使用被标记为destroyed的OSD ID）
ceph orch apply osd --service-id ssd_tier --dry-run

# 确认无误后执行
ceph orch apply osd --service-id ssd_tier
```

### 8. 验证新OSD状态

确认新OSD正常工作：

```bash
# 检查新OSD状态
ceph osd tree
ceph osd status

# 确认OSD已加入正确的CRUSH设备类
ceph osd crush tree --show-shadow

# 检查cache tier状态
ceph osd pool stats cache_pool
```

### 9. 重新配置cache tier（如果需要）

如果cache tier配置受到影响，重新设置：

```bash
# 检查cache tier关系
ceph osd tier ls storage_pool

# 如果需要重新设置overlay
ceph osd tier set-overlay storage_pool cache_pool
```

### 10. 监控集群恢复

监控集群健康状态和数据恢复：

```bash
# 监控集群状态
ceph health detail
ceph -s

# 监控PG恢复进度
ceph pg stat
```

## 注意事项

1. **数据安全**: 在cache tier中，SSD故障通常不会导致数据丢失，因为数据最终存储在HDD层
2. **性能影响**: SSD故障期间，缓存功能会受影响，读写性能可能下降
3. **自动化**: 使用`--replace`标志可以保留原OSD ID，简化重新部署过程  

## Notes

这个流程利用了cephadm的设备替换功能  ，可以自动化处理大部分替换步骤。由于您使用的是cache tier架构，SSD故障主要影响缓存性能，不会造成数据丢失，因为持久化数据存储在HDD层。<cite/>

  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
