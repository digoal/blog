## Ceph 机械盘分组优化思路
                                      
### 作者                                      
digoal                                      
                                      
### 日期                                      
2025-08-14                                      
                                      
### 标签                                      
PostgreSQL , PolarDB , DuckDB , 共享存储 , 块设备 , pfs , direct io , ceph , 2层存储架构 , ssd , 机械盘 , 缓存层 , 存储层 , bcache , flashcache , zfs l2arc , lvm cache      
                                      
----                                      
                                      
## 背景  
接上3篇:  
- [《PolarDB存算分离共享存储 : Ceph SSD+机械盘 2层存储 典型配置实践》](../202508/20250812_03.md)
- [《Ceph Cache Tiering 功能替代方案》](../202508/20250812_07.md)
- [《Ceph Cache Tiering 功能替代方案: LVM Cache 实践》](../202508/20250812_08.md)
  
除了通过ssd cache tier进行优化, 实际上在分组侧也可以优化ceph性能. 组越多、吞吐越大, 但是还要考虑组内数据盘平均分布到所有的存储节点上.   
   
```  
3台主机, 10GB互联, 每台主机10块16TB的机械硬盘, 既要保证可靠性, 又要保证性能.

采用类似raid60的结构(每组2个纠错码, 每组的数据盘平均分布在不同主机上), 尽可能做磁盘高吞吐, 给HTAP型数据库场景使用. 请给出配置(特别注意条带、单个组的盘数、组数).  
```
  
您询问的是关于在Ceph存储系统中配置类似RAID60结构的存储布局，用于HTAP数据库场景的高吞吐量需求。

## 配置建议

基于您的硬件配置（3台主机，每台10块16TB机械硬盘，10GB互联），以下是针对HTAP场景的Ceph OSD配置建议：

### 基础配置参数

**条带配置：**
- 每个纠删码组（EC group）使用8个数据块 + 2个校验块（8+2配置）
- 条带大小：4MB（适合大文件和顺序读写）
- 这样可以获得类似RAID60的冗余性和性能特征

**磁盘分组：**
- 每台主机10块盘全部用作OSD数据设备
- 总共30个OSD，可以形成3个纠删码组
- 每组10个OSD（跨3台主机分布）

### Ceph OSD规格配置

可以使用以下OSD服务规格来部署：  

```yaml
service_type: osd
service_id: htap_osd_spec
placement:
  host_pattern: '*'
spec:
  data_devices:
    size: '16TB'  # 匹配16TB机械硬盘
    rotational: 1  # 机械硬盘
  encrypted: false  # 为了性能考虑
```

### 存储池配置

为HTAP场景创建纠删码存储池：

```bash
# 创建纠删码配置文件
ceph osd erasure-code-profile set htap_profile \
  k=8 m=2 \
  crush-failure-domain=host \
  stripe_unit=4194304

# 创建存储池
ceph osd pool create htap_pool erasure htap_profile
```

### 性能优化配置

**默认副本数设置：**  

**CRUSH规则配置：**  

## 关键配置说明

- **条带单元**：4MB，适合HTAP场景的大数据块读写
- **单个组盘数**：10块（8数据+2校验），提供良好的并行度
- **组数**：3组，充分利用所有30块磁盘
- **故障域**：设置为主机级别，确保数据跨主机分布

## Notes

这个配置基于Ceph的纠删码功能来实现类似RAID60的效果。纠删码8+2配置可以容忍任意2块磁盘同时故障，同时提供比传统副本更好的存储效率。对于HTAP场景，大条带单元有助于提高顺序读写性能，而跨主机的数据分布则确保了网络带宽的充分利用。


---

为了提高吞吐, 可以继续提高组数, 相当于空间换时间.  

```
改成每组(3+2)
```

您想要将纠删码配置从(8+2)改为(3+2)配置。

## 修改后的配置建议

基于您的要求，将每组改为3个数据块+2个校验块的配置：

### 更新的纠删码配置

**条带配置：**
- 每个纠删码组使用3个数据块 + 2个校验块（3+2配置）  
- 条带大小：4MB（保持不变，适合HTAP场景）
- 总共需要5个OSD per组

**磁盘分组重新规划：**
- 30个OSD可以形成6个纠删码组（30÷5=6组）
- 每组5个OSD（3数据+2校验）
- 跨3台主机分布，每台主机参与所有组

### 更新的存储池配置

```bash
# 创建3+2纠删码配置文件
ceph osd erasure-code-profile set htap_profile_3_2 \
  k=3 m=2 \
  crush-failure-domain=host \
  stripe_unit=4194304

# 创建存储池
ceph osd pool create htap_pool erasure htap_profile_3_2
```  

### 配置对比

| 配置项 | 原配置(8+2) | 新配置(3+2) |
|--------|-------------|-------------|
| 数据块数 | 8 | 3 |
| 校验块数 | 2 | 2 |
| 每组OSD数 | 10 | 5 |
| 总组数 | 3 | 6 |
| 存储效率 | 80% | 60% |
| 容错能力 | 2块盘
  
  
