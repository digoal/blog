## update时新tuple如何选择空闲block插入?   
                                                        
### 作者                                                        
digoal                                                        
                                                        
### 日期                                                        
2025-09-17                                                      
                                                        
### 标签                                                        
PostgreSQL , PolarDB , DuckDB , deepwiki , tuple , mvcc , block , HOT , heap only tuple update               
                                                        
----                                                        
                                                        
## 背景      
当update某一条记录时, 新版本会插入到哪个block? 如果原来的tuple在block A, 并且block A和block B都有足够空间放下新的tuple版本, 那么如何选择A还是B?   
  
您询问的是PostgreSQL中UPDATE操作时如何选择新版本tuple插入到哪个block的问题。  
  
## 讲解  
  
当UPDATE一条记录时，PostgreSQL会优先尝试将新版本tuple插入到原始tuple所在的同一个page（block），这样可以实现HOT（Heap-Only Tuple）更新优化(尽量避免tuple新版本引起的索引修改)。   
  
HOT相关知识点可参考:  
- [《PostgreSQL Heap Only Tuple - HOT (降低UPDATE引入的索引写IO放大)》](../201809/20180925_02.md)    
- [《PostgreSQL HOT vacuum 收缩链路对DML where CTID=ctid 安全吗?》](../202204/20220401_03.md)    
  
**File:** src/backend/access/heap/heapam.c (L4006-4027)  
```c  
	if (newbuf == buffer)  
	{  
		/*  
		 * Since the new tuple is going into the same page, we might be able  
		 * to do a HOT update.  Check if any of the index columns have been  
		 * changed.  
		 */  
		if (!bms_overlap(modified_attrs, hot_attrs))  
		{  
			use_hot_update = true;  
  
			/*  
			 * If none of the columns that are used in hot-blocking indexes  
			 * were updated, we can apply HOT, but we do still need to check  
			 * if we need to update the summarizing indexes, and update those  
			 * indexes if the columns were updated, or we may fail to detect  
			 * e.g. value bound changes in BRIN minmax indexes.  
			 */  
			if (bms_overlap(modified_attrs, sum_attrs))  
				summarized_update = true;  
		}  
	}  
```  
  
具体的选择逻辑如下：  
  
### 1. 优先选择原始page（block A）  
  
系统首先检查原始page是否有足够空间容纳新tuple：   
  
**File:** src/backend/access/heap/heapam.c (L3975-3981)  
```c  
	}  
	else  
	{  
		/* No TOAST work needed, and it'll fit on same page */  
		newbuf = buffer;  
		heaptup = newtup;  
	}  
```  
  
如果原始page有足够空间，系统会进一步检查是否可以进行HOT更新：   
  
**File:** src/backend/access/heap/heapam.c (L4013-4026)  
```c  
	if (newbuf == buffer)  
	{  
		/*  
		 * Since the new tuple is going into the same page, we might be able  
		 * to do a HOT update.  Check if any of the index columns have been  
		 * changed.  
		 */  
		if (!bms_overlap(modified_attrs, hot_attrs))  
		{  
			use_hot_update = true;  
  
			/*  
			 * If none of the columns that are used in hot-blocking indexes  
			 * were updated, we can apply HOT, but we do still need to check  
			 * if we need to update the summarizing indexes, and update those  
			 * indexes if the columns were updated, or we may fail to detect  
			 * e.g. value bound changes in BRIN minmax indexes.  
			 */  
			if (bms_overlap(modified_attrs, sum_attrs))  
				summarized_update = true;  
		}  
	}  
```  
  
### 2. 需要新page时的选择策略  
  
当原始page空间不足时，系统会通过`RelationGetBufferForTuple`函数寻找新的page：   
  
**File:** src/backend/access/heap/heapam.c (L3916-3938)  
```c  
		/*  
		 * Now, do we need a new page for the tuple, or not?  This is a bit  
		 * tricky since someone else could have added tuples to the page while  
		 * we weren't looking.  We have to recheck the available space after  
		 * reacquiring the buffer lock.  But don't bother to do that if the  
		 * former amount of free space is still not enough; it's unlikely  
		 * there's more free now than before.  
		 *  
		 * What's more, if we need to get a new page, we will need to acquire  
		 * buffer locks on both old and new pages.  To avoid deadlock against  
		 * some other backend trying to get the same two locks in the other  
		 * order, we must be consistent about the order we get the locks in.  
		 * We use the rule "lock the lower-numbered page of the relation  
		 * first".  To implement this, we must do RelationGetBufferForTuple  
		 * while not holding the lock on the old page, and we must rely on it  
		 * to get the locks on both pages in the correct order.  
		 *  
		 * Another consideration is that we need visibility map page pin(s) if  
		 * we will have to clear the all-visible flag on either page.  If we  
		 * call RelationGetBufferForTuple, we rely on it to acquire any such  
		 * pins; but if we don't, we have to handle that here.  Hence we need  
		 * a loop.  
		 */  
```  
  
新page的选择遵循以下策略：   
  
1. **优先使用缓存的target page**：如果有BulkInsertState，优先使用其缓存的page  
2. **查询FSM（Free Space Map）**：向FSM请求有足够空间的page  
3. **尝试最后一个page**：如果FSM没有记录，尝试relation的最后一个page  
  
**File:** src/backend/access/heap/hio.c (L560-585)  
```c  
	/*  
	 * We first try to put the tuple on the same page we last inserted a tuple  
	 * on, as cached in the BulkInsertState or relcache entry.  If that  
	 * doesn't work, we ask the Free Space Map to locate a suitable page.  
	 * Since the FSM's info might be out of date, we have to be prepared to  
	 * loop around and retry multiple times. (To ensure this isn't an infinite  
	 * loop, we must update the FSM with the correct amount of free space on  
	 * each page that proves not to be suitable.)  If the FSM has no record of  
	 * a page with enough free space, we give up and extend the relation.  
	 *  
	 * When use_fsm is false, we either put the tuple onto the existing target  
	 * page or extend the relation.  
	 */  
	if (bistate && bistate->current_buf != InvalidBuffer)  
		targetBlock = BufferGetBlockNumber(bistate->current_buf);  
	else  
		targetBlock = RelationGetTargetBlock(relation);  
  
	if (targetBlock == InvalidBlockNumber && use_fsm)  
	{  
		/*  
		 * We have no cached target page, so ask the FSM for an initial  
		 * target.  
		 */  
		targetBlock = GetPageWithFreeSpace(relation, targetFreeSpace);  
	}  
```  
  
### 3. 锁定顺序避免死锁  
  
当需要同时锁定两个不同的page时，系统使用固定的锁定顺序来避免死锁：   
  
**File:** src/backend/access/heap/heapam.c (L3916-3938)  
```c  
		/*  
		 * Now, do we need a new page for the tuple, or not?  This is a bit  
		 * tricky since someone else could have added tuples to the page while  
		 * we weren't looking.  We have to recheck the available space after  
		 * reacquiring the buffer lock.  But don't bother to do that if the  
		 * former amount of free space is still not enough; it's unlikely  
		 * there's more free now than before.  
		 *  
		 * What's more, if we need to get a new page, we will need to acquire  
		 * buffer locks on both old and new pages.  To avoid deadlock against  
		 * some other backend trying to get the same two locks in the other  
		 * order, we must be consistent about the order we get the locks in.  
		 * We use the rule "lock the lower-numbered page of the relation  
		 * first".  To implement this, we must do RelationGetBufferForTuple  
		 * while not holding the lock on the old page, and we must rely on it  
		 * to get the locks on both pages in the correct order.  
		 *  
		 * Another consideration is that we need visibility map page pin(s) if  
		 * we will have to clear the all-visible flag on either page.  If we  
		 * call RelationGetBufferForTuple, we rely on it to acquire any such  
		 * pins; but if we don't, we have to handle that here.  Hence we need  
		 * a loop.  
		 */  
```  
  
具体实现在`hio.c`中：   
  
**File:** src/backend/access/heap/hio.c (L640-657)  
```c  
		else if (otherBlock < targetBlock)  
		{  
			/* lock other buffer first */  
			buffer = ReadBuffer(relation, targetBlock);  
			if (PageIsAllVisible(BufferGetPage(buffer)))  
				visibilitymap_pin(relation, targetBlock, vmbuffer);  
			LockBuffer(otherBuffer, BUFFER_LOCK_EXCLUSIVE);  
			LockBuffer(buffer, BUFFER_LOCK_EXCLUSIVE);  
		}  
		else  
		{  
			/* lock target buffer first */  
			buffer = ReadBuffer(relation, targetBlock);  
			if (PageIsAllVisible(BufferGetPage(buffer)))  
				visibilitymap_pin(relation, targetBlock, vmbuffer);  
			LockBuffer(buffer, BUFFER_LOCK_EXCLUSIVE);  
			LockBuffer(otherBuffer, BUFFER_LOCK_EXCLUSIVE);  
		}  
```  
  
## Notes  
  
这个选择策略的设计目标是：  
1. 最大化HOT更新的可能性（通过优先使用原始page）  
2. 避免死锁（通过固定的锁定顺序）  
3. 高效利用空间（通过FSM管理）  
  
B-tree索引的插入策略（在`nbtinsert.c`中）与heap tuple的插入策略是不同的系统，虽然都涉及page选择，但逻辑完全不同。  
  
  
## 参考  
https://deepwiki.com/postgres/postgres  
    
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
