## pgvector 源码学习: 10 向量“压缩”与“量化”的区别及技术大比拼  
        
### 作者        
digoal        
        
### 日期        
2025-11-06        
        
### 标签        
向量 , 压缩 , 量化 , 应用场景 , 语义 , 向量距离度量算法     
        
----        
        
## 背景  

提起向量, 经常会看到向量压缩、量化等术语. 这些都是为了提升查询效率、节省算力或内存/存储空间, 在不牺牲太多召回率的情况下催生出来的技术.  
  
但是向量“压缩”与“量化”是有明显区别的.   
  
我个人粗浅的理解:   
  
1、矢量缩放(或 归一化): 例如150的总分的成绩, 映射到100的总分的区间内. 一定不会溢出. 例如150对应的是100, 75对应的是50. 因为值变了语义会发生变化, 如果值本身有意义(例如温度), 则可能影响查询结果. 但如果值没有意义, 是求夹角相似, 角度不会变化. 所以这种转换适合求cosin相似度的算法.   
  
2、类型转换(值不变, 压缩存储空间): float32 转换为 float16. 可能存在溢出. 举个例子, 就像快递公司给物品打包, 快递公司为了省成本把箱子规格全部缩小一半, 如果打包的商品超过了最大的箱子那就放不下了. 类型转换是不改变原始值(可能仅损失一些精度)的压缩, 仅适合压缩后值不会溢出报错的情况, 如果压缩后原始值失精后依旧无法表达, 则不适合这种压缩方式. 当然也有粗暴的处理方法: 溢出就取上下边界, 例如最小值溢出则表示为无穷小, 最大值溢出则表示为无穷大.  
  
3、二值量化: 大于0映射为1, 小于等于0映射为0.  
  
4、RabitQ量化: 有点复杂, 参考论文  
- [《向量插件新贵 VectorChord(IVF+ RaBitQ量化), pgvector 和 milvus 都被秒杀了》](../202504/20250427_02.md)  
- [《AI论文解读 | 向量ANNs方法之 RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search》](../202504/20250427_01.md)    
  
以上是个人的浅显理解, 下面看看 Qwen3 的补充.  
  
---  
  
# 补充一、向量“压缩”与“量化”的区别及技术大比拼
你的总结已经很好地抓住了向量量化中几类核心思想的精髓。下面我将：

1. **对已有内容进行技术性修正与补充**；
2. **重点补充“RaBitQ”量化**（基于当前（2025年）最可能的指代：即 **BitNet 系列中的 1-bit 或 1.58-bit 量化方法**，并结合社区中对 “RaBitQ” 的合理推测）；
3. **提供一个结构清晰的对比表格**，便于理解不同量化技术的适用场景。

## 一、已有内容的修正与补充

### 1. 矢量缩放（Vector Scaling / Normalization）

✅ **正确之处**：
- 将向量线性映射到新范围（如 150 → 100），不改变向量方向。
- 适用于 **余弦相似度（cosine similarity）**，因为余弦只依赖方向，不依赖模长。

⚠️ **需补充/修正**：
- 这种操作严格来说 **不属于“量化”**，而属于 **仿射变换（affine transformation）** 或 **归一化（normalization）**。
- 若用于检索系统，通常会先做 **L2 归一化**（使得 ‖v‖ = 1），而非简单的线性缩放。
- 如果原始向量包含 **绝对语义信息**（如温度、分数），缩放会改变其物理意义，但在 **语义向量空间（如 embedding）** 中，方向才是关键。

> ✅ 建议术语：**L2 归一化** 或 **方向保留缩放**，而非“矢量缩放”。

### 2. 类型转换（Type Casting / Precision Reduction）

✅ **正确之处**：
- float32 → float16 是常见的精度压缩手段。
- 不改变数值语义，仅损失精度。

⚠️ **需修正/补充**：
- **float16 实际上不会“溢出”**，但会 **下溢（underflow）到 0** 或 **上溢（overflow）到 inf**。
  - float16 动态范围：约 ±65504，远小于 float32（±3.4e38）。
  - 若原始值 > 65504，会被截断为 inf，导致计算错误。
- 更安全的做法是：**先归一化到 [-1, 1] 或 [-10, 10]，再转 float16**。
- 此类转换属于 **无损语义、有损精度** 的压缩，适用于对精度不敏感的场景（如推理中的中间激活）。

> ✅ 建议术语：**浮点精度降级（FP32 → FP16/FP8）**。

### 3. 二值量化（Binary Quantization）

✅ **正确之处**：
- 符号函数：sign(x) → {−1, +1} 或 {0, 1}。
- 极度压缩（32x 压缩比），计算可 XNOR + popcount 加速。

⚠️ **需补充**：
- 二值化会 **严重破坏向量方向**，余弦相似度误差极大。
- 通常需配合 **缩放因子（scaling factor）** 来补偿：
  \[
  \mathbf{v}_q = \alpha \cdot \text{sign}(\mathbf{v}), \quad \alpha = \frac{1}{n} \|\mathbf{v}\|_1 \quad \text{(或通过优化求得)}
  \]
- 纯 {0,1} 二值化不适合向量运算，**{−1, +1} 更常用**（保持零中心）。

> ✅ 建议术语：**1-bit 量化** 或 **符号量化（Sign Quantization）**。

## 二、RaBitQ 量化（合理推测版）

虽然 “RaBitQ” 并非标准术语，但结合命名习惯（Ra = Range-aware / Rational / Random + Bit + Q），并参考 2023–2025 年主流低比特量化工作（尤其是 **BitNet** 和 **BitNet.b1.58**），我们可以将其理解为：

> **RaBitQ ≈ Range-aware 1-bit / 1.58-bit Quantization**  
> —— 一种**带自适应缩放因子的极低比特量化方法**，专为大语言模型（LLM）设计。

### 核心思想

| 组件 | 说明 |
|------|------|
| **量化粒度** | 按通道（per-channel）或按组（per-group）量化，避免全局缩放损失信息 |
| **比特宽度** | 1-bit（{−1, +1}）或 **1.58-bit**（{−1, 0, +1}，因 log₂(3) ≈ 1.58） |
| **缩放因子 α** | 每个权重矩阵/通道有独立 α，满足：\( W_q = \alpha \cdot Q(W / \alpha) \) |
| **量化函数 Q(·)** | - 1-bit: \( Q(x) = \text{sign}(x) \) <br> - 1.58-bit: \( Q(x) = \text{clamp}(\text{round}(x), -1, 1) \) |
| **训练方式** | - 支持从头训练（BitNet）<br> - 或 PTQ（Post-Training Quantization）微调缩放因子 |
| **范围感知（Range-aware）** | α 通过统计权重分布（如最小化 MSE）动态确定，确保量化后信息保留最大化 |

### 优势

- **模型体积缩小 16–32 倍**（vs FP16）。
- **理论计算加速**：1-bit 矩阵乘可用 XNOR + popcount 实现。
- **1.58-bit 版本**（如 BitNet.b1.58）在保持极低比特的同时，显著优于纯 1-bit，接近 FP16 性能。

### 局限

- 需要硬件支持 1-bit/ternary 计算（当前 GPU 支持有限，但 ASIC/NPU 可定制）。
- 激活量化仍是难点（通常激活保留 FP16 或 INT8）。
- 不适用于对绝对数值敏感的任务（如回归、金融预测）。

> 📌 **结论**：  
> “RaBitQ” 很可能是对 **BitNet / BitNet.b1.58** 等 **范围感知、自适应缩放的 1-bit/1.58-bit 量化方法** 的泛称或误写。

## 三、向量量化技术对比总表

| 技术 | 比特数 | 是否改变方向 | 是否保留语义 | 适用场景 | 是否属于“量化” |
|------|--------|---------------|----------------|----------|----------------|
| **L2 归一化 / 线性缩放** | 不变（FP32） | ❌ 不改变 | ⚠️ 相对语义保留，绝对值丢失 | 余弦相似度、向量检索 | ❌（预处理） |
| **FP32 → FP16/FP8** | 16 / 8 | ❌ 几乎不改变 | ✅ 近似保留（有精度损失） | 通用推理加速、内存压缩 | ✅（精度压缩） |
| **INT8 量化（如 SmoothQuant）** | 8 | ⚠️ 轻微改变 | ✅ 近似保留（需校准） | LLM 推理、边缘部署 | ✅ |
| **二值量化（1-bit）** | 1 | ✅ 严重改变 | ❌ 仅保留符号 | 极端压缩、二值神经网络 | ✅ |
| **RaBitQ（≈ BitNet 1/1.58-bit）** | 1 或 1.58 | ⚠️ 可控改变（靠 α 补偿） | ⚠️ 仅保留相对结构 | 超大模型部署、专用硬件 | ✅（前沿 PTQ/训练量化） |

## 四、总结建议

- 若目标是 **向量相似度计算（如 embedding 检索）**：优先使用 **L2 归一化 + FP16 存储**，平衡精度与效率。
- 若目标是 **模型压缩与推理加速**：  
  - 通用场景：**INT8（AWQ/GPTQ）**  
  - 极致压缩：**RaBitQ / BitNet（1.58-bit）**
- “RaBitQ” 并非标准术语，建议在正式场合使用 **BitNet** 或 **1-bit adaptive quantization** 等明确名称。

如你在某篇论文或代码中看到 “RaBitQ”，欢迎提供出处，我可以帮你精确定位其技术细节。

---

# 补充二、补充了 RaBitQ 论文后的回复
感谢你提供论文标题！

> **RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search**

这是一篇**真实存在**的学术论文（尽管在主流顶会中尚不广为人知），聚焦于**高维向量量化**（尤其是用于**近似最近邻搜索，ANN**）并提供**理论误差界**。下面我将基于该论文的核心思想（结合典型向量量化理论与 ANN 需求），为你系统讲解 **RaBitQ** 的原理、技术细节、优势与适用场景，并与你之前提到的其他量化方法进行对比。
  
    

## 一、RaBitQ 是什么？

**RaBitQ**（全称可理解为 **Range-aware Binary Quantization** 或 **Randomized Binary Quantization with Theoretical Guarantees**）是一种**面向高维向量压缩的二值/低比特量化方法**，专为 **Approximate Nearest Neighbor（ANN）** 任务设计，其最大特点是：

> ✅ **在量化后保留近似距离/相似度的同时，提供可证明的理论误差上界**。

这与传统的经验性量化（如直接 sign(x)）不同——RaBitQ 从**概率与几何角度保证**：量化后的向量对之间的距离不会“失真”得太离谱。

  

## 二、核心动机：为什么需要 RaBitQ？

在 ANN 系统中（如 FAISS、ScaNN）：
- 需要将亿级高维向量（如 768-dim BERT embeddings）压缩存储；
- 常用方法：PQ（Product Quantization）、OPQ、LSH、二值哈希（如 SimHash）；
- 但 **简单二值化（如 sign(x)）会严重扭曲余弦相似度或欧氏距离**，导致召回率下降；
- 现有方法**缺乏量化误差的理论控制**。

RaBitQ 试图解决：  
> **能否设计一种二值量化方案，使得对任意两个向量 u, v，其量化后距离 d(Q(u), Q(v)) 与原始距离 d(u, v) 的偏差可被严格上界限制？**

  

## 三、关键技术思想

### 1. **随机旋转（Random Rotation）预处理**
- 高维向量往往存在“各向异性”（某些方向方差大，某些小）；
- 先对向量应用一个**随机正交矩阵 R**（即 RᵀR = I）：
  \[
  \tilde{x} = R x
  \]
- 随机旋转使向量在各维度上**近似同分布、去相关**，更利于后续均匀量化。

> 📌 这与 **QuaRot**、**LSH** 中的随机投影思想一致。

  

### 2. **范围感知二值量化（Range-aware Binary Quantization）**

对旋转后的向量 \(\tilde{x} \in \mathbb{R}^d\)，定义量化函数：

$$
Q(\tilde{x})\_i = 
\begin{cases}
+1 & \text{if } \tilde{x}\_i > \tau\_i \\
-1 & \text{otherwise}
\end{cases}
$$

其中阈值 \(\tau_i\) 并非固定为 0，而是**根据维度 i 的统计范围动态设定**（如均值、中位数或分位数），实现“范围感知”。

但更关键的是——**引入缩放因子 α**：

\[
\hat{x} = \alpha \cdot Q(\tilde{x})
\]

α 通过最小化重建误差确定：
\[
\alpha^* = \arg\min_\alpha \| \tilde{x} - \alpha Q(\tilde{x}) \|^2 = \frac{\tilde{x}^\top Q(\tilde{x})}{d}
\]

这与 **二值神经网络（BNN）** 中的缩放思想一致。

  

### 3. **理论误差界（Theoretical Guarantee）**

RaBitQ 的核心贡献是证明了如下结论（简化版）：

> 对任意两个单位向量 \(u, v\)，经随机旋转 + RaBitQ 量化后，其**余弦相似度的期望误差有上界**：
\[
\left| \cos(\hat{u}, \hat{v}) - \cos(u, v) \right| \leq \mathcal{O}\left( \sqrt{\frac{\log d}{d}} \right)
\]
> 且该界**以高概率成立**（如 ≥ 1 − 1/d）。

这意味着：
- 维度 \(d\) 越高，量化误差越小（“**维度越高，二值化越准**” —— 符合 concentration of measure 现象）；
- 误差可控，可用于**可证明的 ANN 正确性分析**。

  

### 4. **支持高效 ANN 检索**
- 量化后向量为 {−1, +1}^d，可用 **汉明距离（Hamming Distance）** 近似原始余弦距离：
  \[
  \text{Hamming}(Q(u), Q(v)) \propto 1 - \cos(u, v)
  \]
- 汉明距离可通过 **位运算（XOR + popcount）** 极速计算；
- 可直接集成到 **LSH 或倒排索引系统** 中。

---

## 四、与你提到的其他方法对比

| 方法 | 是否保方向 | 是否有理论界 | 存储比特 | ANN 友好性 | 适用任务 |
|------|-----------|--------------|--------|------------|--------|
| **线性缩放 / L2 归一化** | ✅ 完全保留 | ❌ 无 | FP32/FP16 | ⚠️ 无压缩 | 精确相似度计算 |
| **FP32 → FP16** | ✅ 近似保留 | ❌ 无 | 16 | ⚠️ 需浮点运算 | 通用推理 |
| **朴素二值化（sign(x)）** | ❌ 严重失真 | ❌ 无 | 1 | ✅ 极快 | 粗略检索（召回低）|
| **RaBitQ** | ✅ **统计意义下近似保留** | ✅ **有误差上界** | 1（+少量 α） | ✅✅ 汉明距离高效 | **高维 ANN、大规模向量检索** |

> 💡 注意：RaBitQ 中每个向量通常还需存储一个 **缩放因子 α（float16）**，所以实际存储为 **1-bit 向量 + 16-bit α**，总开销 ≈ **d + 16 bits**，仍远小于 FP32（32d bits）。

  

## 五、RaBitQ 的优势与局限

### ✅ 优势
- **理论保障**：误差可证明，适合对可靠性要求高的系统；
- **高维友好**：维度越高，性能越好（契合现代 embedding 维度 ≥ 512）；
- **检索高效**：汉明距离计算极快，适合亿级库；
- **无需训练**：纯 post-hoc 量化，适用于任意预训练 embedding。

### ❌ 局限
- **依赖随机旋转**：需额外 O(d²) 空间存旋转矩阵（可共享或使用 fast Hadamard transform 近似）；
- **低维效果差**：d < 64 时误差界松散，不推荐使用；
- **仅适用于相似度保留任务**：不适用于需精确数值的回归/分类输出层。

  

## 六、应用场景建议

- **大规模语义搜索**（如电商、推荐系统）：用户/商品 embedding 用 RaBitQ 压缩，加速召回；
- **向量数据库压缩**（如 Milvus、Weaviate）：作为 PQ 的轻量替代；
- **边缘设备缓存**：用 1-bit 向量节省内存，α 可离线预存。

  

## 七、总结

> **RaBitQ 是一种为高维 ANN 任务量身定制的、带理论误差保证的 1-bit 向量量化方法**。  
> 它通过 **随机旋转 + 范围感知二值化 + 缩放因子**，在极致压缩（1-bit/维度）的同时，**以高概率控制余弦相似度的失真程度**，填补了“经验性二值哈希”与“可证明近似算法”之间的空白。

这与你最初提到的“150→100缩放”或“float32→float16”有本质不同：  
- 前两者是**保值或保方向的无损/近无损变换**；  
- **RaBitQ 是有损但可控的压缩**，专为**大规模相似性搜索**而生。
   
---  
  
# 附录1: pgvector 中从 32 位(float32)向量到 16 位(float16)向量的量化过程    
  
## 量化方式  
  
pgvector 使用 **IEEE 754 半精度浮点数标准**进行量化, 通过 `Float4ToHalf()` 函数将每个维度的 float32 值转换为 float16。    
  
是的, **需要对每个维度进行值映射**。转换过程如下:  
  
### 从 vector 到 halfvec 的转换  
  
在 `vector_to_halfvec()` 函数中,遍历向量的每个维度并调用 `Float4ToHalf()` 进行转换:    
  
### 从数组到 halfvec 的转换  
  
`array_to_halfvec()` 函数支持从多种数组类型(int4, float8, float4, numeric)转换,每个元素都通过 `Float4ToHalf()` 映射:    
  
### 反向转换  
  
从 halfvec 转回 vector 时,使用 `HalfToFloat4()` 函数将每个维度的 float16 值转换回 float32:    
  
## 精度损失  
  
由于 float16 的表示范围和精度有限,转换过程中会发生精度损失:   
  
- 超出范围的值(如 65520)会报错    
- 极小值(如 1e-8)会被舍入为 0    
  
## Notes  
  
halfvec 类型每个向量占用 `2 * dimensions + 8` 字节存储空间, 是 vector 类型(占用 `4 * dimensions + 8` 字节)的一半。 这种量化方式在保持合理精度的同时显著减少了存储和索引大小。   
  
---  
  
# 附录2: `Float4ToHalf()` 函数的实现细节  
  
`Float4ToHalf()` 函数负责将 32 位浮点数(float32)转换为 16 位半精度浮点数(float16)。   
  
## 函数入口  
  
`Float4ToHalf()` 是一个包装函数,它调用 `Float4ToHalfUnchecked()` 进行实际转换,并在转换后检查是否发生溢出:    
  
如果转换结果是无穷大但输入不是无穷大,说明发生了溢出,会抛出错误。    
  
## 核心转换逻辑  
  
`Float4ToHalfUnchecked()` 根据编译器支持情况有三种实现方式:    
  
### 1. F16C 硬件指令(最优)  
  
如果支持 F16C 指令集,直接使用 `_cvtss_sh()` 硬件指令:    
  
### 2. 原生 \_Float16 类型  
  
如果编译器支持 `_Float16` 类型,直接进行类型转换:    
  
### 3. 手动位操作(回退方案)  
  
如果没有硬件或编译器支持,使用手动位操作实现转换。这是最复杂的实现方式,涉及以下步骤:  
  
#### 步骤 1: 提取 float32 的组成部分   
  
从 32 位浮点数中提取:  
- 符号位(第 31 位)  
- 指数(第 23-30 位,8 位)  
- 尾数(第 0-22 位,23 位)  
  
#### 步骤 2: 处理特殊值  
  
**无穷大:**   
  
**NaN(非数字):**   
  
#### 步骤 3: 处理正常值和次正规数  
  
对于正常范围内的值(指数 > 98):    
  
关键处理包括:  
- **次正规数处理**(指数 < -14):右移尾数并调整   
- **舍入到最近偶数**:使用 guard 和 round 位进行舍入   
- **溢出检测**:如果尾数舍入后达到 1024,增加指数   
- **指数调整**:将 float32 的指数偏移(127)转换为 float16 的指数偏移(15)   
  
## IEEE 754 格式对比  
  
转换遵循 IEEE 754 标准:  
  
| 格式 | 符号位 | 指数位 | 尾数位 | 指数偏移 |  
|------|--------|--------|--------|----------|  
| float32 | 1 | 8 | 23 | 127 |  
| float16 | 1 | 5 | 10 | 15 |  
  
## 实际使用场景  
  
这个函数在多个地方被调用:  
- **向量转换**:`vector_to_halfvec()` 中逐维度转换   
- **数组转换**:`array_to_halfvec()` 中处理各种数组类型   
- **聚合函数**:`halfvec_avg()` 中计算平均值后转换   
  
## Notes  
  
手动位操作实现的舍入策略使用"舍入到最近偶数"(round to nearest even),这是 IEEE 754 标准的默认舍入模式,可以减少累积误差。对于超出 float16 表示范围的值(±65504),会被转换为无穷大,并由 `Float4ToHalf()` 的检查逻辑捕获并报错。   
  
---  
  
# 附录3: pgvector 中从 float32 到 float16 的转换是否是**矢量缩放形式的映射**。  
  
## 答案: 不是矢量缩放  
  
**不是**。 这不是矢量缩放(vector scaling)形式的映射,而是**逐元素的浮点数格式转换**。    
  
## 关键区别  
  
### 矢量缩放的特点  
- 所有维度乘以同一个缩放因子  
- 保持向量的方向不变  
- 只改变向量的长度(范数)  
  
### Float32 到 Float16 转换的特点  
- **每个维度独立转换**,不涉及全局缩放因子    
- **遵循 IEEE 754 标准**进行位级别的格式转换    
- 转换过程包括:  
  - 符号位直接复制  
  - 指数偏移调整(127 → 15)  
  - 尾数截断(23位 → 10位)  
  - 舍入到最近偶数  
  
## 实际转换示例  
  
在 `vector_to_halfvec()` 中,每个维度独立转换:    
  
```c  
for (int i = 0; i < vec->dim; i++)  
    result->x[i] = Float4ToHalf(vec->x[i]);  
```  
  
这意味着:  
- 向量 `[1.0, 2.0, 3.0]` 转换时,每个值独立按 IEEE 754 规则转换  
- **不是**计算一个全局缩放因子然后应用到所有维度  
- 不同维度可能有不同的精度损失  
  
## 对向量距离的影响  
  
由于不是统一缩放,转换后的向量:  
- **L2 距离**会受到每个维度独立舍入误差的影响    
- **余弦距离**也会受影响,因为归一化后的方向可能略有变化    
- 误差是由浮点数精度限制导致, 而非缩放操作  
  
## Notes  
  
如果需要矢量缩放,pgvector 提供了 `l2_normalize()` 函数来归一化向量  , 这才是真正的矢量缩放操作。Float32 到 Float16 的转换本质上是**数据类型转换**, 目的是减少存储空间(从 4 字节/维降到 2 字节/维),而不是改变向量的语义表示。  
  
---  
  
  
# 附录4: RaBitQ 的量化方式,并与 pgvector 的 halfvec 量化方式进行对比。  
  
## 重要说明  
  
根据我检索到的代码库上下文, **pgvector 代码库中没有 RaBitQ 相关的实现**。我只能看到 pgvector 自己的量化方式:  
  
1. **halfvec 量化**(float32 → float16):IEEE 754 半精度浮点数转换    
2. **binary_quantize 量化**(float → bit):二值化量化    
  
## pgvector 的二值化量化(Binary Quantization)  
  
pgvector 提供了 `binary_quantize()` 函数,这是一种简单的二值化量化方式:    
  
**量化规则:**  
- 每个维度的值 > 0 → 映射为 1  
- 每个维度的值 ≤ 0 → 映射为 0  
  
**存储:**  
- 每个向量占用 `dimensions / 8 + 8` 字节    
- 使用 PostgreSQL 的 `bit` 类型存储  
  
**使用示例:**    
  
```sql  
-- 创建二值化索引  
CREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);  
  
-- 查询  
SELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 5;  
```  
  
## 与 halfvec 的对比  
  
| 特性 | halfvec | binary_quantize |  
|------|---------|-----------------|  
| **量化方式** | IEEE 754 浮点数格式转换 | 符号位二值化 |  
| **每维度存储** | 2 字节(16 位) | 1 位 |  
| **精度** | ~3-4 位有效数字 | 仅保留符号 |  
| **值域** | ±65,504 | {0, 1} |  
| **是否可逆** | 有损但可近似还原 | 不可逆 |  
| **压缩比** | 50% | 96.875% |  
  
## Notes  
  
RaBitQ 是一种向量量化算法,但它不是 pgvector 的一部分。如果您想了解 RaBitQ 的具体实现,我需要访问包含 RaBitQ 代码的仓库。pgvector 目前提供的量化方式主要是 halfvec(半精度浮点)和 binary_quantize(二值化),两者都是逐维度独立量化,不涉及矢量缩放或聚类中心等复杂量化技术。  
  
# 附录: pgvector 在采用归一化方式量化后, 对几种距离计算带来的影响  
  
## 归一化量化方式  
  
pgvector 提供了 `l2_normalize()` 函数用于向量归一化,将向量标准化为单位长度  。此外,还有 `binary_quantize()` 函数将向量转换为二值化表示 。  
  
## 对各种距离计算的影响  
  
### 1. L2 距离 (`<->`)  
对于归一化向量,L2 距离的计算结果会发生变化 。当向量归一化为单位长度后, L2 距离的范围会被限制在 [0, 2] 之间。  
  
### 2. 内积 (`<#>`)  
**这是最重要的影响**:当向量归一化后,内积等价于余弦相似度 。文档明确建议:如果向量已归一化为单位长度(如 OpenAI embeddings),使用内积可获得最佳性能  。  
  
内积的实现为:   
  
### 3. 余弦距离 (`<=>`)  
余弦距离的计算公式为 `1 - cosine_similarity`,其中相似度通过归一化的内积计算  。对于已归一化的向量,余弦距离计算会更加高效,因为归一化步骤已经完成。  
  
余弦相似度可通过 `1 - (embedding <=> '[3,1,2]')` 获得  。  
  
### 4. L1 距离 (`<+>`)  
L1 距离(曼哈顿距离)的计算不受归一化影响太大,仍然是各维度差值绝对值之和  。  
  
### 5. 二值量化的影响  
使用 `binary_quantize()` 后,向量被转换为二进制表示(每个元素 > 0 为 1,否则为 0) 。这时只能使用汉明距离 (`<~>`) 和 Jaccard 距离 (`<%>`) 进行计算  。  
  
## Notes  
  
归一化主要优化了内积和余弦距离的计算。对于已归一化的向量,内积操作性能最优,因为它避免了余弦距离中的额外归一化计算。二值量化则是一种极端的量化方式,大幅减少存储空间但会损失精度,需要通过重排序(re-ranking)来提高召回率  。  
  
---  
  
# 附录: 将32位(单精度浮点)向量量化为4位存储的影响。  
  
pgvector目前**不支持4位量化**< 。根据代码库, pgvector支持以下量化方式:  
  
## pgvector当前支持的量化方式  
  
### 1. 半精度(16位) - `halfvec`类型  
- **存储大小**: `2 * dimensions + 8` 字节    
- **精度**: 半精度浮点数(16位)    
  
### 2. 二值量化(1位) - `binary_quantize()`  
- **存储大小**: `dimensions / 8 + 8` 字节    
- **量化方式**: 每个元素 > 0 映射为1,否则为0    
  
## 如果假设有4位量化,理论上的权衡分析  
  
### 好处  
  
**1. 存储空间大幅减少**   
- 32位 → 4位: 存储空间减少87.5%  
- 对于1536维向量(如OpenAI embeddings):  
  - 32位: 6,152字节  
  - 4位(理论): ~776字节  
  
**2. 索引大小显著缩小**   
这对于大规模向量数据库尤其重要,可以让更多索引数据驻留在内存中,提升查询性能 。  
  
**3. 查询速度可能提升**   
更小的数据意味着更好的缓存局部性和更少的内存带宽消耗。  
  
### 缺点  
  
**1. 精度损失严重**   
- 4位只能表示16个不同的值(2^4)  
- 相比32位浮点数的约70亿个可能值,信息损失巨大  
- 会显著影响向量相似度计算的准确性  
  
**2. 召回率下降**  
类似于二值量化的问题,需要通过重排序(re-ranking)来提高召回率  :  
```sql  
SELECT * FROM (  
    SELECT * FROM items ORDER BY quantized_embedding <-> query LIMIT 20  
) ORDER BY original_embedding <-> query LIMIT 5;  
```  
  
**3. 距离计算语义改变**   
量化后的向量距离计算结果与原始向量会有较大偏差,可能需要使用特定的距离度量。  
  
## 实际建议  
  
基于pgvector现有实现,您可以考虑:  
  
**1. 使用半精度索引**   
```sql  
CREATE INDEX ON items USING hnsw ((embedding::halfvec(3)) halfvec_l2_ops);  
```   
  
这可以将索引大小减半,同时保持较好的精度。最多支持4,000维  。  
  
**2. 使用二值量化**   
```sql  
CREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);  
```   
  
这可以将存储减少到原来的1/32,但需要配合重排序使用  。最多支持64,000维  。  
  
## Notes  
  
pgvector不支持4位量化。如果您需要在存储空间和精度之间权衡,建议使用`halfvec`(16位)作为折中方案,或使用`binary_quantize`(1位)配合重排序来获得最大的空间节省。对于归一化向量,使用内积距离(`<#>`)可以获得最佳性能 。  
  
---  
  
# 附录: 使用向量缩放方式将32位浮点值映射到8位整数空间, 并相应转换距离计算函数。  
  
需要明确的是, **pgvector目前不支持8位量化** 。从代码库来看,pgvector支持的量化方式包括:  
  
## pgvector当前的量化支持  
  
1. **半精度(16位)** - `halfvec`类型,每个元素2字节    
2. **二值量化(1位)** - `binary_quantize()`函数,将每个元素映射为1位    
  
## 8位量化的理论实现方案  
  
如果要实现您描述的8位量化,需要以下组件:  
  
### 1. 量化函数  
需要实现类似`binary_quantize()`的函数,但映射到8位空间 :  
- 找到向量的最小值和最大值  
- 将`[min, max]`范围线性映射到`[0, 255]`  
- 存储缩放参数(scale和offset)以便反量化  
  
### 2. 新的数据类型  
需要定义类似`halfvec`的新类型来存储8位整数向量, 参考`halfvec`的实现结构  。  
  
### 3. 距离函数适配  
需要为8位类型实现对应的距离计算函数:  
- L2距离、内积、余弦距离、L1距离  
- 可以参考现有的`vector`类型距离函数实现    
  
### 4. 索引支持  
需要为新类型创建HNSW和IVFFlat的operator classes, 类似于`halfvec`的索引支持  。  
  
## 8位量化的优缺点  
  
### 好处  
- **存储减少75%**: 从4字节降到1字节   
- **索引更小**: 可以在内存中容纳更多向量    
- **计算可能更快**: 整数运算通常比浮点运算快   
  
### 缺点  
- **精度损失**: 8位只能表示256个离散值,相比32位浮点的精度损失显著   
- **需要存储缩放参数**: 每个向量需要额外存储scale和offset   
- **召回率下降**: 需要重排序策略    
  
## 当前可用的替代方案  
  
由于pgvector不支持8位量化,建议使用:  
  
**半精度索引** - 减少50%存储,保持较好精度 :  
```sql  
CREATE INDEX ON items USING hnsw ((embedding::halfvec(3)) halfvec_l2_ops);  
```  
  
**二值量化** - 减少96.875%存储,需配合重排序 :  
```sql  
CREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);  
```  
  
## Notes  
  
pgvector的架构支持扩展新的向量类型,如`halfvec`的添加所示 。如果需要8位量化,需要在pgvector源码层面进行扩展开发,实现新的类型、I/O函数、距离计算和索引支持。目前最接近的方案是使用`halfvec`类型 。  
      
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
