## RaBitQ 向量压缩算法简介     
                                                                                      
### 作者                                                                                      
digoal                                                                                      
                                                                                      
### 日期                                                                                      
2025-11-19                                                                                     
                                                                                      
### 标签                                                                                      
向量 , 压缩 , RabitQ                                                                              
                                                                                      
----                                                                                      
                                                                                      
## 背景       
本文深入浅出地介绍 **RaBitQ**（Random Binary Quantization with Theoretical Error Bound）向量压缩算法，包括其核心思想、压缩后的距离计算流程、量化偏差，以及为什么它通常不需要归一化。  
  
---  
  
## 🚀 RaBitQ 向量压缩算法简介  
  
RaBitQ 是一种用于高维向量（如嵌入向量）压缩的算法，其主要目标是实现**高压缩率**的同时，保证**近似最近邻搜索 (ANN)** 的**准确性**和**理论可控的误差**。它属于**二值量化**（Binary Quantization, BQ）的范畴。  
  
### 核心思想  
  
1.  **随机投影 (Random Projection) 与二值化：**  
    * RaBitQ 首先对原始高维向量 $\mathbf{x} \in \mathbb{R}^D$ 执行一系列**随机线性投影** $P$ ，将向量投影到一个较低维度的空间 $L$ 。  
    * 接着，通过一个**二值化函数**（ 通常是符号函数 $\text{sgn}$ ），将投影后的结果转换为**二进制码** $\mathbf{b} \in \{0, 1\}^L$ 或 $\{-1, 1\}^L$ 。  
    * $$\mathbf{b} = \text{sgn}(\mathbf{P}^T \mathbf{x})$$  
    * 其中， $\mathbf{P}$ 是一个随机矩阵，其元素通常从标准正态分布中采样。  
  
2.  **理论误差界限 (Theoretical Error Bound)：**  
    * 传统二值量化方法的缺点在于缺乏理论上的误差保证，可能导致在某些数据集上搜索精度急剧下降。  
    * **RaBitQ 的创新点**在于它利用了随机投影的性质（如 Johnson-Lindenstrauss 引理的变体）和特定的**无偏距离估计器**，来保证量化后的距离估计与原始距离之间的**理论误差界限**。这意味着算法能更好地控制量化带来的精度损失。  
  
3.  **无偏估计器 (Unbiased Estimator)：**  
    * 为了在压缩的二进制码上可靠地估计原始向量间的距离，RaBitQ 引入了一个**无偏估计器**。这个估计器可以有效地利用二值码中的信息来近似原始空间中的距离（通常是内积或欧氏距离），并最大限度地减少重构误差。  
  
---  
  
## 📏 压缩后向量距离计算流程  
  
在 RaBitQ 中，向量 $\mathbf{x}$ 被压缩成一个**二进制码 $\mathbf{b}$**。对于查询向量 $\mathbf{q}$ 和数据库中的一个向量 $\mathbf{x}$ ，我们不再计算它们原始的距离 $d(\mathbf{q}, \mathbf{x})$ ，而是计算它们二进制码之间的**近似距离** $d(\mathbf{b}_q, \mathbf{b}_x)$ 。  
  
### 1. **计算二值码之间的距离**  
  
由于是二值码，距离计算可以被**极大地加速**，通常使用**汉明距离 (Hamming Distance)** 或基于汉明距离的内积估计。  
  
假设二进制码 $\mathbf{b}_q, \mathbf{b}_x \in \{-1, 1\}^L$ ：  
  
* **汉明距离** $H(\mathbf{b}_q, \mathbf{b}_x)$ ：是两个码中**不同位的数量**。  
    * $$H(\mathbf{b}_q, \mathbf{b}_x) = \frac{1}{2} \sum_{i=1}^L |b_{q,i} - b_{x,i}| = \text{位数不同数量}$$  
    * 这可以通过高效的**位运算**（如 XOR 操作后计算设置位 (popcount)）来完成，速度极快。  
  
* **内积估计** $\hat{s}(\mathbf{q}, \mathbf{x})$ ：  
  
    RaBitQ 主要关注通过二值码来**估计原始向量的内积**（内积与角度和欧氏距离都有关）。  
  
    首先计算两个二值码的**点积**（或相关性）：  
    $$\text{Corr}(\mathbf{b}_q, \mathbf{b}_x) = \sum_{i=1}^L b_{q,i} b_{x,i}$$  
    这个相关性与汉明距离有直接关系： $\text{Corr} = L - 2H$ 。  
  
### 2. **使用无偏估计器估计原始距离**  
  
RaBitQ 使用这个二值码相关性来估计原始向量的**内积** $\langle \mathbf{q}, \mathbf{x} \rangle$ ，这通常涉及到一种**非线性变换** $\phi$ （例如 $\phi(z) = \sin(\frac{\pi}{2} z)$ 或其他更复杂的函数）来校正二值化带来的偏差：  
  
* $$\hat{s}(\mathbf{q}, \mathbf{x}) = \text{Bias\_Correction} \cdot \phi \left( \frac{1}{L} \sum_{i=1}^L b_{q,i} b_{x,i} \right)$$  
    * 其中， $\frac{1}{L} \sum b_{q,i} b_{x,i}$ 是二值码的平均相关性。  
    * 这个估计器 $\hat{s}$ 是**无偏**的，即它的期望值等于原始的内积（或距离度量），且其方差（即估计的波动性）有理论界限。  
  
通过这种方式，RaBitQ 在**牺牲少量精度**的前提下，实现了**原始距离**在**高压缩率**下的**快速、可控误差**的近似计算。  
  
---  
  
## 📉 量化后与量化前计算结果的偏差  
  
**偏差**（或**量化误差**）是量化后的近似距离 $\hat{d}$ 与原始距离 $d$ 之间的差异： $| \hat{d} - d |$ 。  
  
1.  **系统误差 (Systematic Error) 或偏差 (Bias):**  
    * 二值量化本身是一个**非线性**、**有损**的过程，通常会引入系统性的偏差。  
    * **RaBitQ 的关键优势**在于，它所使用的**距离估计器** $\hat{s}$ 是一个**无偏估计器**。  
    * **无偏性**意味着： $$E[\hat{s}(\mathbf{q}, \mathbf{x})] = \langle \mathbf{q}, \mathbf{x} \rangle$$  
    * 即，在**多次随机投影和量化**的平均意义上，估计值是准确的。对于单一的量化结果，系统偏差被有效消除。  
  
2.  **随机误差 (Random Error) 或方差 (Variance):**  
    * 虽然偏差被消除，但每次量化得到的估计值 $\hat{s}$ 仍然会围绕真实值 $\langle \mathbf{q}, \mathbf{x} \rangle$ 波动。这种波动就是随机误差或方差。  
    * RaBitQ 提供了对这种**随机误差的理论界限**：  
        * $$P(|\hat{s}(\mathbf{q}, \mathbf{x}) - \langle \mathbf{q}, \mathbf{x} \rangle| > \epsilon) \leq f(\epsilon, L, D)$$  
        * 这个界限表明，随着**码长 $L$ 的增加**（压缩率降低），估计的误差 $\epsilon$ 会**指数级下降**。  
    * 这个理论保证了**误差是可控的**，并且通过选择合适的码长 $L$ ，可以保证搜索的**召回率 (Recall)** 达到预期的水平。  
  
---  
  
## ❓ 为什么不需要归一化？  
  
在许多向量搜索场景中，**归一化 (Normalization)**（即将向量长度调整为 1）是非常常见且重要的一步。然而，在 **RaBitQ** 的特定设计下，它**可以不需要**对原始向量进行**显式的长度归一化**，原因如下：  
  
### 1. 距离度量和内积的关系  
  
* **内积 ( $\langle \mathbf{q}, \mathbf{x} \rangle$ )**：与向量的**长度**和它们之间的**夹角**都有关。  
* **余弦相似度 ( $\text{cos\_sim}$ )**：只与夹角有关，计算时需要**归一化**：  
    $$\text{cos\_sim}(\mathbf{q}, \mathbf{x}) = \frac{\langle \mathbf{q}, \mathbf{x} \rangle}{\|\mathbf{q}\| \|\mathbf{x}\|}$$  
  
### 2. RaBitQ 的处理方式（处理长度信息）   
  
RaBitQ 的无偏估计器设计可以**同时估计原始向量的内积** $\langle \mathbf{q}, \mathbf{x} \rangle$ ，以及**向量的长度**（或它们长度的乘积）。  
  
* **长度分离：** RaBitQ 通常将**向量的长度信息**（ $||\mathbf{x}||$ ） **单独存储**，而不对其进行二值量化。  
* **距离重构：** 在计算相似度时，可以利用量化得到的**内积估计** $\hat{s}(\mathbf{q}, \mathbf{x})$ 和单独存储的**长度信息**，来重构出任何基于内积的度量：  
    * **余弦相似度估计**： $$\widehat{\text{cos\_sim}}(\mathbf{q}, \mathbf{x}) = \frac{\hat{s}(\mathbf{q}, \mathbf{x})}{\|\mathbf{q}\| \|\mathbf{x}\|}$$  
  
**总结：** RaBitQ 采用的是一种**混合策略**——对**方向信息**（内积或夹角信息）进行**高效的二值量化**，而对**长度信息**进行**精确（非量化）或低损耗的存储**。这避免了在量化前强制归一化带来的信息丢失，同时保留了计算余弦相似度的能力。  
  
## 可不可以这么理解: RaBitQ既存储原始向量, 也存储量化后的bit, 计算相似度时主要依赖bit, 极少用到原始向量  
  
这是一个**很好的问题**，并且您的理解方向**基本正确，但需要更精确地界定** RaBitQ 在“存储”和“使用”上的策略。  
  
  
## 🎯 对 RaBitQ 存储和使用策略的精确定位  
  
您的理解可以概括为：**"RaBitQ 是一种牺牲存储少量额外信息来保证高精度的高级量化方法。"**  
  
| 方面 | 您的理解 (RaBitQ 既存储原始向量，也存储量化后的bit) | 精确策略 |  
| :--- | :--- | :--- |  
| **存储 (Storage)** | **部分正确。** RaBitQ **不存储完整的原始向量**，否则就失去了压缩的意义。它存储的是：<br>1. **量化后的二进制码 (Bits):** 核心压缩数据，用于快速检索。<br>2. **原始向量的长度 (Norms):** 长度信息是**额外存储的少量非量化数据**，用于距离校正。 | 存储：**二进制码 + 向量长度 (Norm)**。**不存储原始高维向量**，除非作为回退或验证步骤。 |  
| **相似度计算 (Similarity)** | **主要依赖 bit，极少用到原始向量。** | **完全正确。** 计算过程是：<br>1. **粗略/近似计算：** 仅依赖 **二进制码**，通过位运算（汉明距离）快速计算近似内积。<br>2. **精确校正：** 使用 **二进制码的计算结果** 和 **存储的长度信息 (Norms)** 来精确估计原始的余弦相似度或欧氏距离。 |  
| **原始向量的使用** | 极少用到。 | **只有在以下情况可能需要：**<br>1. **离线构建:** 在构建索引（生成随机投影矩阵和量化码）时需要使用原始向量。<br>2. **重排/回退:** 在召回了少量候选结果后，可能用原始向量对这些候选结果进行**精确重排 (Re-ranking)**，但这属于搜索流程的一部分，而非 RaBitQ 存储的策略。 |  
  
-----  
  
### 📚 总结您的理解和精确策略  
  
您的核心理解是：**计算相似度主要依赖压缩数据（bit）** ，这是完全正确的，也是 RaBitQ 实现**加速**和**压缩**的核心。  
  
**最精确的理解应该是：**  
  
> **RaBitQ 存储的是原始高维向量的“压缩表示”：即一个高压缩率的**二进制码** （用于方向/角度信息）和一个 **低维度的长度标量** （用于长度信息）。相似度计算 **完全基于** 这些存储的压缩表示（二进制码 + 长度标量），**不需要**完整的原始高维向量。  
    
    
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
