## 大学生数据库实践课: 9 embedding 文本切分实操      
                  
### 作者                  
digoal                  
                  
### 日期                  
2025-12-10                  
                  
### 标签                  
PostgreSQL , DuckDB , 语义搜索 , 向量搜索 , 关键词检索 , 全文检索 , 标量检索 , 混合搜索 , 多模态搜索 , 重排序 , embedding , document split , ORC , 大模型 , 多模态大模型 , Dify , 压力测试 , 索引优化 , 倒排向量索引 , Ivfflat , 图向量索引 , HNSW , DiskANN , 量化 , rabitq , sbq , 二值量化 , bm25 , 相关性 , 相似性 , 召回率 , pgbench , 数据集 , ann-benchmarks , 图搜索 , PGQ , 递归 , 深度优先 , 广度优先 , 最短路径 , N度关系 , LLM , Ollama                  
                  
----                  
                  
## 背景                  
  
postgresml deepwiki:  
  
https://deepwiki.com/search/-1-2-3-4embedding-5embedding-t_f270b6c6-dce9-452e-847a-6cbace2e2aff?mode=fast  
  
面向大学生通俗的给出下面内容的介绍和实操:    
1、文本切分的方法以及各自的优缺点介绍;  
2、进行文本切分之前, 需要配置哪些依赖项;  
3、每种文本切分的实操;  
4、切分后的文件进行embedding;  
5、基于embedding数据进行语义搜索, 并返回TOP-N的原始内容片段;  
    
-----  
  
# 文本切分与语义搜索实操指南  
  
## 概述  
  
PostgresML提供了完整的文本处理和语义搜索解决方案，通过Korvus SDK可以轻松实现文本切分、向量化嵌入和相似度搜索  。  
  
  
  
## 1. 文本切分方法及优缺点  
  
### 主要切分方法  
  
**Recursive Character Splitter（递归字符切分器）**  
- 优点：保持段落完整性，支持重叠切分，适合大多数文档  
- 缺点：可能破坏句子结构  
- 适用场景：长文档、文章、书籍章节    
  
**其他切分器**  
- Token-based：基于词汇切分，适合处理代码  
- Semantic：基于语义切分，保持概念完整性  
  
### 配置参数  
- `chunk_size`：切分块大小（字符数）  
- `chunk_overlap`：块间重叠大小，确保上下文连续性  
  
  
  
## 2. 依赖项配置  
  
### 环境要求  
- PostgresML数据库（版本 >= 2.7.7）    
- Python >= 3.8.1 或 Node.js  
- pgvector扩展（用于向量索引）  
  
### 安装SDK  
```bash  
# Python  
pip install pgml  
  
# JavaScript    
npm install korvus  
```  
  
### 环境变量  
```bash  
export DATABASE_URL="postgresql://user:pass@host:port/db"  
```  
  
  
  
## 3. 文本切分实操  
  
### Python实现  
  
```python  
from korvus import Collection, Pipeline  
import asyncio  
  
# 创建管道配置  
pipeline = Pipeline(  
    "v1",  
    {  
        "text": {  
            "splitter": {  
                "model": "recursive_character",  
                "parameters": {  
                    "chunk_size": 1500,  
                    "chunk_overlap": 40  
                }  
            },  
            "semantic_search": {  
                "model": "Alibaba-NLP/gte-base-en-v1.5"  
            }  
        }  
    }  
)  
  
async def main():  
    collection = Collection("demo")  
    await collection.add_pipeline(pipeline)  
      
    # 上传文档（自动切分）  
    documents = [  
        {  
            "id": "1",   
            "text": "长文本内容..."  
        }  
    ]  
    await collection.upsert_documents(documents)  
```    
  
### JavaScript实现  
  
```javascript  
const korvus = require("korvus");  
  
const pipeline = korvus.newPipeline("v1", {  
  text: {  
    splitter: {   
      model: "recursive_character"   
    },  
    semantic_search: {  
      model: "mixedbread-ai/mxbai-embed-large-v1"  
    }  
  }  
});  
```    
  
  
  
## 4. 文本嵌入（Embedding）  
  
### 使用pgml.embed函数  
  
```sql  
-- 直接生成嵌入向量  
SELECT pgml.embed('Alibaba-NLP/gte-base-en-v1.5', 'passage: 文本内容');  
```    
  
### 批量处理  
  
```sql  
-- 为表中的所有文本生成嵌入  
UPDATE documents   
SET embedding = pgml.embed('mixedbread-ai/mxbai-embed-large-v1', text);  
```  
  
### 常用嵌入模型  
- `Alibaba-NLP/gte-base-en-v1.5`：通用英文模型  
- `mixedbread-ai/mxbai-embed-large-v1`：高质量多语言模型  
- `intfloat/e5-small-v2`：需要passage:前缀    
  
  
  
## 5. 语义搜索与TOP-N结果  
  
### 向量相似度搜索  
  
```python  
# 执行语义搜索  
results = await collection.vector_search(  
    {  
        "query": {  
            "fields": {  
                "text": {  
                    "query": "搜索问题",  
                    "parameters": {  
                        "prompt": "Represent this sentence for searching relevant passages: "  
                    }  
                }  
            }  
        },  
        "limit": 5  # 返回TOP-5结果  
    },  
    pipeline  
)  
```    
  
### SQL实现  
  
```sql  
-- 使用余弦相似度搜索  
WITH embedded_query AS (  
    SELECT pgml.embed('mixedbread-ai/mxbai-embed-large-v1', '搜索问题')::vector embedding  
)  
SELECT   
    text,  
    embedding <=> (SELECT embedding FROM embedded_query) as cosine_distance  
FROM documents  
ORDER BY cosine_distance  
LIMIT 5;  
```    
  
### 性能优化  
  
创建HNSW索引加速搜索：  
```sql  
CREATE INDEX ON documents   
USING hnsw (embedding vector_cosine_ops);  
```    
  
  
## 完整示例流程  
  
```python  
from korvus import Collection, Pipeline  
import asyncio  
  
async def semantic_search_pipeline():  
    # 1. 初始化  
    collection = Collection("knowledge_base")  
      
    # 2. 配置切分和嵌入管道  
    pipeline = Pipeline(  
        "v1",  
        {  
            "text": {  
                "splitter": {"model": "recursive_character"},  
                "semantic_search": {"model": "mixedbread-ai/mxbai-embed-large-v1"}  
            }  
        }  
    )  
      
    # 3. 添加管道并上传文档  
    await collection.add_pipeline(pipeline)  
    documents = [{"id": "1", "text": "文档内容..."}]  
    await collection.upsert_documents(documents)  
      
    # 4. 执行搜索  
    results = await collection.vector_search(  
        {"query": {"fields": {"text": {"query": "搜索问题"}}}, "limit": 3},  
        pipeline  
    )  
      
    return results  
  
# 运行  
results = asyncio.run(semantic_search_pipeline())  
```    
  
  
  
## Notes  
  
- 文本切分是RAG系统的关键步骤，合理的chunk_size和overlap设置影响检索效果  
- PostgresML支持多种开源嵌入模型，可根据需求选择    
- 向量索引（HNSW）对大规模数据集的性能至关重要  
- Korvus SDK简化了整个流程，自动处理切分、嵌入和搜索的复杂性
  
  
---
  
# 简化版实操
这是一个使用 Python 编写的灵活脚本。它集成了 **LangChain** 的文本切分器（支持字符切分和递归字符切分）以及 **Ollama** 的 API。

### 准备工作
  
在运行脚本前，请确保：  

1. 已安装 Ollama 并下载模型：`ollama pull qwen3-embedding:0.6b`
2. 安装必要的 Python 库：
```bash
pip install langchain_text_splitters requests
```
   

### Python 脚本：`embed_split.py`

```python
import json
import argparse
import requests
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter

def get_embedding(text, model_name):
    """调用 Ollama 本地 API 获取 embedding"""
    url = "http://localhost:11434/api/embeddings"
    payload = {
        "model": model_name,
        "prompt": text
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']
    except Exception as e:
        return f"Error: {str(e)}"

def main():
    parser = argparse.ArgumentParser(description="文件切分并计算 Embedding 脚本")
    
    # 基本参数
    parser.add_argument("--file", required=True, help="本地文件路径")
    parser.add_argument("--model", default="qwen3-embedding:0.6b", help="Ollama 模型名称")
    
    # 切分方法参数
    parser.add_argument("--method", choices=['char', 'recursive'], default='recursive', 
                        help="切分方法: char (固定字符), recursive (递归字符/推荐)")
    parser.add_argument("--chunk_size", type=int, default=500, help="每个分片的字符长度")
    parser.add_argument("--chunk_overlap", type=int, default=50, help="相邻分片的重叠长度")

    args = parser.parse_args()

    # 1. 读取文件
    try:
        with open(args.file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"读取文件失败: {e}")
        return

    # 2. 选择切分器
    if args.method == 'char':
        splitter = CharacterTextSplitter(
            separator="", 
            chunk_size=args.chunk_size, 
            chunk_overlap=args.chunk_overlap
        )
    else:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=args.chunk_size, 
            chunk_overlap=args.chunk_overlap,
            separators=["\n\n", "\n", "。", "！", "？", " ", ""]
        )

    chunks = splitter.split_text(content)
    print(f"--- 文件已切分为 {len(chunks)} 份 ---\n")

    # 3. 计算并输出结果
    results = []
    for i, chunk in enumerate(chunks):
        print(f"正在处理第 {i+1}/{len(chunks)} 个分片...")
        embedding = get_embedding(chunk, args.model)
        
        result_item = {
            "id": i + 1,
            "content": chunk,
            "embedding_length": len(embedding) if isinstance(embedding, list) else 0,
            "embedding": embedding
        }
        results.append(result_item)

    # 4. 打印或保存结果 (这里演示前50个字符和embedding的前5维)
    print("\n" + "="*50)
    for item in results:
        print(f"ID: {item['id']}")
        print(f"内容预览: {item['content'][:50].replace('\\n', ' ')}...")
        if isinstance(item['embedding'], list):
            print(f"Embedding (前5维): {item['embedding'][:5]}...")
        else:
            print(f"Embedding 错误: {item['embedding']}")
        print("-" * 30)

    # 可选：保存到 JSON 文件
    # with open("output.json", "w", encoding="utf-8") as f:
    #     json.dump(results, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
```
   

### 如何使用该脚本？

你可以通过命令行传递不同的参数：

#### 1. 使用默认参数（递归切分）

```bash
python embed_split.py --file test.txt
```
  
#### 2. 指定模型和切分块大小

如果你想切得更碎（比如每个块 200 字符），并使用特定的模型：

```bash
python embed_split.py --file my_doc.md --model qwen3-embedding:0.6b --chunk_size 200 --chunk_overlap 20
```
  
#### 3. 使用固定字符切分 (Char Method)

```bash
python embed_split.py --file info.txt --method char --chunk_size 1000
```
  
   

### 参数详解：

* `--method`:
* `recursive`: 智能切分，优先在段落和句子处切断，保证语义完整性。
* `char`: 简单粗暴地按字符数硬切。
  
  
* `--chunk_size`: 每个文本块的最大长度。
* `--chunk_overlap`: 块与块之间的重叠部分。这很重要，可以防止语义在切分点被强行掐断（上下文保留）。
* `--model`: 只要你本地 `ollama list` 里有的模型都可以运行。

    
    
---  

**要义图示**  
  

### 1. 文本处理与搜索全生命周期

这张图展示了从原始文档进入系统，到最后返回搜索结果的完整闭环。

```mermaid
graph TD
    subgraph Data_Ingestion [数据入库阶段]
        Doc[原始文档] --> Splitter[文本切分 Splitter]
        Splitter --> Chunks[文本块 Chunks]
        Chunks --> Embedding[嵌入模型 Embedding]
        Embedding --> VectorDB[(PostgreSQL + pgvector)]
    end

    subgraph Query_Stage [检索阶段]
        UserQ[用户提问] --> Q_Emb[提问向量化]
        Q_Emb --> Search[向量相似度搜索]
        VectorDB --> Search
        Search --> TopN[返回 Top-N 片段]
    end

    style Splitter fill:#f96,stroke:#333
    style Embedding fill:#f96,stroke:#333
```
  

### 2. 实操环境依赖树

在开始实操前，请确保以下“基础设施”已配置妥当。

```mermaid
graph BT
    SDK[Korvus SDK / pgml] --> Python[Python 3.8+ / Node.js]
    Python --> PGML[PostgresML 2.7.7+]
    PGML --> PG[PostgreSQL 核心]
    PG --> PGV[pgvector 扩展]
    
    ENV[DATABASE_URL 环境变量] -.-> SDK
```

  

### 3. 递归字符切分器 (Recursive Character Splitter) 逻辑

通过这张图，你可以理解 `chunk_size` 和 `chunk_overlap` 是如何协同工作的。

```mermaid
graph LR
    subgraph Chunk_1 [片段 1]
        T1[文本内容 A...]
    end
    
    subgraph Overlap [重叠区域 Overlap]
        O1[...重复文本...]
    end
    
    subgraph Chunk_2 [片段 2]
        T2[...文本内容 B]
    end
    
    T1 --- O1
    O1 --- T2

    %% 模拟“跨子图注释”
    Note["重叠确保了语义的连续性，防止上下文被切断"]:::annotation

    %% 可选：用虚线连接到重叠区域（示意关联）
    Overlap -.-> Note

    classDef annotation fill:#e6f7ff,stroke:#1890ff,stroke-dasharray: 5 5, color:#1890ff, font-size:13px, font-style:italic;
```

  

### 4. 自动化管道 (Pipeline) 工作流程

使用 Korvus SDK 时，管道会自动处理切分和向量化。

```mermaid
flowchart LR
    Docs[文档列表] --> Pipeline{Pipeline 配置}
    
    subgraph Pipeline_Config [管道内部自动化]
        direction TB
        S[Splitter: recursive_character]
        M[Model: gte-base / mxbai]
    end
    
    Pipeline --> S --> M --> DB[(Vector Database)]
```

  

### 5. 语义搜索 SQL/代码 执行逻辑

这张图解释了 TOP-N 搜索时，“余弦距离”是如何决定排名先后顺序的。

```mermaid
sequenceDiagram
    participant U as 用户查询
    participant E as Embedding 函数
    participant V as 向量索引 (HNSW)
    participant R as 结果集

    U->>E: "搜索问题"
    E->>V: 转换为向量 [0.12, -0.05, ...]
    V->>V: 执行 <=> (余弦距离计算)
    V->>R: 按照距离从小到大排序 (ORDER BY)
    R->>U: 返回前 N 条原始文本 (LIMIT N)
```

  

### 实验小贴士（给同学们的避坑指南）：

1. **关于 pgvector**：在 SQL 中，`<=>` 代表余弦距离，距离越**小**表示语义越**接近**。
2. **HNSW 索引**：当你的数据量超过 1 万条时，一定要执行 `CREATE INDEX ... USING hnsw`，否则搜索速度会从“毫秒级”掉到“秒级”。
3. **模型选择**：
    * **英文推荐**：`Alibaba-NLP/gte-base-en-v1.5`。
    * **中英混合推荐**：`mixedbread-ai/mxbai-embed-large-v1`。
4. **环境变量**：实操时如果遇到 `Connection Refused`，多半是 `DATABASE_URL` 没有正确 `export` 到当前终端窗口。
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
