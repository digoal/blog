## å¤§å­¦ç”Ÿæ•°æ®åº“å®è·µè¯¾: 9 embedding æ–‡æœ¬åˆ‡åˆ†å®æ“      
                  
### ä½œè€…                  
digoal                  
                  
### æ—¥æœŸ                  
2025-12-10                  
                  
### æ ‡ç­¾                  
PostgreSQL , DuckDB , è¯­ä¹‰æœç´¢ , å‘é‡æœç´¢ , å…³é”®è¯æ£€ç´¢ , å…¨æ–‡æ£€ç´¢ , æ ‡é‡æ£€ç´¢ , æ··åˆæœç´¢ , å¤šæ¨¡æ€æœç´¢ , é‡æ’åº , embedding , document split , ORC , å¤§æ¨¡å‹ , å¤šæ¨¡æ€å¤§æ¨¡å‹ , Dify , å‹åŠ›æµ‹è¯• , ç´¢å¼•ä¼˜åŒ– , å€’æ’å‘é‡ç´¢å¼• , Ivfflat , å›¾å‘é‡ç´¢å¼• , HNSW , DiskANN , é‡åŒ– , rabitq , sbq , äºŒå€¼é‡åŒ– , bm25 , ç›¸å…³æ€§ , ç›¸ä¼¼æ€§ , å¬å›ç‡ , pgbench , æ•°æ®é›† , ann-benchmarks , å›¾æœç´¢ , PGQ , é€’å½’ , æ·±åº¦ä¼˜å…ˆ , å¹¿åº¦ä¼˜å…ˆ , æœ€çŸ­è·¯å¾„ , Nåº¦å…³ç³» , LLM , Ollama                  
                  
----                  
                  
## èƒŒæ™¯                  
  
postgresml deepwiki:  
  
https://deepwiki.com/search/-1-2-3-4embedding-5embedding-t_f270b6c6-dce9-452e-847a-6cbace2e2aff?mode=fast  
  
é¢å‘å¤§å­¦ç”Ÿé€šä¿—çš„ç»™å‡ºä¸‹é¢å†…å®¹çš„ä»‹ç»å’Œå®æ“:    
1ã€æ–‡æœ¬åˆ‡åˆ†çš„æ–¹æ³•ä»¥åŠå„è‡ªçš„ä¼˜ç¼ºç‚¹ä»‹ç»;  
2ã€è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†ä¹‹å‰, éœ€è¦é…ç½®å“ªäº›ä¾èµ–é¡¹;  
3ã€æ¯ç§æ–‡æœ¬åˆ‡åˆ†çš„å®æ“;  
4ã€åˆ‡åˆ†åçš„æ–‡ä»¶è¿›è¡Œembedding;  
5ã€åŸºäºembeddingæ•°æ®è¿›è¡Œè¯­ä¹‰æœç´¢, å¹¶è¿”å›TOP-Nçš„åŸå§‹å†…å®¹ç‰‡æ®µ;  
    
-----  
  
# æ–‡æœ¬åˆ‡åˆ†ä¸è¯­ä¹‰æœç´¢å®æ“æŒ‡å—  
  
## æ¦‚è¿°  
  
PostgresMLæä¾›äº†å®Œæ•´çš„æ–‡æœ¬å¤„ç†å’Œè¯­ä¹‰æœç´¢è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡Korvus SDKå¯ä»¥è½»æ¾å®ç°æ–‡æœ¬åˆ‡åˆ†ã€å‘é‡åŒ–åµŒå…¥å’Œç›¸ä¼¼åº¦æœç´¢  ã€‚  
  
  
  
## 1. æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•åŠä¼˜ç¼ºç‚¹  
  
### ä¸»è¦åˆ‡åˆ†æ–¹æ³•  
  
**Recursive Character Splitterï¼ˆé€’å½’å­—ç¬¦åˆ‡åˆ†å™¨ï¼‰**  
- ä¼˜ç‚¹ï¼šä¿æŒæ®µè½å®Œæ•´æ€§ï¼Œæ”¯æŒé‡å åˆ‡åˆ†ï¼Œé€‚åˆå¤§å¤šæ•°æ–‡æ¡£  
- ç¼ºç‚¹ï¼šå¯èƒ½ç ´åå¥å­ç»“æ„  
- é€‚ç”¨åœºæ™¯ï¼šé•¿æ–‡æ¡£ã€æ–‡ç« ã€ä¹¦ç±ç« èŠ‚    
  
**å…¶ä»–åˆ‡åˆ†å™¨**  
- Token-basedï¼šåŸºäºè¯æ±‡åˆ‡åˆ†ï¼Œé€‚åˆå¤„ç†ä»£ç   
- Semanticï¼šåŸºäºè¯­ä¹‰åˆ‡åˆ†ï¼Œä¿æŒæ¦‚å¿µå®Œæ•´æ€§  
  
### é…ç½®å‚æ•°  
- `chunk_size`ï¼šåˆ‡åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰  
- `chunk_overlap`ï¼šå—é—´é‡å å¤§å°ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿ç»­æ€§  
  
  
  
## 2. ä¾èµ–é¡¹é…ç½®  
  
### ç¯å¢ƒè¦æ±‚  
- PostgresMLæ•°æ®åº“ï¼ˆç‰ˆæœ¬ >= 2.7.7ï¼‰    
- Python >= 3.8.1 æˆ– Node.js  
- pgvectoræ‰©å±•ï¼ˆç”¨äºå‘é‡ç´¢å¼•ï¼‰  
  
### å®‰è£…SDK  
```bash  
# Python  
pip install pgml  
  
# JavaScript    
npm install korvus  
```  
  
### ç¯å¢ƒå˜é‡  
```bash  
export DATABASE_URL="postgresql://user:pass@host:port/db"  
```  
  
  
  
## 3. æ–‡æœ¬åˆ‡åˆ†å®æ“  
  
### Pythonå®ç°  
  
```python  
from korvus import Collection, Pipeline  
import asyncio  
  
# åˆ›å»ºç®¡é“é…ç½®  
pipeline = Pipeline(  
    "v1",  
    {  
        "text": {  
            "splitter": {  
                "model": "recursive_character",  
                "parameters": {  
                    "chunk_size": 1500,  
                    "chunk_overlap": 40  
                }  
            },  
            "semantic_search": {  
                "model": "Alibaba-NLP/gte-base-en-v1.5"  
            }  
        }  
    }  
)  
  
async def main():  
    collection = Collection("demo")  
    await collection.add_pipeline(pipeline)  
      
    # ä¸Šä¼ æ–‡æ¡£ï¼ˆè‡ªåŠ¨åˆ‡åˆ†ï¼‰  
    documents = [  
        {  
            "id": "1",   
            "text": "é•¿æ–‡æœ¬å†…å®¹..."  
        }  
    ]  
    await collection.upsert_documents(documents)  
```    
  
### JavaScriptå®ç°  
  
```javascript  
const korvus = require("korvus");  
  
const pipeline = korvus.newPipeline("v1", {  
  text: {  
    splitter: {   
      model: "recursive_character"   
    },  
    semantic_search: {  
      model: "mixedbread-ai/mxbai-embed-large-v1"  
    }  
  }  
});  
```    
  
  
  
## 4. æ–‡æœ¬åµŒå…¥ï¼ˆEmbeddingï¼‰  
  
### ä½¿ç”¨pgml.embedå‡½æ•°  
  
```sql  
-- ç›´æ¥ç”ŸæˆåµŒå…¥å‘é‡  
SELECT pgml.embed('Alibaba-NLP/gte-base-en-v1.5', 'passage: æ–‡æœ¬å†…å®¹');  
```    
  
### æ‰¹é‡å¤„ç†  
  
```sql  
-- ä¸ºè¡¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬ç”ŸæˆåµŒå…¥  
UPDATE documents   
SET embedding = pgml.embed('mixedbread-ai/mxbai-embed-large-v1', text);  
```  
  
### å¸¸ç”¨åµŒå…¥æ¨¡å‹  
- `Alibaba-NLP/gte-base-en-v1.5`ï¼šé€šç”¨è‹±æ–‡æ¨¡å‹  
- `mixedbread-ai/mxbai-embed-large-v1`ï¼šé«˜è´¨é‡å¤šè¯­è¨€æ¨¡å‹  
- `intfloat/e5-small-v2`ï¼šéœ€è¦passage:å‰ç¼€    
  
  
  
## 5. è¯­ä¹‰æœç´¢ä¸TOP-Nç»“æœ  
  
### å‘é‡ç›¸ä¼¼åº¦æœç´¢  
  
```python  
# æ‰§è¡Œè¯­ä¹‰æœç´¢  
results = await collection.vector_search(  
    {  
        "query": {  
            "fields": {  
                "text": {  
                    "query": "æœç´¢é—®é¢˜",  
                    "parameters": {  
                        "prompt": "Represent this sentence for searching relevant passages: "  
                    }  
                }  
            }  
        },  
        "limit": 5  # è¿”å›TOP-5ç»“æœ  
    },  
    pipeline  
)  
```    
  
### SQLå®ç°  
  
```sql  
-- ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢  
WITH embedded_query AS (  
    SELECT pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'æœç´¢é—®é¢˜')::vector embedding  
)  
SELECT   
    text,  
    embedding <=> (SELECT embedding FROM embedded_query) as cosine_distance  
FROM documents  
ORDER BY cosine_distance  
LIMIT 5;  
```    
  
### æ€§èƒ½ä¼˜åŒ–  
  
åˆ›å»ºHNSWç´¢å¼•åŠ é€Ÿæœç´¢ï¼š  
```sql  
CREATE INDEX ON documents   
USING hnsw (embedding vector_cosine_ops);  
```    
  
  
## å®Œæ•´ç¤ºä¾‹æµç¨‹  
  
```python  
from korvus import Collection, Pipeline  
import asyncio  
  
async def semantic_search_pipeline():  
    # 1. åˆå§‹åŒ–  
    collection = Collection("knowledge_base")  
      
    # 2. é…ç½®åˆ‡åˆ†å’ŒåµŒå…¥ç®¡é“  
    pipeline = Pipeline(  
        "v1",  
        {  
            "text": {  
                "splitter": {"model": "recursive_character"},  
                "semantic_search": {"model": "mixedbread-ai/mxbai-embed-large-v1"}  
            }  
        }  
    )  
      
    # 3. æ·»åŠ ç®¡é“å¹¶ä¸Šä¼ æ–‡æ¡£  
    await collection.add_pipeline(pipeline)  
    documents = [{"id": "1", "text": "æ–‡æ¡£å†…å®¹..."}]  
    await collection.upsert_documents(documents)  
      
    # 4. æ‰§è¡Œæœç´¢  
    results = await collection.vector_search(  
        {"query": {"fields": {"text": {"query": "æœç´¢é—®é¢˜"}}}, "limit": 3},  
        pipeline  
    )  
      
    return results  
  
# è¿è¡Œ  
results = asyncio.run(semantic_search_pipeline())  
```    
  
  
  
## Notes  
  
- æ–‡æœ¬åˆ‡åˆ†æ˜¯RAGç³»ç»Ÿçš„å…³é”®æ­¥éª¤ï¼Œåˆç†çš„chunk_sizeå’Œoverlapè®¾ç½®å½±å“æ£€ç´¢æ•ˆæœ  
- PostgresMLæ”¯æŒå¤šç§å¼€æºåµŒå…¥æ¨¡å‹ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©    
- å‘é‡ç´¢å¼•ï¼ˆHNSWï¼‰å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„æ€§èƒ½è‡³å…³é‡è¦  
- Korvus SDKç®€åŒ–äº†æ•´ä¸ªæµç¨‹ï¼Œè‡ªåŠ¨å¤„ç†åˆ‡åˆ†ã€åµŒå…¥å’Œæœç´¢çš„å¤æ‚æ€§
  
  
---
  
# ç®€åŒ–ç‰ˆå®æ“  
è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Python ç¼–å†™çš„çµæ´»è„šæœ¬ã€‚å®ƒé›†æˆäº† **LangChain** çš„æ–‡æœ¬åˆ‡åˆ†å™¨ï¼ˆæ”¯æŒå­—ç¬¦åˆ‡åˆ†ã€é€’å½’å­—ç¬¦ã€markdownæ ¼å¼æ–‡æœ¬åˆ‡åˆ†ã€æŒ‰å¥å­è¯­ä¹‰åˆ‡åˆ†ï¼‰ä»¥åŠ **Ollama** çš„ APIã€‚  
    
## å‡†å¤‡å·¥ä½œ  
    
åœ¨è¿è¡Œè„šæœ¬å‰ï¼Œè¯·ç¡®ä¿ï¼š    
  
1\. å·²å®‰è£… Ollama å¹¶ä¸‹è½½ embedding æ¨¡å‹ï¼š`ollama pull qwen3-embedding:0.6b`  
  
2\. å·²åœ¨å®¿ä¸»æœºå¯åŠ¨ ollama æœåŠ¡. æ–¹æ³•å‚è€ƒ: [ã€Šå¤§å­¦ç”Ÿæ•°æ®åº“å®è·µè¯¾: 1 å¤§çº² åŠ æ•™ç¨‹å…¥å£ã€‹](../202512/20251202_10.md)  å¸¸ç”¨å‘½ä»¤.   
  
3\. è¿›å…¥å®¹å™¨, å®‰è£… Python  
```  
apt-get update  
apt-get install -y python3 pip  
```  
  
4\. é…ç½®å›½å†…æº  
```bash  
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple  
```  
  
5\. å®‰è£…å¿…è¦çš„ Python åº“  
```bash  
pip install langchain_text_splitters requests numpy --break-system-packages  
```    
    
## Python è„šæœ¬ï¼š`embed_split.py`  
  
```python  
import json
import argparse
import requests
import re
import numpy as np
import os
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter

# --- è‡ªå®šä¹‰è¯­ä¹‰åˆ‡åˆ†é€»è¾‘ ---
class OllamaSemanticSplitter:
    def __init__(self, model_name, base_url, threshold=0.6):
        self.model_name = model_name
        self.base_url = base_url
        self.threshold = threshold

    def _cosine_similarity(self, v1, v2):
        if not isinstance(v1, list) or not isinstance(v2, list): return 0
        v1, v2 = np.array(v1), np.array(v2)
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

    def split_text(self, text):
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n])', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if len(sentences) <= 1: return sentences

        print(f"è¯­ä¹‰åˆ†æä¸­ï¼šæ­£åœ¨è·å– {len(sentences)} ä¸ªå¥å­çš„å‘é‡...")
        embeddings = [get_embedding(s, self.model_name, self.base_url) for s in sentences]

        chunks = []
        current_chunk = sentences[0]
        for i in range(len(sentences) - 1):
            similarity = self._cosine_similarity(embeddings[i], embeddings[i+1])
            if similarity < self.threshold:
                chunks.append(current_chunk)
                current_chunk = sentences[i+1]
            else:
                current_chunk += " " + sentences[i+1]
        chunks.append(current_chunk)
        return chunks

def get_embedding(text, model_name, base_url):
    """è°ƒç”¨ Ollama API è·å– embedding"""
    url = f"{base_url.rstrip('/')}/api/embeddings"
    payload = {"model": model_name, "prompt": text}
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']
    except Exception as e:
        print(f"\n[!] è·å– Embedding å¤±è´¥ (URL: {url}): {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="æ–‡æœ¬åˆ‡åˆ†å¹¶ç”Ÿæˆ Embedding æ•°æ®é›†")

    parser.add_argument("--file", required=True, help="è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="output_embeddings.txt", help="ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--url", default="http://host.docker.internal:11434",
                        help="Ollama API åŸºåœ°å€")
    parser.add_argument("--model", default="qwen3-embedding:0.6b", help="Ollama æ¨¡å‹åç§°")
    parser.add_argument("--method", choices=['char', 'recursive', 'semantic', 'markdown'],
                        default='recursive', help="åˆ‡åˆ†æ–¹æ³•")
    parser.add_argument("--chunk_size", type=int, default=500)
    parser.add_argument("--chunk_overlap", type=int, default=50)
    parser.add_argument("--threshold", type=float, default=0.6)

    args = parser.parse_args()

    if not os.path.exists(args.file):
        print(f"é”™è¯¯: æ–‡ä»¶ {args.file} ä¸å­˜åœ¨")
        return

    with open(args.file, 'r', encoding='utf-8') as f:
        content = f.read()

    # 2. é€‰æ‹©å¹¶æ‰§è¡Œåˆ‡åˆ†
    if args.method == 'char':
        splitter = CharacterTextSplitter(separator="", chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
        chunks = splitter.split_text(content)
    elif args.method == 'markdown':
        splitter = MarkdownTextSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
        chunks = splitter.split_text(content)
    elif args.method == 'semantic':
        splitter = OllamaSemanticSplitter(model_name=args.model, base_url=args.url, threshold=args.threshold)
        chunks = splitter.split_text(content)
    else:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", " ", ""]
        )
        chunks = splitter.split_text(content)

    print(f"--- åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} ä¸ª Chunk ---")

    # 3. è®¡ç®—å¹¶ä¿å­˜
    try:
        with open(args.output, 'w', encoding='utf-8') as f_out:
            f_out.write("chunked_context\tembedding\n")

            for i, chunk in enumerate(chunks):
                print(f"å¤„ç†ä¸­: {i+1}/{len(chunks)}", end='\r')

                embedding_data = get_embedding(chunk, args.model, args.url)
                if embedding_data is None: continue

                # å¤„ç†æ–‡æœ¬å­—æ®µï¼š
                # 1. å…ˆæŠŠåæ–œçº¿æœ¬èº«è½¬ä¹‰ (é˜²æ­¢æŠŠ \n è¯¯è®¤ä¸ºçœŸå®çš„æ¢è¡Œ)
                # 2. è½¬ä¹‰å•å¼•å· ' ä¸º \'
                # 3. å°†ç‰©ç†æ¢è¡Œå’ŒTabè½¬ä¸ºå¯è§å­—ç¬¦
                safe_content = chunk.replace("\\", "\\\\").replace("'", "\\'").replace("\t", "\\t").replace("\n", "\\n").replace("\r", "")

                # ä¸¤ä¸ªå­—æ®µéƒ½ç”¨å•å¼•å·åŒ…è£¹
                context_field = f"'{safe_content}'"
                embedding_field = f"'{json.dumps(embedding_data)}'"

                f_out.write(f"{context_field}\t{embedding_field}\n")

        print(f"\n[æˆåŠŸ] æ•°æ®å·²ä¿å­˜è‡³: {args.output}")
    except Exception as e:
        print(f"\nä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()
```  
  
## åˆ‡åˆ†æ–¹æ³•ä»‹ç»  
### 1. å›ºå®šå­—ç¬¦åˆ‡åˆ† (Char) â€”â€” â€œç›²åˆ‡â€  
  
**æ“ä½œï¼š** ä¸ç®¡ä¸‰ä¸ƒäºŒåä¸€ï¼Œæ¯éš” 500 ä¸ªå­—å°±åˆ‡ä¸€åˆ€ã€‚  
  
* **å½¢è±¡ç†è§£ï¼š** å°±åƒé—­ç€çœ¼ç›åˆ‡é•¿é•¿çš„ç«è…¿è‚ ï¼Œæ¯ä¸€æ®µéƒ½ä¸€æ ·é•¿ã€‚  
* **ç¼ºç‚¹ï¼š** æåº¦ç²—é²ã€‚å®ƒå¯èƒ½ä¼šæŠŠâ€œæˆ‘å–œæ¬¢åƒè‹¹æœâ€åˆ‡æˆâ€œæˆ‘å–œæ¬¢åƒâ€å’Œâ€œè‹¹æœâ€ä¸¤ä¸ªåˆ†ç‰‡ã€‚  
* **é€‚ç”¨ï¼š** å‡ ä¹ä¸ç”¨ï¼Œé™¤éä½ åªæ˜¯æƒ³åšæé€Ÿçš„å‹åŠ›æµ‹è¯•ã€‚  
  
### 2. é€’å½’å­—ç¬¦åˆ‡åˆ† (Recursive) â€”â€” â€œçœ‹ç¼åˆ‡â€  
  
**æ“ä½œï¼š** å®ƒæ˜¯æœ€èªæ˜çš„â€œè£ç¼â€ã€‚å…ˆæ‰¾æ®µè½åˆ†éš”ç¬¦ï¼ˆå›è½¦ï¼‰ï¼Œå¦‚æœæ®µè½å¤ªé•¿ï¼Œå†æ‰¾å¥å·ï¼Œè¿˜å¤ªé•¿å°±æ‰¾ç©ºæ ¼ã€‚  
  
* **å½¢è±¡ç†è§£ï¼š** åƒåˆ‡å¸¦æœ‰**è™šçº¿**çš„é¥¼å¹²ã€‚å®ƒä¼šå°½é‡æ²¿ç€è™šçº¿ï¼ˆæ ‡ç‚¹ã€æ®µè½ï¼‰æ‹†å¼€ï¼Œåªæœ‰å½“ä¸€å—é¥¼å¹²å®åœ¨å¤ªå¤§æ”¾ä¸è¿›å˜´é‡Œæ—¶ï¼Œæ‰ä¼šä»ä¸­é—´æ°æ–­ã€‚  
* **ä¼˜ç‚¹ï¼š** å°½é‡ä¿è¯å¥å­çš„å®Œæ•´ï¼Œæ˜¯ç›®å‰ **RAG ç³»ç»Ÿæœ€é€šç”¨ã€æ•ˆæœæœ€ç¨³**çš„æ–¹æ³•ã€‚  
* **é€‚ç”¨ï¼š** æ™®é€šæ–‡ç« ã€åˆåŒã€æŠ¥å‘Šã€‚  
  
### 3. è¯­ä¹‰åˆ‡åˆ† (Semantic) â€”â€” â€œçœ‹é€»è¾‘åˆ‡â€  
  
**æ“ä½œï¼š** è¿™ç§æ–¹æ³•ä¸æ•°å­—æ•°ã€‚å®ƒè®© AI å…ˆè¯»ä¸€éï¼Œå‘ç°â€œä¸Šé¢åœ¨è¯´å¤©æ°”ï¼Œä¸‹é¢å¼€å§‹è¯´è´¢æŠ¥äº†â€ï¼Œå°±åœ¨è¯é¢˜è½¬æŠ˜çš„åœ°æ–¹åˆ‡ä¸€åˆ€ã€‚  
  
* **å½¢è±¡ç†è§£ï¼š** å°±åƒåˆ‡ä¸€ç›˜**æ‚é”¦æ‹¼ç›˜**ã€‚è™½ç„¶éƒ½æ˜¯é£Ÿç‰©ï¼Œä½†å®ƒä¼šæŠŠå¯¿å¸æ”¾åœ¨ä¸€ç±»ï¼ŒæŠŠæ°´æœæ”¾åœ¨å¦ä¸€ç±»ï¼Œè€Œä¸æ˜¯æŒ‰ç…§å¤§å°æ¥åˆ†ã€‚  
* **ä¼˜ç‚¹ï¼š** æ¯ä¸€ä¸ª Chunk å†…éƒ¨çš„æ„æ€éå¸¸ç»Ÿä¸€ï¼ŒAI æ£€ç´¢æ—¶ä¸ä¼šâ€œè·³æˆâ€ã€‚  
* **é€‚ç”¨ï¼š** é€»è¾‘å¤æ‚ã€è¯é¢˜è½¬æ¢é¢‘ç¹çš„é•¿è®ºæ–‡ã€æ·±åº¦è®¿è°ˆå½•ã€‚  
  
### 4. Markdown åˆ‡åˆ† â€”â€” â€œçœ‹éª¨æ¶åˆ‡â€  
  
**æ“ä½œï¼š** ä¸“é—¨é’ˆå¯¹æœ‰æ ¼å¼çš„æ–‡æ¡£ï¼ˆå¦‚å¸¦æœ‰ `#` ä¸€çº§æ ‡é¢˜ã€`##` äºŒçº§æ ‡é¢˜çš„æ–‡ä»¶ï¼‰ã€‚å®ƒæŒ‰ç…§ç›®å½•ç»“æ„æ¥åˆ‡ã€‚  
  
* **å½¢è±¡ç†è§£ï¼š** å°±åƒæ‹†è§£ä¸€å…·**äººä½“æ¨¡å‹**ã€‚å®ƒä¼šæŒ‰ç…§å¤´ã€èº¯å¹²ã€å››è‚¢è¿™äº›å¤©ç„¶çš„ç»“æ„æ¥æ‹†åˆ†ï¼Œè€Œä¸æ˜¯ä¹±å‰ã€‚  
* **ä¼˜ç‚¹ï¼š** å®Œç¾ä¿ç•™äº†æ–‡æ¡£çš„å±‚çº§å…³ç³»ã€‚å¦‚æœä½ æœâ€œç¬¬äºŒç« çš„å†…å®¹â€ï¼Œå®ƒèƒ½ç²¾å‡†ç»™ä½ æ•´ä¸ªç« èŠ‚ã€‚  
* **é€‚ç”¨ï¼š** æŠ€æœ¯æ–‡æ¡£ã€é¡¹ç›®è¯´æ˜ä¹¦ã€GitHub é‡Œçš„ README æ–‡ä»¶ã€‚  
  
### æ€»ç»“å¯¹æ¯”è¡¨  
  
| æ–¹æ³• | åƒä»€ä¹ˆ | èªæ˜ç¨‹åº¦ | æ¨èç­‰çº§ | ä¸€å¥è¯ç‚¹è¯„ |  
| --- | --- | --- | --- | --- |  
| **Char** | ç›²ç›®ä¹±å‰ | â­ | ğŸ’€ åˆ«ç”¨ | ä¼šæŠŠå•è¯åˆ‡ç¢ï¼Œé€»è¾‘ç¨€ç¢ã€‚ |  
| **Recursive** | é¡ºç€ç¼åˆ‡ | â­â­â­ | ğŸ† **å¿…é€‰** | ç®€å•å¥½ç”¨ï¼Œèƒ½ä¿ä½ç»å¤§å¤šæ•°å¥å­çš„å‘½ã€‚ |  
| **Semantic** | æŒ‰æ„æ€åˆ† | â­â­â­â­â­ | ğŸš€ è¿›é˜¶ | æ•ˆæœæœ€å¥½ä½†æœ€æ…¢ï¼Œé€‚åˆè¿½æ±‚æè‡´ç²¾åº¦çš„åœºæ™¯ã€‚ |  
| **Markdown** | æŒ‰éª¨æ¶åˆ† | â­â­â­â­ | ğŸ“‚ ä¸“ç”¨ | å¦‚æœæ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œç”¨å®ƒå‡†æ²¡é”™ã€‚ |  
  
é»˜è®¤ä½¿ç”¨ `recursive` åˆ‡åˆ†æ–¹æ³•ã€‚    
  
## å¦‚ä½•ä½¿ç”¨è¯¥è„šæœ¬ï¼Ÿ  
æŸ¥çœ‹å¸®åŠ©  
```  
# python3 embed_split.py --help  
usage: embed_split.py [-h] --file FILE [--output OUTPUT] [--url URL] [--model MODEL] [--method {char,recursive,semantic,markdown}] [--chunk_size CHUNK_SIZE] [--chunk_overlap CHUNK_OVERLAP]  
                      [--threshold THRESHOLD]  
  
æ–‡æœ¬åˆ‡åˆ†å¹¶ç”Ÿæˆ Embedding æ•°æ®é›†  
  
options:  
  -h, --help            show this help message and exit  
  --file FILE           è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„  
  --output OUTPUT       ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„  
  --url URL             Ollama API åŸºåœ°å€ (é»˜è®¤: http://host.docker.internal:11434)  
  --model MODEL         Ollama æ¨¡å‹åç§°  
  --method {char,recursive,semantic,markdown}  
                        åˆ‡åˆ†æ–¹æ³•  
  --chunk_size CHUNK_SIZE  
  --chunk_overlap CHUNK_OVERLAP  
  --threshold THRESHOLD  
```  
  
å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜  
---|---|---  
`--file` | (å¿…å¡«) | éœ€è¦å¤„ç†çš„æœ¬åœ°æ–‡æœ¬æ–‡ä»¶è·¯å¾„ã€‚  
`--output` | `output_embeddings.txt` | ç»“æœä¿å­˜è·¯å¾„ï¼ˆTSV æ ¼å¼ï¼‰ã€‚  
`--url` | `http://host.docker.internal:11434` | Ollama API çš„åŸºåœ°å€ã€‚  
`--model` | `qwen3-embedding:0.6b` | æŒ‡å®šç”¨äºç”Ÿæˆçš„ Embedding æ¨¡å‹ã€‚åªè¦ä½ æœ¬åœ° `ollama list` é‡Œæœ‰çš„embeddingæ¨¡å‹éƒ½å¯ä»¥è¿è¡Œã€‚  
`--method` | `recursive` | "åˆ‡åˆ†æ–¹æ³•ï¼š`char`, `recursive`, `semantic`, `markdown`ã€‚"  
`--chunk_size` | `500` | æ¯ä¸ªåˆ†ç‰‡çš„å­—ç¬¦é•¿åº¦ä¸Šé™ï¼ˆä¸é€‚ç”¨äºè¯­ä¹‰åˆ‡åˆ†ï¼‰ã€‚  
`--chunk_overlap` | `50` | ç›¸é‚»åˆ†ç‰‡çš„é‡å å­—ç¬¦æ•°ã€‚å—ä¸å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ã€‚è¿™å¾ˆé‡è¦ï¼Œå¯ä»¥é˜²æ­¢è¯­ä¹‰åœ¨åˆ‡åˆ†ç‚¹è¢«å¼ºè¡Œææ–­ï¼ˆä¸Šä¸‹æ–‡ä¿ç•™ï¼‰ã€‚  
`--threshold` | `0.6` | ä»…é™è¯­ä¹‰åˆ‡åˆ†ï¼šç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„ç›¸é‚»å¥å­ä¼šå½’åˆ°åŒä¸€ä¸ªchunké‡Œ. é˜ˆå€¼è¶Šå°, åˆ‡åˆ†å‡ºæ¥çš„å•ä¸ªchunkè¶Šå¤§, æ€»chunkæ•°è¶Šå°‘ã€‚  
  
  
### ä½¿ç”¨ä¸¾ä¾‹  
  
åœºæ™¯ Aï¼šå¸¸è§„é€’å½’åˆ‡åˆ†ï¼ˆæœ€æ¨èï¼‰  
  
ä¿æŒæ®µè½å®Œæ•´æ€§ï¼Œé€‚åˆå¤§å¤šæ•°æ–‡æ¡£ã€‚  
  
```bash  
python3 embed_split.py --file news.txt --method recursive --chunk_size 600  
```  
  
åœºæ™¯ Bï¼šè¯­ä¹‰åˆ‡åˆ†ï¼ˆAI é©±åŠ¨ï¼‰  
  
åŸºäºå†…å®¹æ„æ€çš„è½¬æŠ˜ç‚¹è‡ªåŠ¨åˆ‡åˆ†ï¼Œä¸å›ºå®šé•¿åº¦ã€‚  
  
```bash  
python3 embed_split.py --file essay.txt --method semantic --threshold 0.65 --url http://localhost:11434  
```  
  
åœºæ™¯ Cï¼šMarkdown æ–‡æ¡£å¤„ç†  
  
è‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å±‚çº§è¿›è¡Œåˆ‡åˆ†ã€‚  
  
```bash  
python3 embed_split.py --file README.md --method markdown --output readme_vec.txt  
```  
  
åœºæ™¯ Dï¼šæŒ‰ char åˆ‡åˆ† (æœ€ä¸æ¨è)  
  
æŒ‡å®šæ¨¡å‹å’Œåˆ‡åˆ†å—å¤§å°, å¦‚æœä½ æƒ³åˆ‡å¾—æ›´ç¢ï¼ˆæ¯”å¦‚æ¯ä¸ªå— 200 å­—ç¬¦ï¼‰ï¼Œå¹¶ä½¿ç”¨ç‰¹å®šçš„æ¨¡å‹ï¼š  
  
```bash  
python3 embed_split.py --file news.txt --model qwen3-embedding:0.6b --method char --chunk_size 200 --chunk_overlap 20  
```  
    
### å¸¸è§é—®é¢˜æ’æŸ¥  
    
* **è¿æ¥å¤±è´¥**ï¼šå¦‚æœæ˜¯åœ¨ Docker å†…è®¿é—® Docker å¤–çš„ Ollamaï¼Œè¯·ç¡®ä¿ç¯å¢ƒå˜é‡ `OLLAMA_HOST=0.0.0.0` å·²è®¾ç½®ã€‚  
* **é€Ÿåº¦è¾ƒæ…¢**ï¼š`semantic` æ¨¡å¼ä¼šæŒ‰è¡Œè¯·æ±‚ Embedding ä»¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œè€—æ—¶è¿œé«˜äºå…¶ä»–æ¨¡å¼ã€‚  
* **ç¼–ç é”™è¯¯**ï¼šè¯·ç¡®ä¿è¾“å…¥æ–‡ä»¶ä¸º `UTF-8` ç¼–ç ã€‚  
    
### è¯­ä¹‰åˆ‡åˆ†ä¾‹å­  
å¯¼å…¥ä¸€ç¯‡è±†ç“£çš„æ–‡ç« åˆ°å®¹å™¨æœ¬åœ° `book.txt`    
    
https://book.douban.com/subject/37415782  
    
```  
ã€Šå†™ç»™éå“²å­¦å®¶çš„å“²å­¦å…¥é—¨ã€‹æ˜¯é˜¿å°”éƒ½å¡ç†è®ºæˆç†ŸæœŸç»™é‚£äº›æ²¡æœ‰å“²å­¦å‡†å¤‡çš„äººå†™çš„ä¸€æœ¬å“²å­¦å…¥é—¨ä¹¦ã€‚ä½†è¿™éƒ¨è‘—ä½œå¹¶éç®€å•çš„é€šä¿—åŒ–è‘—ä½œæˆ–å¯¼è®ºï¼Œè€Œæ˜¯é˜¿å°”éƒ½å¡æœ¬äººæ€æƒ³ä¸­æœ€åŸºæœ¬è®ºç‚¹çš„çœŸæ­£æµ“ç¼©å’Œå“²å­¦è§‚çš„æœ€åæ€»ç»“ã€‚æœ¬ä¹¦ç»“æ„ä¸¥è°¨ï¼Œä»åˆ†æä¸€èˆ¬æ°‘ä¼—å¦‚ä½•çœ‹å¾…å“²å­¦å…¥æ‰‹ï¼Œä»¥â€œæŠ½è±¡â€é—®é¢˜ä¸ºä¸»çº¿ï¼Œé€šè¿‡å¯¹â€œæŠ½è±¡ä¸ å…·ä½“â€åŠå…¶å…³ç³»çš„å…¨é¢ç»†è‡´çš„é˜è¿°ï¼Œä»¥æ¢å–»çš„æ–¹å¼ï¼Œé˜æ˜äº†â€œç†è®ºä¸å®è·µâ€â€œå“²å­¦ä¸æ”¿æ²»â€ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œç³»ç»Ÿè®ºè¯äº†é˜¿å°”éƒ½å¡æœ€åçš„å“²å­¦è§‚ã€‚å…¨ä¹¦ä¸»è¦è®¨è®ºäº†å“²å­¦ä¸å®—æ•™çš„å…³ç³»ï¼ŒæŠ½è±¡æ´»åŠ¨çš„ç‰¹å¾ï¼Œå“²å­¦æŠ½è±¡ä¸å…¶ä»–æŠ½è±¡çš„å…³ç³»å’Œå¼‚åŒï¼Œä»¥åŠç”Ÿäº§å®è·µã€æ”¿æ²»å®è·µã€ç§‘å­¦å®è·µã€è‰ºæœ¯å®è·µä¸å“²å­¦å®è·µä¹‹é—´çš„å…³ç³»å’Œå¼‚åŒï¼Œæœ€åé‡ç‚¹è½åˆ°å“²å­¦ä¸æ„è¯†å½¢æ€ã€å“²å­¦ä¸é©¬å…‹æ€ä¸»ä¹‰é˜¶çº§æ–—äº‰ç§‘å­¦ä¹‹é—´çš„ç´§å¯†å…³ç³»ä¸Šã€‚åœ¨ä¸Šè¿°å¾—åˆ°è¯¦ç»†è®ºè¯çš„å“²å­¦è§‚åŸºç¡€ä¸Šï¼Œé˜¿å°”éƒ½å¡å‘¼åä¸€ç§æ–°çš„å“²å­¦å®è·µã€‚ã€Šå†™ç»™éå“²å­¦å®¶çš„å“²å­¦å…¥é—¨ã€‹å†™äºé˜¿å°”éƒ½å¡å¯¹è‡ªå·±çš„æ€æƒ³è¿›è¡Œå…¨é¢æ€»ç»“çš„é˜¶æ®µï¼Œæ˜¯å¯¹å…¶è·¨è¶Šè¿‘äºŒåå¹´ç†è®ºæ€è€ƒçš„ä¸€æ¬¡ç³»ç»Ÿè€Œèè´¯çš„è¡¨è¾¾ï¼Œå®ƒæ—¢å¯ä»¥çœ‹ä½œæ˜¯20ä¸–çºªä¸‹åŠå¶æœ€æœ‰å½±å“çš„å“²å­¦ä¹‹ä¸€çš„å¿«ç…§ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯å³å°†åˆ°æ¥çš„æ€æƒ³çš„å®£è¨€ã€‚  
  
'äººäººéƒ½æ½œåœ¨åœ°æ˜¯å“²å­¦å®¶ï¼Œå› ä¸ºå‡ä»¥æ—¶é—´å’Œ  æ‰‹æ®µï¼Œäººäººéƒ½èƒ½å¯¹åœ¨å…¶ä¸ªäººå’Œç¤¾ä¼šæ¡ä»¶ä¸­ä»¥è¿™ç§æ–¹å¼è‡ªå‘åœ°ä½“éªŒåˆ°çš„å“²å­¦è¦ç´ è·å¾—æ„è¯†ã€‚ä½†è¦å®é™…ä¸Šæˆä¸ºå“²å­¦å®¶ï¼Œäººä»¬ä¼šå»ºè®®ä»–ä»¬é¦–å…ˆä¸“å¿ƒç ”ç©¶å“²å­¦å®¶çš„å“²å­¦ï¼Œå› ä¸ºæ­£æ˜¯å“²å­¦å®¶çš„è‘—ä½œåŒ…å«ç€æ‰€è°“çš„å“²å­¦ã€‚ç„¶è€Œè¿™ç§è§£å†³åŠæ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯é€ ä½œçš„ï¼Œå› ä¸ºä¹¦æœ¬æ— éæ˜¯ä¹¦æœ¬ï¼Œå¦‚æœå¯¹äºä¹¦æœ¬ä¸­æ‰€è°ˆåˆ°çš„å®è·µæ²¡æœ‰å…·ä½“ç»éªŒï¼Œæˆ‘ä»¬çš„å“²å­¦å­¦å¾’å°±å¯èƒ½æŠ“ä¸ä½å®ƒä»¬çš„æ„ä¹‰ï¼Œä»è€Œä¼šåƒå…ˆå‰ä¸€æ ·ï¼Œé™·å…¥ä¹¦æœ¬æŠ½è±¡ä¸–ç•Œçš„å°é—­åœˆå­ï¼Œè€Œè¿™ä¸ªæŠ½è±¡ä¸–ç•Œå¹¶ä¸ä¼šæä¾›æ‰“å¼€å®ƒè‡ªèº«æ„ä¹‰çš„é’¥åŒ™ã€‚  
  
åœ¨è¿™ç§æ„ä¹‰ä¸Šï¼Œé‚£äº›ä¼Ÿå¤§çš„å“²å­¦å®¶â€”â€”ä»æŸ æ‹‰å›¾åˆ°åº·å¾·ï¼Œå“ªæ€•ä»–ä»¬æ˜¯å”¯å¿ƒä¸»ä¹‰è€…â€”â€”å¾ˆæœ‰é“ç†ï¼šä»–ä»¬ä¸»å¼ å“²å­¦ä¸æ˜¯æ•™å‡ºæ¥çš„ï¼Œæ—¢ä¸æ˜¯é€šè¿‡ä¹¦æœ¬ï¼Œä¹Ÿä¸æ˜¯é€šè¿‡è€å¸ˆæ•™å‡ºæ¥çš„ï¼Œè€Œæ˜¯ä»å®è·µä¸­å­¦åˆ°çš„ï¼Œæ¡ä»¶æ˜¯è¦å¯¹å®è·µçš„æ¡ä»¶æœ‰åæ€ï¼Œå¯¹æ”¯é…ç€å®è·µçš„æŠ½è±¡æœ‰åæ€ï¼Œå¯¹ç»Ÿæ²»ç€ç¤¾ä¼šåŠå…¶æ–‡åŒ–çš„é‚£ä¸ªå……æ»¡å†²çªçš„ä½“ç³»æœ‰åæ€ã€‚æˆ‘ä»¬å½“ç„¶åº”è¯¥åˆ©ç”¨ä¹¦æœ¬ï¼Œä½†è¦åƒåœ¨å“²å­¦ä¸Šæ²¡æœ‰å—è¿‡åŸºæœ¬è®­ç»ƒçš„åˆ—å®é‚£æ ·ï¼Œå˜æˆä¸ä¸“ä¸šçš„å“²å­¦å®¶æ——é¼“ç›¸å½“çš„å“²å­¦å®¶ï¼Œå°±å¿…é¡»åœ¨å®è·µä¸­ï¼Œåœ¨ä¸åŒçš„å®è·µä¸­ï¼Œé¦–å…ˆåœ¨é˜¶çº§æ–—äº‰çš„å®è·µä¸­ï¼Œå»å­¦ä¹ å“²å­¦ã€‚  
  
å¦‚æœæœ‰äººé—®ï¼šå¯æ˜¯ï¼Œå“²å­¦å®¶    åˆ°åº•æ˜¯ä»€\'\'ä¹ˆå‘¢ï¼Ÿæˆ‘ä¼šè¯´ï¼šå“²å­¦å®¶å°±æ˜¯åœ¨ç†è®ºä¸­æˆ˜æ–—çš„äººã€‚è€Œä¸ºäº†æˆ˜æ–—ï¼Œå°±å¿…é¡»åœ¨æˆ˜æ–—ä¸­å­¦ä¹ æˆ˜æ–—ï¼›ä¸ºäº†åœ¨ç†è®ºä¸­æˆ˜æ–—ï¼Œå°±å¿…é¡»é€šè¿‡ç§‘å­¦å®è·µã€æ„è¯†å½¢æ€æ–—äº‰å®è·µå’Œæ”¿æ²»æ–—äº‰å®è·µå˜æˆç†è®ºå®¶ã€‚  
  
â€”â€”''é˜¿å°”'éƒ½\\\tå¡  
  
ã€Šè®ºå†ç”Ÿäº§ã€‹çš„ç¬¬ä¸€ç« åº”è¯¥ä¼šåœ¨ç¬¬äºŒ        å·ä¸­å¾—åˆ°\tæ‰©å±•ï¼Œæ ¹æ®â€œå‘Šè¯»è€…â€çš„é¢„å‘Šï¼Œåœ¨ç¬¬ä¸€å·çš„â€œå¤§è¿‚å›â€ä¹‹åï¼Œç¬¬äºŒå·å°†å‘å±•å‡ºâ€œä¸€ä¸ªå…³äºå“²å­¦çš„ç§‘å­¦å®šä¹‰â€ã€‚ç„¶è€Œã€Šè®ºå†ç”Ÿäº§ã€‹çš„ç¬¬ä¸€å·ç›´åˆ°ä½œè€…å»ä¸–äº”å¹´åæ‰å®Œæ•´å‡ºç‰ˆï¼Œè‡³äºç¬¬äºŒå·ï¼Œåˆ™ä»æœªå†™å‡ºæ¥ã€‚ä¸è¿‡åœ¨1970å¹´ä»£ï¼Œé˜¿å°”éƒ½å¡å´ä»¥æŸç§æ–¹å¼é‡å†™äº†è¿™éƒ¨ä¸å­˜åœ¨çš„å“²å­¦æ•™æï¼šé¦–å…ˆæ˜¯åœ¨1976å¹´ä»¥140é¡µæ‰‹ç¨¿çš„å½¢å¼[å³ã€Šåœ¨å“²å­¦ä¸­æˆä¸ºé©¬å…‹æ€ä¸»ä¹‰è€…ã€‹]ï¼›ç„¶åï¼Œä¸€ä¸¤å¹´åï¼Œä»¥æˆ‘ä»¬åœ¨è¿™é‡Œå‡ºç‰ˆçš„è¿™ä¸ªæ–‡æœ¬çš„å½¢å¼â€¦â€¦ç†Ÿæ‚‰é˜¿å°”éƒ½å¡è‘—ä½œçš„äººä¼šæ³¨æ„åˆ°ï¼Œ1978å¹´çš„è¿™æœ¬â€œæ•™æâ€åœ¨å¼•å¯¼â€œéå“²å­¦å®¶â€è¯»è€…è¿›å…¥å“²å­¦ä¹‹é—¨çš„åŒæ—¶ï¼Œä¹Ÿåœ¨å¼•å¯¼ä»–ä»¬è¿›å…¥ä½œè€…çš„å“²å­¦ä¹‹é—¨ï¼Œç‰¹åˆ«æ˜¯é˜¿å°”éƒ½å¡ä»1966-1967å¹´çš„â€œåç†è®ºä¸»ä¹‰â€è½¬å‘å‡ºå‘åˆ¶å®šçš„å“²å­¦ä¹‹é—¨ã€‚  
  
â€”â€”æˆˆä»€''åŠ è¿  
```  
    
æŒ‰è¯­ä¹‰åˆ‡åˆ†    
```bash  
python3 embed_split.py --file book.txt --output book.csv --method semantic --threshold 0.6     
```  
    
### markdown æ ¼å¼åˆ‡åˆ†ä¾‹å­  
ä¸‹è½½    
```bash  
curl -L https://gitee.com/polardb-tianchi/polardb_competition_2025/raw/master/test/README.md -o ./readme.md  
```  
    
æŒ‰ markdown æ ¼å¼åˆ‡åˆ†    
```bash  
python3 embed_split.py --file readme.md --output readme.csv --method markdown   
```  
    
### å¯¼å…¥æ•°æ®åº“  
    
è¿æ¥åˆ°æ•°æ®åº“    
```bash  
psql  
```  
     
å¯¼å…¥testè¡¨    
```sql  
-- å¦‚æœè¿˜æ²¡å®‰è£…æ‰©å±•  
CREATE EXTENSION IF NOT EXISTS vector;  
  
-- åˆ›å»ºè¡¨  
CREATE TABLE test (  
    id serial PRIMARY KEY,  
    content text,  
    embedding vector(1024) -- è¯·æ ¹æ®ä½ æ¨¡å‹çš„ç»´åº¦ä¿®æ”¹ï¼Œä¾‹å¦‚ qwen3-embedding é€šå¸¸æ˜¯ 1536 æˆ– 1024  
);  
  
-- å¯¼å…¥ test è¡¨  
\copy test(content, embedding) FROM 'book.csv' WITH (FORMAT csv, DELIMITER E'\t', QUOTE $$'$$, ESCAPE '\', HEADER true);  
  
-- å¯¼å…¥ test è¡¨  
\copy test(content, embedding) FROM 'readme.csv' WITH (FORMAT csv, DELIMITER E'\t', QUOTE $$'$$, ESCAPE '\', HEADER true);  
```  
     
æŸ¥è¯¢    
```  
select * from test;  
```  
        
    
    
---  

**è¦ä¹‰å›¾ç¤º**  
  

### 1. æ–‡æœ¬å¤„ç†ä¸æœç´¢å…¨ç”Ÿå‘½å‘¨æœŸ

è¿™å¼ å›¾å±•ç¤ºäº†ä»åŸå§‹æ–‡æ¡£è¿›å…¥ç³»ç»Ÿï¼Œåˆ°æœ€åè¿”å›æœç´¢ç»“æœçš„å®Œæ•´é—­ç¯ã€‚

```mermaid
graph TD
    subgraph Data_Ingestion [æ•°æ®å…¥åº“é˜¶æ®µ]
        Doc[åŸå§‹æ–‡æ¡£] --> Splitter[æ–‡æœ¬åˆ‡åˆ† Splitter]
        Splitter --> Chunks[æ–‡æœ¬å— Chunks]
        Chunks --> Embedding[åµŒå…¥æ¨¡å‹ Embedding]
        Embedding --> VectorDB[(PostgreSQL + pgvector)]
    end

    subgraph Query_Stage [æ£€ç´¢é˜¶æ®µ]
        UserQ[ç”¨æˆ·æé—®] --> Q_Emb[æé—®å‘é‡åŒ–]
        Q_Emb --> Search[å‘é‡ç›¸ä¼¼åº¦æœç´¢]
        VectorDB --> Search
        Search --> TopN[è¿”å› Top-N ç‰‡æ®µ]
    end

    style Splitter fill:#f96,stroke:#333
    style Embedding fill:#f96,stroke:#333
```
  

### 2. å®æ“ç¯å¢ƒä¾èµ–æ ‘

åœ¨å¼€å§‹å®æ“å‰ï¼Œè¯·ç¡®ä¿ä»¥ä¸‹â€œåŸºç¡€è®¾æ–½â€å·²é…ç½®å¦¥å½“ã€‚

```mermaid
graph BT
    SDK[Korvus SDK / pgml] --> Python[Python 3.8+ / Node.js]
    Python --> PGML[PostgresML 2.7.7+]
    PGML --> PG[PostgreSQL æ ¸å¿ƒ]
    PG --> PGV[pgvector æ‰©å±•]
    
    ENV[DATABASE_URL ç¯å¢ƒå˜é‡] -.-> SDK
```

  

### 3. é€’å½’å­—ç¬¦åˆ‡åˆ†å™¨ (Recursive Character Splitter) é€»è¾‘

é€šè¿‡è¿™å¼ å›¾ï¼Œä½ å¯ä»¥ç†è§£ `chunk_size` å’Œ `chunk_overlap` æ˜¯å¦‚ä½•ååŒå·¥ä½œçš„ã€‚

```mermaid
graph LR
    subgraph Chunk_1 [ç‰‡æ®µ 1]
        T1[æ–‡æœ¬å†…å®¹ A...]
    end
    
    subgraph Overlap [é‡å åŒºåŸŸ Overlap]
        O1[...é‡å¤æ–‡æœ¬...]
    end
    
    subgraph Chunk_2 [ç‰‡æ®µ 2]
        T2[...æ–‡æœ¬å†…å®¹ B]
    end
    
    T1 --- O1
    O1 --- T2

    %% æ¨¡æ‹Ÿâ€œè·¨å­å›¾æ³¨é‡Šâ€
    Note["é‡å ç¡®ä¿äº†è¯­ä¹‰çš„è¿ç»­æ€§ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡è¢«åˆ‡æ–­"]:::annotation

    %% å¯é€‰ï¼šç”¨è™šçº¿è¿æ¥åˆ°é‡å åŒºåŸŸï¼ˆç¤ºæ„å…³è”ï¼‰
    Overlap -.-> Note

    classDef annotation fill:#e6f7ff,stroke:#1890ff,stroke-dasharray: 5 5, color:#1890ff, font-size:13px, font-style:italic;
```

  

### 4. è‡ªåŠ¨åŒ–ç®¡é“ (Pipeline) å·¥ä½œæµç¨‹

ä½¿ç”¨ Korvus SDK æ—¶ï¼Œç®¡é“ä¼šè‡ªåŠ¨å¤„ç†åˆ‡åˆ†å’Œå‘é‡åŒ–ã€‚

```mermaid
flowchart LR
    Docs[æ–‡æ¡£åˆ—è¡¨] --> Pipeline{Pipeline é…ç½®}
    
    subgraph Pipeline_Config [ç®¡é“å†…éƒ¨è‡ªåŠ¨åŒ–]
        direction TB
        S[Splitter: recursive_character]
        M[Model: gte-base / mxbai]
    end
    
    Pipeline --> S --> M --> DB[(Vector Database)]
```

  

### 5. è¯­ä¹‰æœç´¢ SQL/ä»£ç  æ‰§è¡Œé€»è¾‘

è¿™å¼ å›¾è§£é‡Šäº† TOP-N æœç´¢æ—¶ï¼Œâ€œä½™å¼¦è·ç¦»â€æ˜¯å¦‚ä½•å†³å®šæ’åå…ˆåé¡ºåºçš„ã€‚

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·æŸ¥è¯¢
    participant E as Embedding å‡½æ•°
    participant V as å‘é‡ç´¢å¼• (HNSW)
    participant R as ç»“æœé›†

    U->>E: "æœç´¢é—®é¢˜"
    E->>V: è½¬æ¢ä¸ºå‘é‡ [0.12, -0.05, ...]
    V->>V: æ‰§è¡Œ <=> (ä½™å¼¦è·ç¦»è®¡ç®—)
    V->>R: æŒ‰ç…§è·ç¦»ä»å°åˆ°å¤§æ’åº (ORDER BY)
    R->>U: è¿”å›å‰ N æ¡åŸå§‹æ–‡æœ¬ (LIMIT N)
```

  

### å®éªŒå°è´´å£«ï¼ˆç»™åŒå­¦ä»¬çš„é¿å‘æŒ‡å—ï¼‰ï¼š

1. **å…³äº pgvector**ï¼šåœ¨ SQL ä¸­ï¼Œ`<=>` ä»£è¡¨ä½™å¼¦è·ç¦»ï¼Œè·ç¦»è¶Š**å°**è¡¨ç¤ºè¯­ä¹‰è¶Š**æ¥è¿‘**ã€‚
2. **HNSW ç´¢å¼•**ï¼šå½“ä½ çš„æ•°æ®é‡è¶…è¿‡ 1 ä¸‡æ¡æ—¶ï¼Œä¸€å®šè¦æ‰§è¡Œ `CREATE INDEX ... USING hnsw`ï¼Œå¦åˆ™æœç´¢é€Ÿåº¦ä¼šä»â€œæ¯«ç§’çº§â€æ‰åˆ°â€œç§’çº§â€ã€‚
3. **æ¨¡å‹é€‰æ‹©**ï¼š
    * **è‹±æ–‡æ¨è**ï¼š`Alibaba-NLP/gte-base-en-v1.5`ã€‚
    * **ä¸­è‹±æ··åˆæ¨è**ï¼š`mixedbread-ai/mxbai-embed-large-v1`ã€‚
4. **ç¯å¢ƒå˜é‡**ï¼šå®æ“æ—¶å¦‚æœé‡åˆ° `Connection Refused`ï¼Œå¤šåŠæ˜¯ `DATABASE_URL` æ²¡æœ‰æ­£ç¡® `export` åˆ°å½“å‰ç»ˆç«¯çª—å£ã€‚
  
  
#### [PolarDB å­¦ä¹ å›¾è°±](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL è§£å†³æ–¹æ¡ˆé›†åˆ](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [å¾·å“¥ / digoal's Github - å…¬ç›Šæ˜¯ä¸€è¾ˆå­çš„äº‹.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About å¾·å“¥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
