## 大学生数据库实践课: 10 Dify RAG 配置详解     
                  
### 作者                  
digoal                  
                  
### 日期                  
2025-12-10                  
                  
### 标签                  
PostgreSQL , DuckDB , 语义搜索 , 向量搜索 , 关键词检索 , 全文检索 , 标量检索 , 混合搜索 , 多模态搜索 , 重排序 , embedding , document split , ORC , 大模型 , 多模态大模型 , Dify , 压力测试 , 索引优化 , 倒排向量索引 , Ivfflat , 图向量索引 , HNSW , DiskANN , 量化 , rabitq , sbq , 二值量化 , bm25 , 相关性 , 相似性 , 召回率 , pgbench , 数据集 , ann-benchmarks , 图搜索 , PGQ , 递归 , 深度优先 , 广度优先 , 最短路径 , N度关系 , LLM , Ollama                  
                  
----                  
                  
## 背景                  
    
dify deepwiki:    
    
https://deepwiki.com/search/-1dify-rag-2-dify-ollama-embed_02fe7dab-36e1-474b-bcd5-ebcfe535b398?mode=fast  
  
面向大学生通俗讲解以下内容, 并给出示例:    
1、Dify RAG 配置示例;  
2、详细讲解 Dify 文本切分配置、模型配置(使用ollama本地模型, 包括embedding、ranking、推理模型)、模型调参;  
    
    
-----    
  
# Dify RAG 配置详解  
  
## 1. RAG 配置示例  
  
Dify的RAG配置主要包括知识库创建、文档上传、检索设置等步骤。以下是一个典型的配置流程：  
  
### 创建知识库  
```yaml  
# 知识库基本配置  
name: "我的知识库"  
description: "存储课程相关资料"  
indexing_technique: "high_quality"  # 使用高质量模式  
embedding_model: "text-embedding-ada-002"  
embedding_model_provider: "langgenius/openai/openai"  
```  
  
### 检索配置  
```yaml  
retrieval_model:  
  search_method: "semantic_search"  # 向量检索  
  top_k: 3  # 返回最相关的3个片段  
  score_threshold: 0.5  # 相似度阈值  
  reranking_enable: true  # 启用重排序  
```  
  
## 2. 文本切分配置详解  
  
文本切分是RAG的核心步骤，Dify提供了两种切分模式：  
  
### 自动切分模式  
系统自动根据文档内容进行切分，使用默认参数  ：  
- 最大分段长度：由系统自动决定  
- 重叠度：系统自动设置  
- 分隔符：`["\n\n", "。", ". ", " ", ""]`  
  
### 自定义切分模式  
你可以精确控制切分参数  ：  
  
```python  
# 切分参数示例  
processing_rule_mode = "custom"  
max_tokens = 500  # 最大分段长度（50-5000之间）  
chunk_overlap = 50  # 分段重叠长度  
separator = "\n\n"  # 自定义分隔符  
```  
  
**参数说明**：  
- `max_tokens`：每个分段的最大token数，必须在50到系统最大值之间    
- `chunk_overlap`：相邻分段之间的重叠token数，保证上下文连续性  
- `separator`：切分时使用的分隔符，支持`\n`转义  
  
### 父子分段模式  
适用于层次化文档，如论文、报告等  ：  
- 父分段：较大的文档块  
- 子分段：更细粒度的内容片段  
  
## 3. 模型配置（使用Ollama本地模型）  
  
### Embedding模型配置  
```yaml  
embedding_model: "nomic-embed-text"  # Ollama的embedding模型  
embedding_model_provider: "ollama"  
```  
  
**重要提示**：切换embedding模型会导致向量维度不一致，影响检索效果  。  
  
### Rerank模型配置  
```yaml  
reranking_model:  
  model: "bge-reranker-base"  # Ollama的rerank模型  
  provider: "ollama"  
```  
  
Rerank模型会对检索结果重新排序，提高准确性  。  
  
### 推理模型配置  
```yaml  
reasoning_model:  
  model: "llama3:8b"  # Ollama的LLM模型  
  provider: "ollama"  
```  
  
## 4. 模型调参详解  
  
### 核心参数说明  ：  
  
| 参数 | 含义 | 调参建议 |  
|------|------|----------|  
| `temperature` | 控制回复随机性 | 0.7-1.0（创意），0.3-0.7（平衡），0-0.3（精确） |  
| `top_p` | 核采样，控制生成多样性 | 0.8-0.95，与temperature不建议同时调整 |  
| `presence_penalty` | 话题新鲜度 | 0-2，值越大越容易引入新话题 |  
| `frequency_penalty` | 频率惩罚度 | 0-2，值越大越避免重复词汇 |  
| `max_tokens` | 单次回复长度限制 | 建议不超过模型上下文窗口的2/3 |  
  
### 检索参数调优  
  
```yaml  
retrieval_params:  
  top_k: 3-5  # 根据问题复杂度调整  
  score_threshold: 0.3-0.7  # 根据数据质量调整  
  reranking_enable: true  # 对准确性要求高时启用  
```  
  
### 实际调参示例  
  
**场景1：课程问答系统**  
```yaml  
# 追求准确性  
temperature: 0.2  
top_k: 3  
score_threshold: 0.5  
reranking_enable: true  
```  
  
**场景2：创意写作助手**  
```yaml  
# 追求创意性  
temperature: 0.9  
top_p: 0.95  
presence_penalty: 0.8  
max_tokens: 2000  
```  
  
## Notes  
  
- 文本切分配置直接影响检索效果，建议根据文档类型选择合适的模式  
- 使用Ollama本地模型时，确保模型已正确安装并运行  
- 模型调参需要根据具体应用场景反复试验  
- 切换embedding模型后需要重新索引知识库    
- 高质量模式消耗tokens但检索效果更好，经济模式使用关键词检索不消耗tokens   
  
    
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
