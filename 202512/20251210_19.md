## 大学生数据库实践课: 10 Dify RAG 配置详解     
                  
### 作者                  
digoal                  
                  
### 日期                  
2025-12-10                  
                  
### 标签                  
PostgreSQL , DuckDB , 语义搜索 , 向量搜索 , 关键词检索 , 全文检索 , 标量检索 , 混合搜索 , 多模态搜索 , 重排序 , embedding , document split , ORC , 大模型 , 多模态大模型 , Dify , 压力测试 , 索引优化 , 倒排向量索引 , Ivfflat , 图向量索引 , HNSW , DiskANN , 量化 , rabitq , sbq , 二值量化 , bm25 , 相关性 , 相似性 , 召回率 , pgbench , 数据集 , ann-benchmarks , 图搜索 , PGQ , 递归 , 深度优先 , 广度优先 , 最短路径 , N度关系 , LLM , Ollama                  
                  
----                  
                  
## 背景   
实操参考: [《3分钟速玩Dify(高度可定制的企业级AI应用开源项目)》](../202504/20250404_01.md)  
```
# https://github.com/langgenius/dify/

git clone --depth 1 -b 1.11.1 https://github.com/langgenius/dify
cd dify/docker
cp .env.example .env
docker compose up -d
```
    
dify deepwiki:    
    
https://deepwiki.com/search/-1dify-rag-2-dify-ollama-embed_02fe7dab-36e1-474b-bcd5-ebcfe535b398?mode=fast  
  
面向大学生通俗讲解以下内容, 并给出示例:    
1、Dify RAG 配置示例;  
2、详细讲解 Dify 文本切分配置、模型配置(使用ollama本地模型, 包括embedding、ranking、推理模型)、模型调参;  
    
    
-----    
  
# Dify RAG 配置详解  
  
## 1. RAG 配置示例  
  
Dify的RAG配置主要包括知识库创建、文档上传、检索设置等步骤。以下是一个典型的配置流程：  
  
### 创建知识库  
```yaml  
# 知识库基本配置  
name: "我的知识库"  
description: "存储课程相关资料"  
indexing_technique: "high_quality"  # 使用高质量模式  
embedding_model: "text-embedding-ada-002"  
embedding_model_provider: "langgenius/openai/openai"  
```  
  
### 检索配置  
```yaml  
retrieval_model:  
  search_method: "semantic_search"  # 向量检索  
  top_k: 3  # 返回最相关的3个片段  
  score_threshold: 0.5  # 相似度阈值  
  reranking_enable: true  # 启用重排序  
```  
  
## 2. 文本切分配置详解  
  
文本切分是RAG的核心步骤，Dify提供了两种切分模式：  
  
### 自动切分模式  
系统自动根据文档内容进行切分，使用默认参数  ：  
- 最大分段长度：由系统自动决定  
- 重叠度：系统自动设置  
- 分隔符：`["\n\n", "。", ". ", " ", ""]`  
  
### 自定义切分模式  
你可以精确控制切分参数  ：  
  
```python  
# 切分参数示例  
processing_rule_mode = "custom"  
max_tokens = 500  # 最大分段长度（50-5000之间）  
chunk_overlap = 50  # 分段重叠长度  
separator = "\n\n"  # 自定义分隔符  
```  
  
**参数说明**：  
- `max_tokens`：每个分段的最大token数，必须在50到系统最大值之间    
- `chunk_overlap`：相邻分段之间的重叠token数，保证上下文连续性  
- `separator`：切分时使用的分隔符，支持`\n`转义  
  
### 父子分段模式  
适用于层次化文档，如论文、报告等  ：  
- 父分段：较大的文档块  
- 子分段：更细粒度的内容片段  
  
## 3. 模型配置（使用Ollama本地模型）  
  
### Embedding模型配置  
```yaml  
embedding_model: "nomic-embed-text"  # Ollama的embedding模型  
embedding_model_provider: "ollama"  
```  
  
**重要提示**：切换embedding模型会导致向量维度不一致，影响检索效果  。  
  
### Rerank模型配置  
```yaml  
reranking_model:  
  model: "bge-reranker-base"  # Ollama的rerank模型  
  provider: "ollama"  
```  
  
Rerank模型会对检索结果重新排序，提高准确性  。  
  
### 推理模型配置  
```yaml  
reasoning_model:  
  model: "llama3:8b"  # Ollama的LLM模型  
  provider: "ollama"  
```  
  
## 4. 模型调参详解  
  
### 核心参数说明  ：  
  
| 参数 | 含义 | 调参建议 |  
|------|------|----------|  
| `temperature` | 控制回复随机性 | 0.7-1.0（创意），0.3-0.7（平衡），0-0.3（精确） |  
| `top_p` | 核采样，控制生成多样性 | 0.8-0.95，与temperature不建议同时调整 |  
| `presence_penalty` | 话题新鲜度 | 0-2，值越大越容易引入新话题 |  
| `frequency_penalty` | 频率惩罚度 | 0-2，值越大越避免重复词汇 |  
| `max_tokens` | 单次回复长度限制 | 建议不超过模型上下文窗口的2/3 |  
  
### 检索参数调优  
  
```yaml  
retrieval_params:  
  top_k: 3-5  # 根据问题复杂度调整  
  score_threshold: 0.3-0.7  # 根据数据质量调整  
  reranking_enable: true  # 对准确性要求高时启用  
```  
  
### 实际调参示例  
  
**场景1：课程问答系统**  
```yaml  
# 追求准确性  
temperature: 0.2  
top_k: 3  
score_threshold: 0.5  
reranking_enable: true  
```  
  
**场景2：创意写作助手**  
```yaml  
# 追求创意性  
temperature: 0.9  
top_p: 0.95  
presence_penalty: 0.8  
max_tokens: 2000  
```  
  
## Notes  
  
- 文本切分配置直接影响检索效果，建议根据文档类型选择合适的模式  
- 使用Ollama本地模型时，确保模型已正确安装并运行  
- 模型调参需要根据具体应用场景反复试验  
- 切换embedding模型后需要重新索引知识库    
- 高质量模式消耗tokens但检索效果更好，经济模式使用关键词检索不消耗tokens   
  
    
---  

**要义图示**  
  


### 1. Dify RAG 配置全流程

这张图展示了从原始文档到最终成为 AI 知识库的标准化配置步骤。

```mermaid
graph TD
    Start[1\. 创建知识库] --> Indexing[2\. 选择索引方式: 高质量/经济]
    Indexing --> Chunking[3\. 文本切分配置: 自动/自定义]
    Chunking --> Cleaning[4\. 文本清洗配置: 替换空格/URL等]
    Cleaning --> ModelSelect[5\. 模型选择: Embedding + Rerank]
    ModelSelect --> Execute[6\. 保存并等待向量化完成]
    Execute --> Test[7\. 命中测试与效果调优]

    style Indexing fill:#f96,stroke:#333
    style ModelSelect fill:#f96,stroke:#333
```



### 2. 文本切分参数：自定义模式逻辑

通过这张图，你可以直观地理解 `max_tokens`（长度）和 `overlap`（重叠）是如何共同保证语义不丢失的。

```mermaid
graph LR
    subgraph Chunk_A [分段 A]
        T1[文本内容...]
        O1[重叠部分 Overlap]
    end
    
    subgraph Chunk_B [分段 B]
        O2[重叠部分 Overlap]
        T2[后续文本内容...]
    end

    O1 -.->|内容相同| O2

    Note["Overlap 建议设为 10%-25%<br/>确保跨段落的语义连续"]:::noteStyle

    Chunk_A -.-> Note
    Chunk_B -.-> Note

    classDef noteStyle fill:#fff9db,stroke:#ffa940,stroke-width:1px,stroke-dasharray: 4 4,color:#d97706,font-size:13px,font-style:italic,text-align:center;
```



### 3. 本地模型组合架构 (Dify + Ollama)

这张图展示了如何在 Dify 中配置三类本地模型，实现全流程的“本地 AI 闭环”。

```mermaid
graph LR
    subgraph Ollama_Server [Ollama 本地服务]
        direction TB
        M1[nomic-embed-text <br/> Embedding 模型]
        M2[bge-reranker <br/> Rerank 重排模型]
        M3[llama3:8b <br/> Reasoning 推理模型]
    end

    subgraph Dify_RAG [Dify 知识库引擎]
        Search[1\. 向量检索] --> ReRank[2\. 重排序]
        ReRank --> Generation[3\. 答案生成]
    end

    M1 -.-> Search
    M2 -.-> ReRank
    M3 -.-> Generation
```



### 4. 模型调参决策脑图

这张图将复杂的参数转化为“应用场景”导向的选择，帮助你快速做出配置决定。

```mermaid
mindmap
  root((Dify 调参指南))
    准确性优先_事实问答
      Temperature: 0 ~ 0.3
      Top_K: 3
      Score_Threshold: 0.5+
      Reranking: 开启
    创造性优先_文案故事
      Temperature: 0.8 ~ 1.0
      Top_P: 0.9
      Presence_Penalty: 0.5+
      Max_Tokens: 2000+
    检索精度调优
      Score_Threshold: 过滤低相关性
      Top_K: 决定参考资料数量
      Reranking: 纠正向量检索偏差
```



### 实验配置建议（给同学们的复习贴士）：

1. **关于重排（Rerank）**：如果你发现 AI 总是引用不相关的资料，**开启重排模型**是最高效的解决办法。它虽然会让回复稍微变慢，但召回精度会大幅提升。
2. **Embedding 陷阱**：一旦你的知识库建立了索引，**千万不要随意更换 Embedding 模型**。更换模型意味着向量维度变了，你必须清空知识库并重新上传文档。
3. **命中测试**：在 Dify 知识库侧边栏有一个“命中测试”功能。在正式发布 AI 应用前，先用几个常见问题测试一下，看看系统能不能准确找到文档所在的段落。
4. **本地连接**：在 Dify 中配置 Ollama 时，如果 Dify 是容器部署的，API 地址通常要填 `http://host.docker.internal:11434` 而不是 `localhost`。

  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
