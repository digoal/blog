## 大学生数据库实践课: 13 向量搜索优化实战  
  
### 作者  
digoal  
  
### 日期  
2025-12-11  
  
### 标签  
PostgreSQL , DuckDB , 语义搜索 , 向量搜索 , 关键词检索 , 全文检索 , 标量检索 , 混合搜索 , 多模态搜索 , 重排序 , embedding , document split , ORC , 大模型 , 多模态大模型 , Dify , 压力测试 , 索引优化 , 倒排向量索引 , Ivfflat , 图向量索引 , HNSW , DiskANN , 量化 , rabitq , sbq , 二值量化 , bm25 , 相关性 , 相似性 , 召回率 , pgbench , 数据集 , ann-benchmarks , 图搜索 , PGQ , 递归 , 深度优先 , 广度优先 , 最短路径 , N度关系 , LLM , Ollama  
  
----  
  
## 背景  
  
pgvector deepwiki:  
  
https://gitee.com/polardb-tianchi/polardb_competition_2025/tree/master/test  
  
1、拉取数据集  
  
```  
wget http://ann-benchmarks.com/nytimes-16-angular.hdf5  
  
# 以下为 500 MB 以下的数据集，适合于本地运行，参数含义如下:  
# "dataset_name": dataset_size, dimension x vector_nums, distance_operator  
# 特别注意: 请在测试时选择正确的距离操作符  
  
# 可以使用 wget http://ann-benchmarks.com/${dataset_name}.hdf5 的方式拉取数据  
# 以 nytimes-16-angular 为例，可以使用 wget http://ann-benchmarks.com/nytimes-16-angular.hdf5 拉取数据集  
"fashion-mnist-784-euclidean": 217 MB, 784 x 60,000, Euclidean (L2)  
"mnist-784-euclidean": 217 MB, 784 x 60,000, Euclidean (L2)  
"glove-25-angular": 121 MB, 25 x 1,183,514, Angular (Cosine)  
"glove-50-angular": 235 MB, 50 x 1,183,514, Angular (Cosine)  
"glove-100-angular": 463 MB, 100 x 1,183,514, Angular (Cosine)  
"sift-128-euclidean": 501 MB, 128 x 1,000,000, Euclidean (L2)  
"nytimes-256-angular": 301 MB, 256 x 290,000, Angular (Cosine)  
"nytimes-16-angular": 26 MB, 16 x 290,000, Angular (Cosine)  
"lastfm-64-dot": 135 MB, 65 x 292,385, Angular (Cosine)  
  
# 使用 wget https://github.com/fabiocarrara/str-encoders/releases/download/v0.1.3/${dataset_name}.hdf5 的方式拉取数据  
"coco-i2i-512-angular": 136 MB, 512 x 113,287, Angular (Cosine)  
"coco-t2i-512-angular": 136 MB, 512 x 113,287, Angular (Cosine)  
```  
  
2、python 环境配置  
  
```  
# 建议将虚拟环境创建在 /tmp/test 目录下，也可以自行选择其他目录  
mkdir /tmp/test  
cd /tmp/test  
  
# 安装 python 相关库  
apt-get update  
apt-get install -y python3-pip python3-requests python3-venv  
  
# 创建并激活 venv 环境  
python3 -m venv pg-venv  
source pg-venv/bin/activate  
  
# 安装 python 包  
pip3 install psycopg2 numpy h5py -i https://mirrors.aliyun.com/pypi/simple/  
```  
  
  
3、初始化数据库表结构  
  
```  
# 请根据选择的数据集选择对应的向量维度，如使用 nytimes-16-angular 则需将 VECTOR_DIM 设置为 16  
  
# 根据向量维度创建表结构  
VECTOR_DIM=16  
  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
  
# 使用超级用户登录数据库  
psql -h 127.0.0.1 -p 5432 -U postgres << EOF  
-- 创建数据库用户  
CREATE USER $USER WITH LOGIN SUPERUSER PASSWORD '$PASSWORD';  
-- 创建测试数据库  
DROP DATABASE IF EXISTS $DBNAME;  
CREATE DATABASE $DBNAME OWNER $USER;  
EOF  
  
# 创建插件与测试表  
psql -h 127.0.0.1 -p 5432 -U "$USER" -d "$DBNAME" << EOF  
-- 创建插件  
CREATE EXTENSION IF NOT EXISTS vector;  
-- 创建表  
DROP TABLE IF EXISTS vector_table;  
CREATE TABLE vector_table (id bigserial PRIMARY KEY, embedding vector($VECTOR_DIM));  
ALTER TABLE vector_table ALTER COLUMN embedding SET STORAGE PLAIN;  
EOF  
```  
  
  
4、插入数据  
  
```  
# 请根据选择数据集选择对应的数据集名称，如使用 nytimes-16-angular 则需将 DATASET_NAME 设置为 nytimes-16-angular  
  
# 填写数据集名称  
DATASET_NAME=nytimes-16-angular  
  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
  
# 确保安装 python 依赖包或已激活虚拟 venv环境  
source pg-venv/bin/activate  
  
# 运行测试  
python3 load.py \
--host 127.0.0.1 \
--port 5432 \
--database $DBNAME \
--user $USER \
--password $PASSWORD \
--filename "${DATASET_NAME}.hdf5" \
--tablename vector_table \
--batch_size 1000 \
--num_workers 8  
```  
  
5、建立索引  
  
```  
# 当前主要使用这三种操作符:  
vector_l2_ops(<->)  
vector_cosine_ops(<=>)  
vector_ip_ops(<#>)  
  
# 请连接数据库 (psql -h 127.0.0.1 -p 5432 -U "$USER" -d "$DBNAME") 后执行下述索引创建语句:  
```  
  
实际使用时, 一个索引即可. 观察这两种语句的创建耗时  
  
```  
\timing  
  
-- 创建 HNSW 索引  
CREATE INDEX ON vector_table  
USING hnsw (embedding vector_cosine_ops)  
WITH (m = 16, ef_construction = 64);  

 
-- 创建 IVFFLAT 索引  
CREATE INDEX ON vector_table  
USING ivfflat (embedding vector_cosine_ops)  
WITH (lists = 2000);  
```  
  
hnsw 索引创建示例：  
```  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
  
psql -h 127.0.0.1 -p 5432 -U "$USER" -d "$DBNAME" << EOF  
CREATE INDEX ON vector_table  
USING hnsw (embedding vector_cosine_ops)  
WITH (m = 16, ef_construction = 64);  
EOF  
```  
  
  
6、 性能测试  
  
```  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
DATASET_NAME="nytimes-16-angular"  
  
# 基本使用方式  
usage: query.py  
  [-h] [--explain EXPLAIN] --hdf5_file HDF5_FILE  
  --table_name TABLE_NAME [--host HOST] [--port PORT]  
  [--database DATABASE] [--user USER] [--password PASSWORD]  
  [--k K] [--metric {l2,cosine,inner}]  
  [--max_vid MAX_VID] [--workers WORKERS]  
  [--pgvector_hnsw_ef_search PGVECTOR_HNSW_EF_SEARCH]  
  [--pgvector_ivf_probes PGVECTOR_IVF_PROBES]  
  [--query_num QUERY_NUM] [--offset OFFSET]  
```  
  
### 脚本参数说明  
  
*   `--hdf5_file`: 指定 HDF5 格式的输入文件路径，包含测试向量和真实近邻数据  
  
#### 数据库连接参数  
  
*   `--host`: PostgreSQL 数据库主机地址，默认为 127.0.0.1  
  
*   `--port`: PostgreSQL 数据库端口号，默认为 5432  
  
*   `--database`: 数据库名称，默认为 postgres  
  
*   `--user`: 数据库用户名，默认为 postgres  
  
*   `--password`: 数据库密码  
  
*   `--table_name`: 指定要查询的数据库表名  
  
*   `--explain`: 是否启用 SQL 解释模式，开启后会输出查询执行计划而非实际执行查询  
  
  
#### 查询配置参数  
  
*   `--k`: 指定返回最近邻的数量，默认为 10  
  
*   `--metric`: 指定距离度量方式，可选'l2'(欧氏距离 <->)、'cosine'(余弦距离 <=>)、'inner'(内积 <#>)，默认为'l2'，**在进行测试时，应该按照数据集要求选择距离度量方式，避免 Recall 计算异常**  
  
*   `--max_vid`: 最大向量 ID 限制，用于计算召回率时过滤超出范围的向量 ID（通常应该 > 向量总数）  
  
*   `--workers`: 指定并行查询的进程数量，默认为 8  
  
  
#### 索引搜索参数  
  
*   `--pgvector_hnsw_ef_search`: HNSW 索引搜索参数，控制搜索过程中候选节点的数量  
  
*   `--pgvector_ivf_probes`: IVF 索引搜索参数，控制探测的聚类中心数量  
  
  
#### 查询控制参数  
  
*   `--query_num`: 指定要执行的查询数量，0 表示执行所有查询  
  
*   `--offset`: 指定查询起始偏移量，用于分段执行查询  
  
  
### HNSW 索引查询示例  
  
*   请注意设置 `--metric` 参数的值，需要与数据集中指定的操作符保持一致，如使用 nytimes-16-angular，因为其操作符为 Angular (Cosine)，需要将 `--metric` 参数设置为 'cosine'  
  
```bash  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
DATASET_NAME=  
  
python3 query.py \
  --host 127.0.0.1 \
  --port 5432 \
  --database $DBNAME \
  --user $USER \
  --password $PASSWORD \
  --table_name vector_table \
  --hdf5_file "${DATASET_NAME}.hdf5" \
  --k 10 \
  --metric cosine \
  --workers 8 \
  --pgvector_hnsw_ef_search 120  
```  
  
### IVF\_FLAT 索引查询示例  
  
*   请注意设置 `--metric` 参数的值，需要与数据集中指定的操作符保持一致，如使用 nytimes-16-angular，因为其操作符为 Angular (Cosine)，需要将 `--metric` 参数设置为 'cosine'  
  
```bash  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
DATASET_NAME=  
  
python3 query.py \
  --host 127.0.0.1 \
  --port 5432 \
  --database $DBNAME \
  --user $USER \
  --password $PASSWORD \
  --table_name vector_table \
  --hdf5_file "${DATASET_NAME}.hdf5" \
  --k 10 \
  --metric cosine \
  --workers 8 \
  --pgvector_ivf_probes 30  
```  
  
### EXPLAIN 打印执行计划  
  
```bash  
USER="testuser"  
PASSWORD="testPawword"  
DBNAME="testdb"  
DATASET_NAME=  
  
python3 query.py \
  --host 127.0.0.1 \
  --port 5432 \
  --database $DBNAME \
  --user $USER \
  --password $PASSWORD \
  --table_name vector_table \
  --hdf5_file "${DATASET_NAME}.hdf5" \
  --k 10 \
  --metric cosine \
  --workers 8 \
  --explain true \
  --query_num 1 \
  --pgvector_hnsw_ef_search 120  
```  
  
7、删除索引, 调整hnsw和ivfflat索引参数, 并重建索引, 再次进行性能测试, 观察性能差别.  
  
  
  
  
# 附录  
1、 `test/load.py`  
  
```  
import h5py  
from multiprocessing import Process, JoinableQueue  
import psycopg2  
from psycopg2.extras import execute_values  
import numpy as np  
import argparse  
  
# 默认数据库参数  
DATABASE = "postgres"  
USER = "postgres"  
PASSWORD = ""  
HOST = "127.0.0.1"  
PORT = "5432"  
  
def parse_args():  
    parser = argparse.ArgumentParser(description="parser")  
  
    parser.add_argument("--database", type=str, default="postgres")  
    parser.add_argument("--user", type=str, default="postgres")  
    parser.add_argument("--password", type=str)  
    parser.add_argument("--host", type=str, default="127.0.0.1")  
    parser.add_argument("--port", type=str, default="5432")  
  
    parser.add_argument("--filename", type=str, required=True)  
    parser.add_argument("--tablename", type=str, required=True)  
  
    parser.add_argument("--batch_size", type=int, default=1000)  
    parser.add_argument("--dim_bytes", type=int, default=4)  
    parser.add_argument("--vec_offset", type=int, default=0)  
    parser.add_argument("--vec_num_limit", type=int, default=100000000)  
    parser.add_argument("--num_workers", type=int, default=8)  
  
    return parser.parse_args()  
  
def worker_process(task_queue, batch_size, tablename):  
  
    conn = psycopg2.connect(  
        database=DATABASE,  
        user=USER,  
        password=PASSWORD,  
        host=HOST,  
        port=PORT)  
  
    conn.autocommit = True  
    cursor = conn.cursor()  
  
    while True:  
        start_id, end_id, chunk = task_queue.get()  
        if start_id is None:  
            task_queue.task_done()  
            break  
  
        try:  
            # 为每个向量生成ID并准备插入数据  
            data = []  
            for vec_id, vec in zip(range(start_id, end_id+1), chunk):  
                data.append((vec_id, vec.tolist()))  # (id, vector)元组  
  
            execute_values(  
                cursor,  
                "INSERT INTO "+tablename+" (id, embedding) VALUES %s",  
                data,  
                page_size=batch_size  
            )  
        except Exception as e:  
            print(f"Error in worker: {e}")  
        finally:  
            task_queue.task_done()  
  
    cursor.execute("select count(*) from " + tablename)  
    result = cursor.fetchall()  
    print("[INFO] Final count:", result[0][0])  
    cursor.close()  
    conn.close()  
  
def read_h5py_file_and_insert(hdf5_filename, tablename, num_workers,  
                              batch_size, vec_offset, vec_num_limit):  
    # 创建任务队列, 最大容量为工作进程数的2倍  
    task_queue = JoinableQueue(maxsize=num_workers*2)  
  
    # 打开HDF5文件进行读取  
    f = h5py.File(hdf5_filename, 'r')  
    print(f"[INFO] HDF5文件中的数据集键: {list(f.keys())}")  
  
    # 获取向量维度, 优先从文件属性中读取, 否则从训练数据第一项推断  
    dim = int(f.attrs["dimension"]) if "dimension" in f.attrs else len(f["train"][0])  
    print(f"[INFO] 向量维度: {dim}")  
  
    # 读取训练数据集作为基础向量  
    base_vectors = np.array(f["train"])  
    print(f"[INFO] 成功加载基础向量集, 大小为 ({base_vectors.shape[0]} * {dim})")  
  
    # 创建工作进程  
    workers = []  
    for _ in range(num_workers):  
        p = Process(target=worker_process, args=(task_queue, batch_size, tablename))  
        p.start()  
        workers.append(p)  
  
    # 初始化已插入向量计数和当前ID  
    inserted_vec_num = vec_offset  
    vec_num = min(vec_num_limit, base_vectors.shape[0])  
    current_id = vec_offset+1  
  
    # 分批处理向量数据并添加到任务队列  
    while inserted_vec_num < vec_num:  
        actual_batch_size = min(batch_size, vec_num - inserted_vec_num)  
  
        # 为当前批次分配ID范围  
        start_id = current_id  
        end_id = start_id + actual_batch_size - 1  
        chunk = base_vectors[start_id-1:end_id, :]  
        task_queue.put((start_id, end_id, chunk))  # 发送ID范围和数据  
  
        inserted_vec_num += actual_batch_size  
        current_id += actual_batch_size  
  
        # 打印本次插入的记录数量  
        print(f"[INFO] 插入 {len(chunk)} 条记录到表 {tablename}, 从 {start_id} 到 {end_id}")  
  
    for _ in range(num_workers):  
        task_queue.put((None, None, None))  
  
    task_queue.join()  
  
    for p in workers:  
        p.join()  
  
  
if __name__ == "__main__":  
    args = parse_args()  
  
    DATABASE = args.database  
    USER = args.user  
    PASSWORD = args.password  
    HOST = args.host  
    PORT = args.port  
  
    read_h5py_file_and_insert(args.filename, args.tablename, args.num_workers,  
                              args.batch_size, args.vec_offset, args.vec_num_limit)  
```  
  
2、 `test/query.py`  
  
```  
import argparse  
import atexit  
import numpy as np  
import struct  
import multiprocessing  
from psycopg2 import pool  
from functools import partial  
from psycopg2.extras import execute_values  
import time  
import h5py  
  
  
def parse_arguments():  
  
    parser = argparse.ArgumentParser(description='query')  
  
    parser.add_argument('--explain', type=bool, default=False, help='explain_sql')  
    parser.add_argument('--hdf5_file', type=str, required=True, help='hdf5_file')  
    parser.add_argument("--table_name", type=str, required=True, help='table_name')  
  
    parser.add_argument('--host', type=str, default="127.0.0.1", help='PostgreSQL host')  
    parser.add_argument('--port', type=str, default="5432", help='PostgreSQL port')  
    parser.add_argument('--database', type=str, default="postgres", help='database name')  
    parser.add_argument('--user', type=str, default="postgres", help='username')  
    parser.add_argument('--password', help='password')  
  
    parser.add_argument('--k', type=int, default=10, help='top k')  
    parser.add_argument('--metric', choices=['l2', 'cosine', 'inner'], default='l2', help='distance metrics')  
    parser.add_argument('--max_vid', type=int, default=100000000, help='max vid')  
    parser.add_argument('--workers', type=int, default=8, help='worker num')  
  
    parser.add_argument('--pgvector_hnsw_ef_search', type=int, default=0, help='hnsw_ef_search')  
    parser.add_argument('--pgvector_ivf_probes', type=int, default=0, help='pgvector_ivf_probes')  
  
    parser.add_argument('--query_num', type=int, default=0, help='number of queries to run')  
    parser.add_argument('--offset', type=int, default=0, help='offset of queries to run')  
  
    return parser.parse_args()  
  
# 创建连接池  
connection_pool = None  
  
def close_worker_connection_pool():  
    global connection_pool  
    if connection_pool:  
        connection_pool.closeall()  
  
def init_worker_connection_pool(args):  
    """在子进程里初始化自己的连接池"""  
    global connection_pool  
    connection_pool = pool.SimpleConnectionPool(  
        minconn=1,  
        maxconn=2,  
        host=args.host,  
        port=args.port,  
        dbname=args.database,  
        user=args.user,  
        password=args.password  
    )  
  
    # 注册进程退出时关闭连接池  
    atexit.register(close_worker_connection_pool)  
  
def get_distance_operator(metric):  
  
    return {  
        'l2': '<->',  
        'cosine': '<=>',  
        'inner': '<#>'  
    }[metric]  
  
def write_logs(results, log_name):  
    for res in results:  
        query_idx = res['query_idx']  
        query_plan = res['plan']  
  
        with open(log_name, 'a', encoding='utf-8') as file:  
            file.write(f'query_id: {query_idx}\n')  
            file.write(f'log_info:\n {query_plan}\n')  
            file.write('--------------------------\n')  
  
def set_local_search_parameters(cur, args):  
  
    if args.pgvector_hnsw_ef_search != 0:  
        cur.execute(f"SET hnsw.ef_search = {args.pgvector_hnsw_ef_search}")  
  
    if args.pgvector_ivf_probes != 0:  
        cur.execute(f"SET ivfflat.probes = {args.pgvector_ivf_probes}")  
  
def cal_avg_recall(results, ground_truth, k, max_vid):  
    avg_recall = 0  
  
    query_num = len(results)  
  
    for res in results:  
        query_idx = res['query_idx']  
        query_results = res['results']  
  
        query_vid = []  
        for row in query_results:  
            query_vid.append(row[0])  
  
        ground_truth_of_query_idx = ground_truth[query_idx]  
  
        match_num = 0  
        compare_num = 0  
        for gt_vid in ground_truth_of_query_idx:  
            if(gt_vid <= max_vid):  
                compare_num += 1  
                if(compare_num > k):  
                    break  
  
                if((gt_vid+1) in query_vid):  
                    match_num += 1  
        recall = match_num / min(k, compare_num)  
        avg_recall += recall  
  
    avg_recall = avg_recall / query_num  
    return avg_recall  
  
def cal_latency_distribution(results):  
  
    latencies = []  
    for res in results:  
        latency = res['latency']  
        latencies.append(latency)  
  
    percentiles = [50, 90, 95, 99, 99.9]  
    print('\nlatency distribution:')  
    for p in percentiles:  
        print(f"{p}%", end="   ")  
    print()  
    for p in percentiles:  
        print(f"{np.percentile(latencies, p):.3f}ms" , end="   ")  
    print()  
    avg_latency = np.mean(latencies)  
    print(f'avg latency is: {avg_latency:.3f}ms')  
  
def worker_func(args, query_data):  
    query, query_idx = query_data  
    conn = connection_pool.getconn()  
    conn.autocommit = True  
  
    operator = get_distance_operator(args.metric)  
    vec_str = '[' + ','.join(map(str, query)) + ']'  
  
    try:  
        with conn.cursor() as cur:  
            set_local_search_parameters(cur, args)  
  
            execution_plan = ""  
  
            if(args.explain):  
                cur.execute(f"""  
                    EXPLAIN (ANALYZE, BUFFERS, VERBOSE)  
                    SELECT id FROM {args.table_name} ORDER BY embedding {operator} %s  
                    LIMIT %s  
                """, (vec_str, args.k))  
                execution_plan = "\n".join(row[0] for row in cur.fetchall())  
                tmp_search_result=[]  
                latency=0  
            else:  
                start = time.time()  
                cur.execute(f"""  
                    SELECT id FROM {args.table_name} ORDER BY embedding {operator} %s  
                    LIMIT %s  
                """, (vec_str, args.k))  
                end = time.time()  
                tmp_search_result = cur.fetchall()  
                latency = (end - start) * 1000  
  
            result = {  
                'query_idx': query_idx,  
                'results': tmp_search_result,  
                'latency': latency,  
                'metric': args.metric,  
                'plan': execution_plan  
            }  
  
    except Exception as e:  
        result = {  
            'query_idx': query_idx,  
            'error': str(e),  
            'results': [],  
            'latency': 0,  
            'plan': execution_plan  
        }  
        conn.rollback()  
    finally:  
        cur.close()  
        connection_pool.putconn(conn)  
  
    return result  
  
def query_database(args, queries):  
    query_data = [(query, idx) for idx, query in enumerate(queries)]  
    num_workers = args.workers  
  
    with multiprocessing.Pool(  
        num_workers,  
        initializer=init_worker_connection_pool,  
        initargs=(args,)  
    ) as pool:  
        worker = partial(worker_func, args)  
        results = pool.map(worker, query_data)  
  
    return results  
  
def get_output_file_name(args):  
    if args.pgvector_hnsw_ef_search != 0:  
        index = "hnsw"  
  
    if args.pgvector_ivf_probes != 0:  
        index = "ivf"  
  
    return f"explain_{args.table_name}_{index}.log"  
  
  
if __name__ == "__main__":  
    args = parse_arguments()  
  
    # 打开HDF5文件读取测试向量集  
    f = h5py.File(args.hdf5_file, 'r')  
    print(f"[INFO] HDF5文件中的数据集键: {list(f.keys())}")  
  
    queries = np.array(f["test"])  
    ground_truth = np.array(f["neighbors"])  
  
    # 根据 offset 和 query_num 参数控制查询范围  
    if args.query_num > 0:  
        query_num = min(args.query_num, len(queries))  
        offset = min(args.offset, len(queries) - 1)  
        queries = queries[offset:offset+query_num]  
        ground_truth = ground_truth[offset:offset+query_num]  
  
    print(f"[INFO] Read {len(queries)} query vectors(dim={queries.shape[1]})")  
    print(f"[INFO] Read {len(ground_truth)} ground truth ids(top k={ground_truth.shape[1]})")  
  
    if args.pgvector_hnsw_ef_search != 0:  
        print(f"[INFO] SET hnsw.ef_search = {args.pgvector_hnsw_ef_search}")  
  
    if args.pgvector_ivf_probes != 0:  
        print(f"[INFO] SET ivfflat.probes = {args.pgvector_ivf_probes}")  
  
    # 向量查询  
    start = time.time()  
    results = query_database(args, queries)  
    end = time.time()  
  
    # 异常报错  
    for res in results:  
        error = res.get('error')  
        if error:  
            print(f"[ERROR] query error: {error}")  
            exit(1)  
  
    # 计算执行时间与 QPS  
    execute_time = end - start  
    QPS = len(queries) / execute_time  
    print(f"\nquery finished, distance metric: {args.metric}, execute time is: {execute_time:.4f}s, QPS is: {QPS:.3f}")  
  
    if(args.explain):  
        file_name = get_output_file_name(args)  
        write_logs(results, file_name)  
    else:  
        cal_latency_distribution(results)  
        avg_recall = cal_avg_recall(results, ground_truth, args.k, args.max_vid)  
        print(f"avg recall@{args.k} of {len(queries)} queries is: {avg_recall:.6f}")  
```  
    
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
