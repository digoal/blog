## 大学生数据库实践课: 实验手册  
  
### 作者  
digoal  
  
### 日期  
2025-12-30  
  
### 标签  
PostgreSQL , DuckDB , 语义搜索 , 向量搜索 , 关键词检索 , 全文检索 , 标量检索 , 混合搜索 , 多模态搜索 , 重排序 , embedding , document split , ORC , 大模型 , 多模态大模型 , Dify , 压力测试 , 索引优化 , 倒排向量索引 , Ivfflat , 图向量索引 , HNSW , DiskANN , 量化 , rabitq , sbq , 二值量化 , bm25 , 相关性 , 相似性 , 召回率 , pgbench , 数据集 , ann-benchmarks , 图搜索 , PGQ , 递归 , 深度优先 , 广度优先 , 最短路径 , N度关系 , LLM , Ollama  
  
----  
  
## 背景  
实验环境: [《大学生数据库实践课, docker 镜像》](../202512/20251205_02.md)  
  
## pgbench 内置压测模型  
1、初始化数据  
  
```bash  
pgbench -i -s 100  
```  
  
2、读写测试  
  
```bash  
pgbench -M prepared -n -r -P 1 -c 10 -j 10 -T 120  
```  
  
3、只读测试  
  
```bash  
pgbench -M prepared -n -r -P 1 -c 10 -j 10 -T 120 -S  
```  
  
## pgbench 自定义压测  
  
参考  
  
[《沉浸式学习PostgreSQL|PolarDB 12: 如何快速构建 海量 逼真 测试数据》](../202309/20230906_02.md)  如何快速构建高斯分布的数据  
  
[《HTAP数据库 PostgreSQL 场景与性能测试之 30 - (OLTP) 秒杀 - 高并发单点更新》](../201711/20171107_31.md)  注意观察 `pg_try_advisory_xact_lock(id)` 条件带来的性能提升.  
  
[《HTAP数据库 PostgreSQL 场景与性能测试之 14 - (OLTP) 字符串搜索 - 全文检索》](../201711/20171107_15.md)   
  
[《HTAP数据库 PostgreSQL 场景与性能测试之 16 - (OLTP) 文本特征向量 - 相似特征(海明...)查询》](../201711/20171107_17.md)  复用测试数据生成方法, 但是将索引改成 hnsw 的海明距离搜索.  
  
## ollama  
  
1、部署、模型拉取实验  
  
参考 [《2025-大学生数据库实践课 : AI搜索技术(向量、关键词、标量、图、以及混合搜索)》](../202512/20251202_10.md)  命令部分  
  
2、ollama shell 交互  
  
```bash  
ollama run qwen3:0.6b  
```  
  
3、ollama shell 内配置模型调用参数  
  
```  
/set parameter  
```  
  
4、ollama api 交互  
  
参考  
  
[《大学生数据库实践课: 7.4 ollama API介绍》](../202512/20251210_16.md)  “使用curl调用Ollama API的实操例子” 章节  
  
https://docs.ollama.com/api/introduction  
  
5、ollama api 配置模型调用参数  
  
参考  
  
https://docs.ollama.com/api/introduction  
  
## 云端服务  
  
参考 阿里云百炼 API  
  
https://bailian.console.aliyun.com/  
   
补充 reranker API 的实验, 因ollama目前不支持 reranker API.  
  
**reranker 原理:**  
  
求查询语句 query 和 4个返回结果 document1-4 的相关性得分, 然后排序.    
  
正常情况下应该得到4个得分. 但是ollama暂时不支持reranker API.     
  
```  
ollama pull dengcao/Qwen3-Reranker-0.6B:Q8_0  
  
query="如何通过饮食调节来降低高血压？"  
document1="针对高血压患者，建议采用 DASH 饮食方案，即减少钠盐摄入，多吃富含钾、钙、镁的蔬菜水果和全谷物，这被证明能有效降低血压。"  
document2="减少食盐摄入是控制血压的关键，每日摄入量应控制在 5 克以下，同时应避免食用腌制食品。"  
document3="高血压是一种慢性病，除了药物治疗外，规律的体育锻炼、戒烟限酒以及保持心理平衡对于康复也至关重要。"  
document4="低血糖患者在饮食上应注意少食多餐，适量补充糖分，避免空腹剧烈运动，以防止晕厥发生。"  
  
  
curl http://localhost:11434/api/generate -d '{    
  "model": "qllama/bge-reranker-v2-m3:latest",    
  "stream": false,  
  "prompt": "query: ${query}\ndocument: ${document1}"    
}'  
  
curl http://localhost:11434/api/generate -d '{    
  "model": "dengcao/Qwen3-Reranker-0.6B:Q8_0",    
  "stream": false,  
  "prompt": "query: ${query}\ndocument: ${document2}"    
}'  
  
curl http://localhost:11434/api/generate -d '{    
  "model": "dengcao/Qwen3-Reranker-0.6B:Q8_0",    
  "stream": false,  
  "prompt": "query: ${query}\ndocument: ${document3}"    
}'  
  
curl http://localhost:11434/api/generate -d '{    
  "model": "dengcao/Qwen3-Reranker-0.6B:Q8_0",    
  "stream": false,  
  "prompt": "query: ${query}\ndocument: ${document4}"    
}'  
```  
  
可使用下面的方式模拟观察 reranker 的效果.  
  
假设通过标量查询、向量查询、关键词查询等混合查询返回了N条结果, 并已经使用 RRF 融合查询得分取出了前 4 条结果.    
  
然后我们要对这4条结果再进行 reranker.  
  
其实就是计算 query 的 embedding , 以及这4条结果的 embedding, 然后按向量距离进行排序.    
  
```
# 在宿主机上执行
query="如何通过饮食调节来降低高血压？"  
document1="针对高血压患者，建议采用 DASH 饮食方案，即减少钠盐摄入，多吃富含钾、钙、镁的蔬菜水果和全谷物，这被证明能有效降低血压。"  
document2="减少食盐摄入是控制血压的关键，每日摄入量应控制在 5 克以下，同时应避免食用腌制食品。"  
document3="高血压是一种慢性病，除了药物治疗外，规律的体育锻炼、戒烟限酒以及保持心理平衡对于康复也至关重要。"  
document4="低血糖患者在饮食上应注意少食多餐，适量补充糖分，避免空腹剧烈运动，以防止晕厥发生。"  
  
# 注意 reranker 的顺序和模型的参数量有极大关系
# 如果 模型不行, 建议还不如不要做 reranker 这步

curl http://localhost:11434/api/embed -d '{    
  "model": "qwen3-embedding:0.6b",    
  "stream": false,  
  "input": "${query}"    
}'  
  
curl http://localhost:11434/api/embed -d '{    
  "model": "qwen3-embedding:0.6b",    
  "stream": false,  
  "input": "${document1}"    
}'  
  
curl http://localhost:11434/api/embed -d '{    
  "model": "qwen3-embedding:0.6b",    
  "stream": false,  
  "input": "${document2}"    
}'  
  
curl http://localhost:11434/api/embed -d '{    
  "model": "qwen3-embedding:0.6b",    
  "stream": false,  
  "input": "${document3}"    
}'  
  
curl http://localhost:11434/api/embed -d '{    
  "model": "qwen3-embedding:0.6b",    
  "stream": false,  
  "input": "${document4}"    
}'  
```  
  
将向量结果放到 PostgreSQL 数据库中进行向量距离查询  
```
-- 进入容器在psql shell中执行
-- 创建向量插件
create extension if not exists vector;

-- 计算查询文本和结果文本的向量距离
with q(v) as (values ('[填入结果1]'::vector)),  
d(id,v) as (values (1, '[填入结果2]'::vector),(2, '[填入结果3]'::vector),(3, '[填入结果4]'::vector),(4, '[填入结果5]'::vector))  
select d.id,q.v <-> d.v as reranker_score from   
q,d; 
```  
    
  
## 文本切分方法和向量化  
  
参考 [《大学生数据库实践课: 9 embedding 文本切分实操》](../202512/20251210_18.md)  “简化版实操” 章节  
  
## dify  
  
参考 [《3分钟速玩Dify(高度可定制的企业级AI应用开源项目)》](../202504/20250404_01.md)  
  
## 向量检索实验  
  
### 1、生成模拟向量数据  
  
创建向量插件 vector 和 测试表 vector_test  
  
也包括了测试全文检索, 标量检索, 混合检索, 量化索引等需要的字段.  
  
```sql  
create extension if not exists vector;  
  
drop table if exists vector_test;  
  
create table vector_test (  
  id serial primary key,  
  category int,  
  price int,  
  label smallint[],  
  ts timestamp,  
  info text,  
  tsvec tsvector,  
  is_center boolean,  
  center_id int,  
  embedding vector(32)  
);  
```  
  
创建一个自定义函数, 生成取值范围是 0-N 的 M个随机数组  
```sql  
create or replace function gen_array(int default 127, int default 7) returns smallint[] as $$  
  select array(select (random()*$1)::smallint from generate_series(1, 3+(random()*$2)::int));  
$$ language sql strict;  
```  
  
创建一个自定义函数, 模拟生成用于全文检索的关键字字典, 可以理解为字典化后的 tokenize 稀疏向量.  
```sql  
create or replace function gen_rand_tsvector(int,int) returns tsvector as $$  
  select array_to_tsvector(array_agg((random()*$1)::int::text)) from generate_series(1,$2);  
$$ language sql strict;  
```  
  
创建一个自定义函数, 模拟真实场景, 输入N个中心点, 每个中心点周围按高斯分布扰动生成M个点, 共N*(M+1)个点.  
  
每个维度的取值范围为 `[-1.0, 1.0]`  
  
```sql  
CREATE OR REPLACE FUNCTION generate_vector_data(  
    center_count INT,          -- 中心点数量 (例如 1000)  
    points_per_center INT,     -- 每个中心点周围生成的点数 (例如 2 → 总计 2000)  
    dims INT,                  -- 向量维度（例如 16）  
    std_dev_mean FLOAT8 DEFAULT 0.1,    -- 类内标准差的均值  
    std_dev_sigma FLOAT8 DEFAULT 0.01   -- 类内标准差的分布标准差（即“聚集性强弱”的波动）  
) RETURNS TEXT AS $$  
DECLARE  
    center_id INT;  
    center_vec FLOAT8[];  
    new_vec FLOAT8[];  
    local_std_dev FLOAT8;      -- 每个中心点自己的类内标准差  
    perturbed_val FLOAT8;  
    i INT;  
    j INT;  
    k INT;  
    u1 FLOAT8;  
    u2 FLOAT8;  
    z0 FLOAT8;  
    z_std FLOAT8;              -- 用于生成 local_std_dev 的正态扰动  
    fuhao int;  
BEGIN  
    -- 1. 生成 center_count 个完全离散随机的中心点  
    FOR i IN 1..center_count LOOP  
        -- 生成 d 维 [0,1) 随机向量作为中心  
        center_vec := ARRAY(SELECT 2*random()-1 FROM generate_series(1, dims));  
  
        -- 为当前中心点生成其“类内标准差”：N(std_dev_mean, std_dev_sigma)，但必须 > 0  
        LOOP  
            u1 := random();  
            u2 := random();  
            -- 避免除零  
            IF u1 = 0 THEN u1 := 1e-10; END IF;  
            z_std := sqrt(-2.0 * ln(u1)) * cos(2.0 * pi() * u2);  
            local_std_dev := std_dev_mean + z_std * std_dev_sigma;  
            EXIT WHEN local_std_dev > 0;  -- 确保标准差为正  
        END LOOP;  
  
        -- 插入中心点  
        INSERT INTO vector_test (category, price, label, ts, info, tsvec, embedding, is_center)  
        VALUES (random()*9, random()*999, gen_array(), clock_timestamp(), md5(random()::text), gen_rand_tsvector(5000000, 64), center_vec::vector, TRUE)  
        RETURNING id INTO center_id;  
  
        -- 2. 围绕该中心点生成 points_per_center 个聚集点  
        FOR j IN 1..points_per_center LOOP  
            new_vec := '{}';  
            FOR k IN 1..dims LOOP  
                -- 生成标准正态扰动  
                u1 := random();  
                u2 := random();  
                IF u1 = 0 THEN u1 := 1e-10; END IF;  
                z0 := sqrt(-2.0 * ln(u1)) * cos(2.0 * pi() * u2);  
  
                select case ceil(random()-0.5) when 0 then -1 else 1 end case into fuhao;  
                -- 扰动 = 中心值 + N(0, local_std_dev)  
                perturbed_val := center_vec[k] + z0 * local_std_dev * fuhao;  
  
                -- 截断到 [-1, 1]  
                perturbed_val := GREATEST(-1.0, LEAST(1.0, perturbed_val));  
  
                new_vec := array_append(new_vec, perturbed_val);  
            END LOOP;  
  
            -- 插入非中心点，并可选记录 center_id（便于后续分析）  
            INSERT INTO vector_test (category, price, label, ts, info, tsvec, embedding, is_center, center_id)  
            VALUES (random()*9, random()*999, gen_array(), clock_timestamp(), md5(random()::text), gen_rand_tsvector(5000000, 64), new_vec::vector, FALSE, center_id);  
        END LOOP;  
    END LOOP;  
  
    RETURN 'Successfully generated ' ||  
           (center_count + center_count * points_per_center)::TEXT ||  
           ' vectors (' || center_count || ' centers + ' ||  
           (center_count * points_per_center) || ' neighbors).';  
END;  
$$ LANGUAGE plpgsql;  
```  
  
定制pgbench脚本, 用来生成测试数据  
```sql  
\! echo "SELECT generate_vector_data(  
    :center_count,  
    :points_per_center,  
    :dims,  
    :std_dev_mean,  
    :std_dev_sigma  
);" > ~/1.sql  
```  
  
  
使用pgbench , 10个进程, 每个负责100个中心点, 共1000个中心点. 每个中心点附近999条记录. 32个维度. 共100万条记录 :  
```sql  
-- 插入100万条测试数据  
-- category: 0-9  
-- price: 0-1000  
-- label: 3-10 个 0-127的随机数组成的数组  
-- embedding: 32维的向量  
  
  
\! pgbench -M prepared -n -r -f ~/1.sql -D center_count=100 -D points_per_center=999 -D dims=32 -D std_dev_mean=0.1 -D std_dev_sigma=0.01 -c 10 -j 10 -t 1  
```  
  
确认数据是否正常?  
  
查询记录数  
```sql  
select count(*) from vector_test;  
  
  
  count  
---------  
 1000000  
(1 row)  
```  
  
查看表大小  
```sql  
\dt+  
  
  
                                         List of tables  
 Schema |      Name      | Type  |  Owner   | Persistence | Access method |  Size  | Description  
--------+----------------+-------+----------+-------------+---------------+--------+-------------  
 public | vector_test    | table | postgres | permanent   | heap          | 977 MB |  
```  
  
查看数据样本  
```sql  
select * from vector_test limit 10;  
```  
  
### 2、观察向量索引带来的性能提升  
  
1、无索引  
```sql  
\timing  
  
set max_parallel_workers_per_gather = 0;  
set jit=off;  
  
-- 查看执行计划, 是不是使用了 seq scan 全表扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
2、观察创建 hnsw 索引的耗时  
  
```sql  
-- 设置创建hnsw索引构图时最大可用内存, 设置并行度, 创建hnsw 图向量索引.  
-- 注意 m=16, ef_construction=64 的含义  
-- 注意 vector_l2_ops 必须和查询时的距离算法匹配, vector_l2_ops 表示计算欧式距离 <-> .  还有  
-- vector_l1_ops , <+>  
-- vector_cosine_ops , <=>  
-- vector_ip_ops , <#>  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
drop index if exists idx_vector_test_hnsw;  
create index idx_vector_test_hnsw on vector_test using hnsw (embedding vector_l2_ops) with (m=16, ef_construction=64);  
  
  
-- 可以在另一个 psql 页面观察创建hnsw索引的完成进度  
SELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS "%" FROM pg_stat_progress_create_index;  
```  
  
3、观察 hnsw 索引查询耗时  
```sql  
set max_parallel_workers_per_gather = 0;  
set jit=off;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果, 记录耗时  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
### 3、召回率实验  
  
1、理解如何统计召回率.  
  
召回率等于“索引近似搜索”和“全表精确扫描”的交集占比  
  
```sql  
-- 例如下面计算的是: ef_search = 45 时, top-50 recall .  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32);  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 45;  -- 检查当查询参数 hnsw.ef_search = 45 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
2、观察搜索参数如何影响召回率  
  
```sql  
-- 加大搜索参数, 可提高召回率  
-- 例如下面计算的是: ef_search = 200 时, top-50 recall .  
-- 对比前面 hnsw.ef_search = 45 时的 top-50 recall .  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 200;  -- 检查当查询参数 hnsw.ef_search = 200 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
  
思考一下, 既然直接修改 ef_search 就可以提升recall, 为什么还要重建索引这么麻烦来修改 m 和 ef_construction ?  
  
  
3、观察索引参数如何影响召回率  
  
保持m不变, 加大ef_construction, 可提高召回率. 但会使创建索引的时间变长.  
  
```sql  
-- 请自行尝试  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
drop index if exists idx_vector_test_hnsw;  
create index idx_vector_test_hnsw on vector_test using hnsw (embedding vector_l2_ops) with (m=16, ef_construction=1000);  
  
  
-- 例如下面计算的是: ef_search = 45 时, top-50 recall .  
-- 对比前面 m=16, ef_construction=64 , hnsw.ef_search = 45 时的 top-50 recall .  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 45;  -- 检查当查询参数 hnsw.ef_search = 45 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
  
加大m, 可提高召回率. 但会使创建索引的时间变长.  
  
```sql  
-- 请自行尝试  
  
-- 例如下面计算的是: ef_search = 45 时, top-50 recall .  
-- 对比前面 m=16, ef_construction=64 , hnsw.ef_search = 45 时的 top-50 recall .  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
drop index if exists idx_vector_test_hnsw;  
create index idx_vector_test_hnsw on vector_test using hnsw (embedding vector_l2_ops) with (m=32, ef_construction=64);  
  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 45;  -- 检查当查询参数 hnsw.ef_search = 45 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
### 4、观察同等条件下, 索引带来的性能提升  
对比在满足一定 recall 前提下, 有索引和无索引的查询性能  
  
记录全表扫描查询性能  
  
```sql  
set max_parallel_workers_per_gather = 0;  
set jit=off;  
set enable_seqscan=on;  
set enable_bitmapscan=off;  
set enable_indexscan=off;  
set hnsw.ef_search = 20;  
  
-- 查看执行计划, 是不是使用了 seq scan 全表扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果和性能  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
记录 hnsw 索引扫描查询性能  
```sql  
set max_parallel_workers_per_gather = 0;  
set jit=off;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果和性能  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
### 5、对比hnsw和ivfflat索引  
对比hnsw和ivfflat索引的创建耗时、同等召回率下的查询性能  
  
1、创建 ivfflat 索引  
```sql  
-- 删除hnsw索引  
drop index if exists idx_vector_test_hnsw;  
-- 创建ivfflat索引. 思考 lists 和 ivfflat.probes 如何设置最佳?  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_ivfflat on vector_test using ivfflat(embedding vector_l2_ops) with (lists=1000);  
  
-- 可以在另一个 psql 页面观察创建ivfflat索引的完成进度  
SELECT phase, round(100.0 * tuples_done / nullif(tuples_total, 0), 1) AS "%" FROM pg_stat_progress_create_index;  
```  
  
2、ivfflat 索引 top-50 recall  
```  
-- 注意调整 ivfflat.probes 参数, 观察对召回的影响  
  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local ivfflat.probes = 1;  -- 检查当查询参数 ivfflat.probes = 1 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
3、ivfflat 索引查询性能  
```  
set jit=off;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
set ivfflat.probes = 5;  
  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果和性能  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
### 6、量化索引实验  
  
  
1、了解量化  
```sql  
select binary_quantize('[1,10,100,1000,-1,0,-100,-234]'::vector);  
  
select id, binary_quantize(embedding), embedding from vector_test where center_id=1 limit 10;  
```  
  
2、创建量化索引  
```sql  
-- 删除旧索引  
drop index if exists idx_vector_test_ivfflat;  
-- 创建量化表达式索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_hnsw on vector_test using hnsw ((binary_quantize(embedding)::bit(32)) bit_hamming_ops) with (m=16, ef_construction=64);  
```  
  
3、使用量化索引  
```sql  
-- 注意 使用 <~> 表示 hamming 距离  
set hnsw.ef_search = 20;  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain select id, category, price, label, ts, info,  
  binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) as distance  
from vector_test  
  order by binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) limit 10;  
  
-- 查询看看结果和性能  
select id, category, price, label, ts, info,  
  binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) as distance  
from vector_test  
  order by binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) limit 10;  
```  
  
4、量化索引召回率测试  (思考: 为什么量化后的召回率差? https://deepwiki.com/search/-11-labelsbq_88a9a072-efd1-409d-b137-071f320da724?mode=fast )  
```sql  
-- 采用 hamming 距离  
-- 计算 ef_search = 45 时, top-50 recall .  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32);  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 45;  -- 检查当查询参数 hnsw.ef_search = 45 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by binary_quantize(embedding)::bit(32) <~> binary_quantize(v_embedding)::bit(32) limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
5、使用量化索引和二次精排  
```sql  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain  
with a as (  
select id, category, price, label, ts, info, embedding,  
  binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) as distance  
from vector_test  
  order by binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) limit 500  
)  
select id, category, price, label, ts, info from a order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
  
-- 查询看看结果和性能  
with a as (  
select id, category, price, label, ts, info, embedding,  
  binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) as distance  
from vector_test  
  order by binary_quantize(embedding)::bit(32) <~> (select binary_quantize(embedding)::bit(32) from vector_test where id=1234) limit 500  
)  
select id, category, price, label, ts, info from a order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
6、采用二次精排, 提高 recall  
  
```sql  
-- 采用 hamming 距离  
-- 采用二次精排  
-- 计算 ef_search = 500 , 量化召回 500 条后, 精排序 top-50 recall .  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32);  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local hnsw.ef_search = 500;  -- 检查当查询参数 hnsw.ef_search = 500 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array( with a as (select id,embedding from vector_test order by binary_quantize(embedding)::bit(32) <~> binary_quantize(v_embedding)::bit(32) limit 500) select id from a order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```
  
**未来再补充: vectorchord的scalar8类型和相关量化方法, 对比采用以上二值量化的recall,QPS.**  
  
### 7、体验目前综合能力最强向量索引 vectorchord  
vectorchord 做到了索引构建速度快、召回率高和查询性能好的兼得.  
  
对比 vchordrq 和 hnsw 索引的创建耗时、同等召回率下的查询性能  
  
  
1、配置参数  
  
```sql  
\! echo "shared_preload_libraries='vchord,pg_tokenizer,vchord_bm25'" >> /var/lib/postgresql/18/docker/postgresql.auto.conf  
```  
  
2、重启 docker 容器  
  
```bash  
-- 先退出到 powershell / cmd / shell 命令行  
-- 重启后进入 psql 命令行  
docker stop pg  
docker start pg  
docker exec -ti pg bash  
psql  
```  
  
3、创建 vchord 向量插件  
```sql  
create extension if not exists vchord;  
```  
  
4、构建 vchordrq 向量索引  
```sql  
-- 删除 hnsw 量化向量索引  
drop index if exists idx_vector_test_hnsw;  
  
-- 创建 vchordrq 索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_vchord on vector_test using vchordrq(embedding vector_l2_ops) with (  
options = $$  
residual_quantization = true  
[build.internal]  
lists = [1000]  
build_threads = 8  
spherical_centroids = true  
kmeans_algorithm.hierarchical = {}  
[build]  
pin=2  
$$  
);  
```  
  
  
检查召回率 top-50 recall  
```sql  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local vchordrq.probes = 1;  -- 检查当查询参数 vchordrq.probes = 1 时, 返回 top-50 的 recall 的值是多少?  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test order by embedding <-> v_embedding limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
观察 vchordrq 索引查询性能  
```sql  
set jit=off;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
set vchordrq.probes = 1;  
  
-- 查看执行计划, 是不是使用了 index scan 索引扫描  
explain select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
  
-- 查询看看结果和性能  
select id, category, price, label, ts, info,  
  embedding <-> (select embedding from vector_test where id=1234) as distance  
from vector_test  
  order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
  
## 全文检索实验  
  
1、查询数据样本  
  
```sql  
select tsvec from vector_test limit 10;  
```  
  
2、从样本提取几个值  
  
```  
'1264451' '138541'  
```  
  
3、包含任意关键词 或 所有关键词的查询, 全表扫描  
  
观察全表扫描的执行计划和查询性能  
```sql  
-- 包含以下任意关键词  
explain select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
  
-- 包含以下所有关键词  
explain select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
```  
  
4、包含任意关键词 或 所有关键词的查询, 索引扫描  
  
创建 gin 倒排索引  
  
```sql  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_tsvec on vector_test using gin (tsvec);  
```  
  
观察索引扫描的执行计划和查询性能  
  
```sql  
-- 包含以下任意关键词  
explain select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
  
-- 包含以下所有关键词  
explain select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
select id, tsvec from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
```  
  
对比全表扫描和索引扫描的耗时  
  
```sql  
-- 全表扫描  
set enable_seqscan=on;  
set max_parallel_workers_per_gather =0;  
set enable_bitmapscan=off;  
  
-- 包含以下任意关键词  
explain select id from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
select id from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
  
-- 包含以下所有关键词  
explain select id from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
select id from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
  
  
-- 索引扫描  
set enable_seqscan=off;  
set max_parallel_workers_per_gather =0;  
set enable_bitmapscan=on;  
-- 包含以下任意关键词  
explain select id from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
select id from vector_test where tsvec @@ to_tsquery('1264451 | 138541');  
  
-- 包含以下所有关键词  
explain select id from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
select id from vector_test where tsvec @@ to_tsquery('1264451 & 138541');  
```
  
### 全文检索之 bm25  
```

-- 创建 pg_tokenizer 插件, 支持将文本转换为关键字/稀疏向量
CREATE EXTENSION IF NOT EXISTS pg_tokenizer CASCADE;  -- for tokenizer

-- 创建 vchord_bm25 插件, 支持自动更新bm25算法所需的统计信息, 构建 bm25 搜索和相关性排序索引
CREATE EXTENSION IF NOT EXISTS vchord_bm25 CASCADE;   -- for bm25 ranking

-- 设置 schema 查询路径, 否则会找不到 以上2个插件 相关的类型和内置函数等
set search_path="$user", bm25_catalog, tokenizer_catalog, public;

-- 创建文本表
CREATE TABLE documents (
    id SERIAL8 PRIMARY KEY,
    passage TEXT,  -- 存储原始文本
    embedding bm25vector  -- 由触发器自动将原始文本转换为字典, 再自动根据字典更新这个稀疏向量
);

-- 创建一个文本分析器, 采用内置的支持中英文的jieba 分词器
-- create a text analyzer which uses jieba pre-tokenizer
SELECT create_text_analyzer('text_analyzer1', $$
[pre_tokenizer.jieba]
$$);

-- 在文本表上创建触发器. 将 :
--   自动创建一个自定义分词器/tokenizer(采用前面定义好的文本分析器), 
--   自动创建字典表并自动更新 bm25 算法模型需要的统计信息(下面例子模型命名为model1)
SELECT create_custom_model_tokenizer_and_trigger(
    tokenizer_name => 'tokenizer1',  --  自定义分词器/tokenizer 
    model_name => 'model1',   -- 将自动创建一张字典表, 名字为 model_model1 
    text_analyzer_name => 'text_analyzer1',  前面定义好的文本分析器  
    table_name => 'documents',  -- 指定 表名、文本字段、稀疏向量字段
    source_column => 'passage',
    target_column => 'embedding'
);

-- 查询模型表 model_model1 , 现在还没有往表里写入数据, 所以下面的 query 应该没有记录
select * from model_model1;  -- 现在应该有字典了.

-- 观察 documents 表, 应该会出现2个触发器
\d+ documents



                                                       Table "bm25_catalog.documents"
  Column   |    Type    | Collation | Nullable |                Default                | Storage  | Compression | Stats target | Description 
-----------+------------+-----------+----------+---------------------------------------+----------+-------------+--------------+-------------
 id        | bigint     |           | not null | nextval('documents_id_seq'::regclass) | plain    |             |              | 
 passage   | text       |           |          |                                       | extended |             |              | 
 embedding | bm25vector |           |          |                                       | external |             |              | 
Indexes:
    "documents_pkey" PRIMARY KEY, btree (id)
Not-null constraints:
    "documents_id_not_null" NOT NULL "id"
Triggers:
    model_model1_trigger BEFORE INSERT OR UPDATE OF passage ON documents FOR EACH ROW EXECUTE FUNCTION custom_model_insert_trigger('model1', 'passage', 'text_analyzer1')
    model_model1_trigger_insert BEFORE INSERT OR UPDATE OF passage ON documents FOR EACH ROW EXECUTE FUNCTION custom_model_tokenizer_set_target_column_trigger('tokenizer1', 'passage', 'embedding')
Access method: heap



-- 插入测试数据
INSERT INTO documents (passage) VALUES 
('红海早过了，船在印度洋面上开驶着，但是太阳依然不饶人地迟落早起，侵占去大部分的夜。'),
('夜仿佛纸浸了油变成半透明体；它给太阳拥抱住了，分不出身来，也许是给太阳陶醉了，所以夕照晚霞褪后的夜色也带着酡红。'),
('到红消醉醒，船舱里的睡人也一身腻汗地醒来，洗了澡赶到甲板上吹海风，又是一天开始。'),
('这是七月下旬，合中国旧历的三伏，一年最热的时候。在中国热得更比常年利害，事后大家都说是兵戈之象，因为这就是民国二十六年【一九三七年】。'),
('这条法国邮船白拉日隆子爵号（VicomtedeBragelonne）正向中国开来。'),
('早晨八点多钟，冲洗过的三等舱甲板湿意未干，但已坐满了人，法国人、德国流亡出来的犹太人、印度人、安南人，不用说还有中国人。'),
('海风里早含着燥热，胖人身体给炎风吹干了，上一层汗结的盐霜，仿佛刚在巴勒斯坦的死海里洗过澡。'),
('毕竟是清晨，人的兴致还没给太阳晒萎，烘懒，说话做事都很起劲。'),
('那几个新派到安南或中国租界当警察的法国人，正围了那年轻善撒娇的犹太女人在调情。'),
('俾斯麦曾说过，法国公使大使的特点，就是一句外国话不会讲；这几位警察并不懂德文，居然传情达意，引得犹太女人格格地笑，比他们的外交官强多了。'),
('这女人的漂亮丈夫，在旁顾而乐之，因为他几天来，香烟、啤酒、柠檬水沾光了不少。'),
('红海已过，不怕热极引火，所以等一会甲板上零星果皮、纸片、瓶塞之外，香烟头定又遍处皆是。'),
('法国人的思想是有名的清楚，他的文章也明白干净，但是他的做事，无不混乱、肮脏、喧哗，但看这船上的乱糟糟。'),
('这船，倚仗人的机巧，载满人的扰攘，寄满人的希望，热闹地行着，每分钟把沾污了人气的一小方小面，还给那无情、无尽、无际的大海。');

-- 再次查询模型表 model_model1
select * from model_model1;  -- 现在应该有字典了.

-- 创建索引
CREATE INDEX documents_embedding_bm25 ON documents USING bm25 (embedding bm25_ops);

-- 使用该索引进行关键词检索.
set enable_seqscan=off;
set enable_indexscan=on;

-- 注意, to_bm25query 里面使用的 tokenize 必须和被查询表 documents 使用的一致: tokenizer1  
explain SELECT id, passage, embedding <&> to_bm25query('documents_embedding_bm25', tokenize('犹太女人', 'tokenizer1')) AS rank
FROM documents
ORDER BY rank
LIMIT 10;

SELECT id, passage, embedding <&> to_bm25query('documents_embedding_bm25', tokenize('犹太女人', 'tokenizer1')) AS rank
FROM documents
ORDER BY rank
LIMIT 10;

-- 调整 bm25 得分排名第一的句子, 加一些无关的内容.  
update documents set passage =passage || '夜仿佛纸浸了油变成半透明体；它给太阳拥抱住了，分不出身来，也许是给太阳陶醉了，所以夕照晚霞褪后的夜色也带着酡红。' where id=9;

-- 按照 bm25 得分计算原理, 匹配到的关键词一样的情况下, 句子越长, 得分应该越低.  
-- 验证是否符合预期
SELECT id, passage, embedding <&> to_bm25query('documents_embedding_bm25', tokenize('犹太女人', 'tokenizer1')) AS rank
FROM documents
ORDER BY rank
LIMIT 10;

-- 验证 bm25_limit 的作用
-- 默认为多少?
show bm25_catalog.bm25_limit ;

-- 修改为 1 , 还能返回10条记录吗? 
set bm25_catalog.bm25_limit =1;

SELECT id, passage, embedding <&> to_bm25query('documents_embedding_bm25', tokenize('犹太女人', 'tokenizer1')) AS rank
FROM documents
ORDER BY rank
LIMIT 10;

-- 重制前面修改过的参数
reset enable_seqscan;
reset enable_indexscan;
```  
  
## 混合搜索实验  
  
混合搜索SQL  
```sql  
set vchordrq.probes=20;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
  
select id from vector_test where category=1 and price<5 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
### 1、多索引, 数据库自动根据代价选择某个条件的索引  
  
观察当前的SQL用了什么索引?  
```sql  
-- 创建标量索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_category_price on vector_test (category, price);  
  
set vchordrq.probes=20;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
explain (analyze,verbose,timing,costs,buffers) select id from vector_test where category=1 and price<5 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
select id from vector_test where category=1 and price<5 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
如果把`price<5`条件去掉呢?  用了哪个索引?  
  
```sql  
set vchordrq.probes=20;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
explain (analyze,verbose,timing,costs,buffers) select id from vector_test where category=1 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
select id from vector_test where category=1 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
  
### 2、对比多索引和分区索引的性能区别  
什么情况下, 多索引不能解决问题?  
  
使用任何单一的索引时, 命中满足其他字段条件的记录都很少, 需要扫描很多的索引页.  
  
使用单一非向量索引时, 返回的结果集太大, 需要进行向量排序的记录数过多.  
  
多索引性能不佳的原因: 选择单一索引要更多的cpu运算和IO, 高并发时qps不佳.  
  
1、使用hnsw索引进行对照测试  
```sql  
-- 删除 hnsw 量化向量索引  
drop index if exists idx_vector_test_vchord;  
  
-- 创建 hnsw 索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_hnsw on vector_test using hnsw(embedding vector_l2_ops) with (m=16, ef_construction=64);  
```  
  
查看执行计划, 确认使用了向量索引?  
```  
explain (analyze,verbose,timing,costs,buffers) select id from vector_test where category=1 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
收集一份 qps 数据, 表示 选择单一索引要更多的cpu运算和IO, 高并发时qps不佳.  
  
```  
\! echo "  
set vchordrq.probes=20;  
set enable_seqscan=off;  
set enable_bitmapscan=off;  
set enable_indexscan=on;  
select id from vector_test where category=1 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
" > ~/2.sql  
  
\! pgbench -M prepared -n -r -P 1 -f ~/2.sql -c 10 -j 10 -T 120  
```  
  
2、使用 partial index 模拟“支持分区的向量索引”, 提升混合搜索性能  
  
```sql  
-- 删除 hnsw 量化向量索引  
drop index if exists idx_vector_test_hnsw;  
  
-- 创建 hnsw 索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_hnsw on vector_test using hnsw(embedding vector_l2_ops) with (m=16, ef_construction=64)  
where category=1;   -- 这里加了一个条件, 创建partial index, 与SQL的where条件匹配  
```  
  
查看执行计划, 确认使用了向量索引?  
```  
explain (analyze,verbose,timing,costs,buffers) select id from vector_test where category=1 order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
观察 “支持分区的向量索引” 带来的性能提升  
```  
\! pgbench -M prepared -n -r -P 1 -f ~/2.sql -c 10 -j 10 -T 120  
```  
  
### 3、向量+标签混合搜索  
向量+标签混合搜索SQL  
```sql  
select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
1、仅使用向量索引的性能  
  
```sql  
-- 删除 hnsw 量化向量索引  
drop index if exists idx_vector_test_hnsw;  
  
-- 创建 hnsw 索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_hnsw on vector_test using hnsw(embedding vector_l2_ops) with (m=16, ef_construction=64);  
```  
  
开始压测  
```  
\! echo "  
set hnsw.ef_search=20;  
select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
" > ~/3.sql  
```  
  
记录性能  
```  
\! pgbench -M prepared -n -r -P 1 -f ~/3.sql -c 10 -j 10 -T 120  
```  
  
2、使用向量+标签混合索引的性能.  
  
创建vectorscale 插件, 创建带标签的diskann图索引.  
  
相当于两张逻辑图, 图中支持label或公共入口点, 带label时可快速进行裁剪.  
  
```sql  
-- 创建vectorscale diskann 向量索引插件  
create extension if not exists vectorscale ;  
  
-- 删除 hnsw 量化向量索引  
drop index if exists idx_vector_test_hnsw ;  
  
-- 创建 diskann 索引  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_diskann on vector_test using diskann(embedding vector_l2_ops, label) with (  
num_neighbors=20,  -- 每个节点的最大邻居数  
search_list_size=40,  -- 构图时的最大候选列表大小  
max_alpha=1.2,  -- 图连接的紧密程度, 设大有可能引入较远地方的点, 使得图的搜索性能更好. 但也不能太大可能导致性能变差, 类似lora, 优化过度了.  
storage_layout='memory_optimized',  -- 使用量化  
num_bits_per_dimension = 8  -- 每个维度量化为几个bit ?  
);  
  
-- 可以在另一个 psql 页面观察创建hnsw索引的完成进度  
SELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS "%" FROM pg_stat_progress_create_index;  
```  
  
检查是否使用了diskann索引  
```sql  
set diskann.query_search_list_size = 1000;  
set diskann.query_rescore = 500;  -- 如果索引使用了量化技术, 建议回表进行重排序 , 获得更好的 recall  
explain (analyze,verbose,timing,costs,buffers) select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
```  
  
检查召回率  ( 思考: 为什么召回率差?  https://deepwiki.com/search/-11-labelsbq_b8449e49-68f2-4b3c-b281-6c01d9a60e81?mode=fast )  
```sql  
do $$  
declare  
  v_indexscan int[];  
  v_seqscan int[];  
  v_recall float8;  
  v_topk int := 50;  -- top-50, 返回50条与查询向量 v_embedding 最相似的结果  
  v_embedding vector(32) ;  
begin  
  select embedding into v_embedding from vector_test where id=1234;  
  set local diskann.query_search_list_size = 1000;  -- 检查当查询参数 diskann.query_search_list_size = 1000 时, 返回 top-50 的 recall 的值是多少?  
  set local diskann.query_rescore = 500;    -- 前500条索引扫描的结果回表使用 embedding <-> embedding 精排  
  set local enable_seqscan=off;  
  set local enable_indexscan=on;  
  select array(select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit v_topk) into v_indexscan;  
  set local enable_seqscan=on;  
  set local enable_indexscan=off;  
  select array(select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit v_topk) into v_seqscan;  
  select count(*)/v_topk::numeric into v_recall from unnest(v_seqscan) as t1  
    join unnest(v_indexscan) as t2 on t1 = t2;  
  raise notice '返回 top-50, recall值为: % %%', 100*v_recall;  
end  
$$ language plpgsql;  
```  
  
保证召回率低情况下, 对比 diskann 与没有混合索引时的性能差别  
  
```  
\! echo "  
set diskann.query_search_list_size =1000;  
set diskann.query_rescore = 500;  
select id from vector_test where label && array[1]::smallint[] order by embedding <-> (select embedding from vector_test where id=1234) limit 10;  
" > ~/4.sql  
```  
  
记录性能  
```  
\! pgbench -M prepared -n -r -P 1 -f ~/4.sql -c 10 -j 10 -T 120  
```  
  
### 4、王炸: RRF 例子  
改写SQL, 把各种条件单独进行查询, 然后用RRF算法融合排序, 使得每个索引都能用上, 最高效.  
  
0、将向量索引恢复到最常用的 hnsw 索引.  
  
```sql  
drop index if exists idx_vector_test_diskann;  
  
set maintenance_work_mem ='3GB';  
set max_parallel_maintenance_workers =8;  
create index idx_vector_test_hnsw on vector_test using hnsw(embedding vector_l2_ops) with (m=16, ef_construction=64);  
```  
  
1、分解SQL  
  
标量条件 `category = 1 and price < 100`  `ORDER BY price`  
  
全文检索条件 `tsvec @@ to_tsquery('1264451 | 138541')`  `ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC`  
  
向量条件 `order by embedding <-> (select embedding from vector_test where id=1234)`  
  
2、RRF SQL  
  
使用RRF算法融合 向量查询、全文检索 和 标量查询的3个结果. 每种排序取出前 20条, 取最终取融合排序后的前10条.  
  
```sql  
-- 先从表中取出一条真实存在的向量作为目标向量. 替换下面SQL语句中的向量  
select embedding from vector_test where id=1234;  
  
-- 融合查询  
WITH  
-- 向量查询, 提取相关的前20条  
vector_results AS (  
    SELECT id,  
           -- 窗口函数 row_number() 的作用是取出结果顺序号.  
           ROW_NUMBER() OVER (ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)) as vector_rank  
    FROM vector_test  
    ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)  
    LIMIT 20  
),  
-- 标量查询, 提取符合标量查询条件的前20条  
scale_results AS (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY price) as scale_rank  
    FROM vector_test  
    WHERE category = 5 AND price < 100  
    ORDER BY price  
    LIMIT 20  
),  
-- 全文检索, 提取符合全文检索条件的前20条  
-- 备注: ts_rank_cd 是一种全文检索相关性统计算法, 计算查询条件和矢量文本之间的相关性, 值越大越相关  
tsvec_results as (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC) as tsvec_rank  
    FROM vector_test  
    WHERE tsvec @@ to_tsquery('1264451 | 138541')  
    ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC  
    LIMIT 20  
),  
-- 最终使用RRF进行重排  
-- 向量权重: 0.5, 标量权重: 0.25, 全文检索权重: 0.25  
rrf_scores AS (  
    SELECT  
        COALESCE(v.id, t.id, ts.id) as id,  
        -- 100 为惩罚值, 表示仅在某一种查询中存在的行. 例如只在向量查询中有返回, 但是其他查询中无返回.  
        -- 惩罚值必须大于最终返回的LIMIT 条数, 这里是10, 所以惩罚值必须大于10  
        -- 惩罚值也可以设置为不一样. 例如你希望向量查询的惩罚值低一点, 那就设小一点, 让它优先于标量和全文检索查询结果  
        COALESCE(v.vector_rank, 100) as vector_rank,  
        COALESCE(t.scale_rank, 200) as scale_rank,  
        COALESCE(ts.tsvec_rank, 200) as tsvec_rank,  
        -- RRF算法：k=60 , 向量权重 0.5 , scale 权重 0.25 , 全文检索 权重 0.25  
        (0.5 * 1.0/(60 + COALESCE(v.vector_rank, 100)) +  
         0.25 * 1.0/(60 + COALESCE(t.scale_rank, 200)) +  
         0.25 * 1.0/(60 + COALESCE(ts.tsvec_rank, 200))) as rrf_score  
    FROM vector_results v  
    FULL OUTER JOIN scale_results t USING(id)  
    FULL OUTER JOIN tsvec_results ts USING(id)  
)  
SELECT p.id, p.category, p.price,  -- 这里还可以添加要返回的 vector_test 表的其他字段  
       r.rrf_score, r.vector_rank, r.scale_rank, r.tsvec_rank  
FROM rrf_scores r  
JOIN vector_test p ON r.id = p.id  
-- 按融合后的分值排序, 分越大越相关. 取出前 10 条.  
ORDER BY r.rrf_score DESC  
LIMIT 10;  
```  
  
3、观察 RRF SQL 执行计划, 确保每个条件都用了最佳索引  
  
```sql  
set max_parallel_workers_per_gather =0;  
set jit=off;  
  
explain (analyze,verbose,timing,costs,buffers)  
WITH  
-- 向量查询, 提取相关的前20条  
vector_results AS (  
    SELECT id,  
           -- 窗口函数 row_number() 的作用是取出结果顺序号.  
           ROW_NUMBER() OVER (ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)) as vector_rank  
    FROM vector_test  
    ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)  
    LIMIT 20  
),  
-- 标量查询, 提取符合标量查询条件的前20条  
scale_results AS (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY price) as scale_rank  
    FROM vector_test  
    WHERE category = 5 AND price < 100  
    ORDER BY price  
    LIMIT 20  
),  
-- 全文检索, 提取符合全文检索条件的前20条  
-- 备注: ts_rank_cd 是一种全文检索相关性统计算法, 计算查询条件和矢量文本之间的相关性, 值越大越相关  
tsvec_results as (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC) as tsvec_rank  
    FROM vector_test  
    WHERE tsvec @@ to_tsquery('1264451 | 138541')  
    ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC  
    LIMIT 20  
),  
-- 最终使用RRF进行重排  
-- 向量权重: 0.5, 标量权重: 0.25, 全文检索权重: 0.25  
rrf_scores AS (  
    SELECT  
        COALESCE(v.id, t.id, ts.id) as id,  
        -- 100 为惩罚值, 表示仅在某一种查询中存在的行. 例如只在向量查询中有返回, 但是其他查询中无返回.  
        -- 惩罚值必须大于最终返回的LIMIT 条数, 这里是10, 所以惩罚值必须大于10  
        -- 惩罚值也可以设置为不一样. 例如你希望向量查询的惩罚值低一点, 那就设小一点, 让它优先于标量和全文检索查询结果  
        COALESCE(v.vector_rank, 100) as vector_rank,  
        COALESCE(t.scale_rank, 200) as scale_rank,  
        COALESCE(ts.tsvec_rank, 200) as tsvec_rank,  
        -- RRF算法：k=60 , 向量权重 0.5 , scale 权重 0.25 , 全文检索 权重 0.25  
        (0.5 * 1.0/(60 + COALESCE(v.vector_rank, 100)) +  
         0.25 * 1.0/(60 + COALESCE(t.scale_rank, 200)) +  
         0.25 * 1.0/(60 + COALESCE(ts.tsvec_rank, 200))) as rrf_score  
    FROM vector_results v  
    FULL OUTER JOIN scale_results t USING(id)  
    FULL OUTER JOIN tsvec_results ts USING(id)  
)  
SELECT p.id, p.category, p.price,  -- 这里还可以添加要返回的 vector_test 表的其他字段  
       r.rrf_score, r.vector_rank, r.scale_rank, r.tsvec_rank  
FROM rrf_scores r  
JOIN vector_test p ON r.id = p.id  
-- 按融合后的分值排序, 分越大越相关. 取出前 10 条.  
ORDER BY r.rrf_score DESC  
LIMIT 10;  
```  
  
4、观察单条 RRF 性能  
  
```sql  
\timing  
  
-- 融合查询  
WITH  
-- 向量查询, 提取相关的前20条  
vector_results AS (  
    SELECT id,  
           -- 窗口函数 row_number() 的作用是取出结果顺序号.  
           ROW_NUMBER() OVER (ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)) as vector_rank  
    FROM vector_test  
    ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)  
    LIMIT 20  
),  
-- 标量查询, 提取符合标量查询条件的前20条  
scale_results AS (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY price) as scale_rank  
    FROM vector_test  
    WHERE category = 5 AND price < 100  
    ORDER BY price  
    LIMIT 20  
),  
-- 全文检索, 提取符合全文检索条件的前20条  
-- 备注: ts_rank_cd 是一种全文检索相关性统计算法, 计算查询条件和矢量文本之间的相关性, 值越大越相关  
tsvec_results as (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC) as tsvec_rank  
    FROM vector_test  
    WHERE tsvec @@ to_tsquery('1264451 | 138541')  
    ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC  
    LIMIT 20  
),  
-- 最终使用RRF进行重排  
-- 向量权重: 0.5, 标量权重: 0.25, 全文检索权重: 0.25  
rrf_scores AS (  
    SELECT  
        COALESCE(v.id, t.id, ts.id) as id,  
        -- 100 为惩罚值, 表示仅在某一种查询中存在的行. 例如只在向量查询中有返回, 但是其他查询中无返回.  
        -- 惩罚值必须大于最终返回的LIMIT 条数, 这里是10, 所以惩罚值必须大于10  
        -- 惩罚值也可以设置为不一样. 例如你希望向量查询的惩罚值低一点, 那就设小一点, 让它优先于标量和全文检索查询结果  
        COALESCE(v.vector_rank, 100) as vector_rank,  
        COALESCE(t.scale_rank, 200) as scale_rank,  
        COALESCE(ts.tsvec_rank, 200) as tsvec_rank,  
        -- RRF算法：k=60 , 向量权重 0.5 , scale 权重 0.25 , 全文检索 权重 0.25  
        (0.5 * 1.0/(60 + COALESCE(v.vector_rank, 100)) +  
         0.25 * 1.0/(60 + COALESCE(t.scale_rank, 200)) +  
         0.25 * 1.0/(60 + COALESCE(ts.tsvec_rank, 200))) as rrf_score  
    FROM vector_results v  
    FULL OUTER JOIN scale_results t USING(id)  
    FULL OUTER JOIN tsvec_results ts USING(id)  
)  
SELECT p.id, p.category, p.price,  -- 这里还可以添加要返回的 vector_test 表的其他字段  
       r.rrf_score, r.vector_rank, r.scale_rank, r.tsvec_rank  
FROM rrf_scores r  
JOIN vector_test p ON r.id = p.id  
-- 按融合后的分值排序, 分越大越相关. 取出前 10 条.  
ORDER BY r.rrf_score DESC  
LIMIT 10;  
```  
  
5、压力测试 RRF 性能  
  
```sql  
\! echo "  
WITH  
-- 向量查询, 提取相关的前20条  
vector_results AS (  
    SELECT id,  
           -- 窗口函数 row_number() 的作用是取出结果顺序号.  
           ROW_NUMBER() OVER (ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)) as vector_rank  
    FROM vector_test  
    ORDER BY embedding <-> '[0.5668827,-0.2442255,1,0.7500094,-0.31195357,0.030093279,-0.12537633,0.68836486,-0.018421616,-0.37933782,0.9605927,-0.3399773,-0.8192224,-0.6839713,-0.6513173,0.42495644,-0.3043482,-0.020086138,0.6450944,1,0.50533813,0.61879873,0.33264527,0.1594014,1,0.5975187,0.79589015,-0.88029814,-0.65116,-0.38115847,-0.64158386,-0.0052799657]'::vector(32)  
    LIMIT 20  
),  
-- 标量查询, 提取符合标量查询条件的前20条  
scale_results AS (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY price) as scale_rank  
    FROM vector_test  
    WHERE category = 5 AND price < 100  
    ORDER BY price  
    LIMIT 20  
),  
-- 全文检索, 提取符合全文检索条件的前20条  
-- 备注: ts_rank_cd 是一种全文检索相关性统计算法, 计算查询条件和矢量文本之间的相关性, 值越大越相关  
tsvec_results as (  
    SELECT id,  
           ROW_NUMBER() OVER (ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC) as tsvec_rank  
    FROM vector_test  
    WHERE tsvec @@ to_tsquery('1264451 | 138541')  
    ORDER BY ts_rank_cd(tsvec, to_tsquery('1264451 | 138541')) DESC  
    LIMIT 20  
),  
-- 最终使用RRF进行重排  
-- 向量权重: 0.5, 标量权重: 0.25, 全文检索权重: 0.25  
rrf_scores AS (  
    SELECT  
        COALESCE(v.id, t.id, ts.id) as id,  
        -- 100 为惩罚值, 表示仅在某一种查询中存在的行. 例如只在向量查询中有返回, 但是其他查询中无返回.  
        -- 惩罚值必须大于最终返回的LIMIT 条数, 这里是10, 所以惩罚值必须大于10  
        -- 惩罚值也可以设置为不一样. 例如你希望向量查询的惩罚值低一点, 那就设小一点, 让它优先于标量和全文检索查询结果  
        COALESCE(v.vector_rank, 100) as vector_rank,  
        COALESCE(t.scale_rank, 200) as scale_rank,  
        COALESCE(ts.tsvec_rank, 200) as tsvec_rank,  
        -- RRF算法：k=60 , 向量权重 0.5 , scale 权重 0.25 , 全文检索 权重 0.25  
        (0.5 * 1.0/(60 + COALESCE(v.vector_rank, 100)) +  
         0.25 * 1.0/(60 + COALESCE(t.scale_rank, 200)) +  
         0.25 * 1.0/(60 + COALESCE(ts.tsvec_rank, 200))) as rrf_score  
    FROM vector_results v  
    FULL OUTER JOIN scale_results t USING(id)  
    FULL OUTER JOIN tsvec_results ts USING(id)  
)  
SELECT p.id, p.category, p.price,  -- 这里还可以添加要返回的 vector_test 表的其他字段  
       r.rrf_score, r.vector_rank, r.scale_rank, r.tsvec_rank  
FROM rrf_scores r  
JOIN vector_test p ON r.id = p.id  
-- 按融合后的分值排序, 分越大越相关. 取出前 10 条.  
ORDER BY r.rrf_score DESC  
LIMIT 10;  
" > ~/5.sql  
```  
  
压测  
```sql  
\! pgbench -M prepared -n -r -P 1 -f ~/5.sql -c 10 -j 10 -T 120  
```  
  
6、回顾混合搜索的几种技术  
  
参考 [《混合搜索的三大技术流派, 差异在哪? 哪个更有前途?》](../202510/20251027_09.md)  
  
## PGQ 实验  
使用 duckdb 数据库完成 PGQ 实验.  
  
进入容器  
```bash  
docker exec -ti pg bash  
```  
  
进入 duckdb 数据库 shell  
```bash  
duckdb  
```  
  
下面的实验要从网络上下载并绑定测试数据集. 如果无法完成下载, 请先从外部能下载的机器进行下载, 然后拷贝到 pg 容器在宿主机上的卷目录中, 进入容器后修改文件权限例如 `chmod 555 snb.duckdb` . 并将 ATTACH 命令改成本地路径, 例如 `ATTACH '/var/lib/postgresql/snb.duckdb';`  
  
  
1、掌握PGQ语法, 参考: [《用DuckDB将PGQ语法一次整明白》](../202507/20250727_01.md)  
  
2、社交网络  
  
```sql  
-- 加载图插件  
load duckpgq;  
  
-- 绑定 snb 数据库  
ATTACH 'https://github.com/Dtenwolde/duckpgq-docs/raw/refs/heads/main/datasets/snb.duckdb';  
use snb;  
  
-- 这个测试数据集有哪些表?  
show tables;  
  
-- 查看某个表的结构  
desc Person;  
  
-- 创建属性图  
CREATE or replace PROPERTY GRAPH snb  
VERTEX TABLES (  
  Person, Forum  
)  
EDGE TABLES (  
  Person_knows_person     SOURCE KEY (Person1Id) REFERENCES Person (id)  
                          DESTINATION KEY (Person2Id) REFERENCES Person (id)  
                          LABEL knows,  
  Forum_hasMember_Person  SOURCE KEY (ForumId) REFERENCES Forum (id)  
                          DESTINATION KEY (PersonId) REFERENCES Person (id)  
                          LABEL hasMember  
);  
  
-- find the shortest path from one person to all other persons  
-- 找出 id=14 的人直接或间接认识的人  
FROM GRAPH_TABLE (snb  
  MATCH p = ANY SHORTEST (p1:person WHERE p1.id = 14)-[k:knows]->+(p2:person)  
  COLUMNS (p1.id, p2.id as other_person_id, element_id(p), path_length(p) as len)  
)  
order by len;  
  
-- Find mutual friends between two users  
-- 找出 id=16 和 id=32 这两个人的共同好友  
FROM GRAPH_TABLE (snb  
  MATCH (p1:Person WHERE p1.id = 16)-[k:knows]->(p2:Person)<-[k2:knows]-(p3:Person WHERE p3.id = 32)  
  COLUMNS (p2.firstName, p2.lastName, p2.locationIP)  
);  
  
-- Find the 3 most popular people  
-- 找出3个大网红, 被很多人认识的人. 业界领袖.  
FROM GRAPH_TABLE (snb  
  MATCH (follower:Person)-[follows:knows]->(person:Person)  
  COLUMNS (person.id AS personID, person.firstname, person.lastname, follower.id AS followerID)  
)  
SELECT personID, firstname, lastname, COUNT(followerID) AS numFollowers  
GROUP BY ALL  
ORDER BY numFollowers DESC  
LIMIT 3;  
```  
  
3、航空数据  
  
```sql  
-- 加载图插件  
load duckpgq;  
  
-- 绑定 airline 数据库  
ATTACH 'https://github.com/Dtenwolde/duckpgq-docs/raw/refs/heads/airline-data/datasets/airline-data-small.duckdb' as airline;  
use airline;  
  
-- 这个测试数据集有哪些表?  
show tables;  
  
-- 查看某个表的结构  
desc flights;  
  
-- 创建属性图  
CREATE PROPERTY GRAPH flight_graph  
  VERTEX TABLES (  
    aircrafts_data, airports_data,  
    bookings, flights,  
    tickets, seats  
  )  
  EDGE TABLES (  
    route  
      SOURCE KEY (departure_airport) REFERENCES airports_data(airport_code)  
      DESTINATION KEY (arrival_airport) REFERENCES airports_data(airport_code),  
    ticket_flights  
      SOURCE KEY (ticket_no) REFERENCES tickets(ticket_no)  
      DESTINATION KEY (flight_id) REFERENCES flights(flight_id),  
    bookings_tickets  
      SOURCE KEY (book_ref) REFERENCES bookings(book_ref)  
      DESTINATION KEY (ticket_no) REFERENCES tickets(ticket_no),  
    boarding_passes  
      SOURCE KEY (ticket_no) REFERENCES tickets(ticket_no)  
      DESTINATION KEY (seat_no) REFERENCES seats(seat_no)  
);  
  
-- Shortest Route Between Airports  
-- 查询UKX 和 CNN两个机场之间的最短路径  
FROM (  
  SELECT unnest(flights) AS flights  
    FROM GRAPH_TABLE (  
    flight_graph  
    MATCH o = ANY SHORTEST (a:airports_data WHERE a.airport_code = 'UKX')  
      -[fr:route]->*  
      (a2:airports_data WHERE a2.airport_code = 'CNN')  
    COLUMNS (edges(o) AS flights)  
  )  
)  
JOIN route f  
  ON f.rowid = flights;  
  
-- Most Expensive Seats on Average  
-- 查询平均票价最贵的座椅  
-- 观察结果是否符合逻辑? 头等舱是不是最贵的?  
FROM GRAPH_TABLE (  
  flight_graph  
  MATCH (b:bookings)-[bt:bookings_tickets]->(t:tickets)-[bp:boarding_passes]->(s:seats)  
)  
SELECT round(avg(total_amount), 2) avg_amount, seat_no  
GROUP BY seat_no  
ORDER BY avg_amount DESC;  
```  
  
4、揭示金融犯罪  
  
```sql  
-- 加载图插件  
load duckpgq;  
  
-- 绑定 finbench 数据库  
ATTACH 'https://github.com/Dtenwolde/duckpgq-docs/raw/refs/heads/main/datasets/finbench.duckdb' AS finbench;  
use finbench;  
  
-- 这个测试数据集有哪些表?  
show tables;  
  
-- 查看某个表的结构  
desc Account;  
desc Person;  
desc Company;  
  
-- 创建属性图  
CREATE OR REPLACE PROPERTY GRAPH finbench  
VERTEX TABLES (  
  Account, Company, Loan,  
  Medium, Person  
)  
EDGE TABLES (  
  AccountRepayLoan        SOURCE KEY (accountId) REFERENCES Account (accountId)  
                          DESTINATION KEY (loanId) REFERENCES Loan (loanId)  
                          LABEL repay,  
  AccountTransferAccount  SOURCE KEY (fromId) REFERENCES Account (accountId)  
                          DESTINATION KEY (toId) REFERENCES Account (AccountId)  
                          LABEL transfer,  
  AccountWithdrawAccount  SOURCE KEY (fromId) REFERENCES Account (accountId)  
                          DESTINATION KEY (toId) REFERENCES Account (AccountId)  
                          LABEL withdraw,  
  CompanyApplyLoan        SOURCE KEY (companyId) REFERENCES Company (companyId)  
                          DESTINATION KEY (loanId) REFERENCES Loan (loanId)  
                          LABEL companyApply,  
  CompanyGuaranteeCompany SOURCE KEY (fromId) REFERENCES Company (companyId)  
                          DESTINATION KEY (toId) REFERENCES Company (companyId)  
                          LABEL companyGuarantee,  
  CompanyInvestCompany    SOURCE KEY (investorId) REFERENCES Company (companyId)  
                          DESTINATION KEY (companyId) REFERENCES Company (companyId)  
                          LABEL companyInvest,  
  CompanyOwnAccount       SOURCE KEY (companyId) REFERENCES Company (companyId)  
                          DESTINATION KEY (accountId) REFERENCES Account (accountId)  
                          LABEL companyOwn,  
  LoanDepositAccount      SOURCE KEY (loanId) REFERENCES Loan (loanId)  
                          DESTINATION KEY (accountId) REFERENCES Account (accountId)  
                          LABEL deposit,  
  MediumSignInAccount     SOURCE KEY (mediumId) REFERENCES Medium (mediumId)  
                          DESTINATION KEY (accountId) REFERENCES Account (accountId)  
                          LABEL signIn,  
  PersonApplyLoan         SOURCE KEY (personId) REFERENCES Person (personId)  
                          DESTINATION KEY (loanId) REFERENCES Loan (loanId)  
                          LABEL personApply,  
  PersonGuaranteePerson   SOURCE KEY (fromId) REFERENCES Person (personId)  
                          DESTINATION KEY (toId) REFERENCES Person (personId)  
                          LABEL personGuarantee,  
  PersonInvestCompany     SOURCE KEY (investorId) REFERENCES Person (personId)  
                          DESTINATION KEY (companyId) REFERENCES Company (companyId)  
                          LABEL personInvest,  
  PersonOwnAccount        SOURCE KEY (personId) REFERENCES Person (personId)  
                          DESTINATION KEY (accountId) REFERENCES Account (accountId)  
                          LABEL personOwn  
);  
  
-- Find blocked accounts via transfers  
-- 查看 16607023625929101 账户在往哪些已冻结账户转账  
FROM GRAPH_TABLE (  
  finbench  
  MATCH (src:Account where src.accountId = 16607023625929101)  
    <-[e1:transfer]-(mid:Account)  
    -[e2:transfer]->(dst:Account where dst.isBlocked = true)  
  COLUMNS (src.accountId as src_id, dst.accountId as dst_id)  
)  
SELECT src_id, dst_id  
WHERE src_Id <> dst_id;  
  
-- Filter high-value transfers by time  
-- 查询在某个时间段内, 大于某个额度的转账  
FROM GRAPH_TABLE (  
  finbench  
  MATCH (src:Account)-[e:Transfer]->(dst:Account)  
  WHERE '2022-07-13 09:18:33.137' < e.createtime  
    AND e.createtime < '2022-09-03 02:31:47.812'  
    AND e.amount > 4829783  
  );  
```  
  
  
## 课程竞赛  
  
参考  
  
[《大学生数据库实践课 : 课后大作业》](../202512/20251217_02.md)  竞赛章节  
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")  
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")  
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")  
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")  
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")  
