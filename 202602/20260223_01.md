## 德说-第412期, 重创英伟达, 新“卖铲人”直接把模型刻进芯片了  
                              
### 作者                                
digoal                                
                             
### 日期                                
2026-02-23                           
                                
### 标签                                
AI , 芯片 , 通用 , 专用                       
                                
----                                
                                
## 背景    
  
碾压英伟达50倍？估值3000万的初创公司，正在撕裂AI芯片的“第一性原理”  
  
作为在科技赛道摸爬滚打多年的投资人，我每天都会看到无数号称“颠覆英伟达”的BP（商业计划书）。但绝大多数只是在GPU的生态护城河外蹭蹭痒。然而，今天进入视野的初创公司 **Taalas** 和它的 HC1 芯片，却让我闻到了一丝真正危险且迷人的血腥味。  
  
这不仅是因为它宣称性能是Nvidia Blackwell架构的近50倍，更是因为它**从第一性原理出发，对当前AI算力底层逻辑发起了一场极端的“自杀式冲锋”。**  
  
  
  
## 一、 算力的第一性原理：对抗“冯·诺依曼瓶颈”  
  
要理解Taalas的疯狂，我们必须回归芯片设计的第一性原理： **计算的本质，是数据的搬运与处理。**  
  
当前所有主流GPU（包括英伟达），都受制于传统的“冯·诺依曼架构”：计算单元与存储单元分离。当运行大模型时，GPU把大部分的时间和巨大能耗（高达数十千瓦，甚至需要液冷），都浪费在了把模型参数从HBM（高带宽内存）搬运到计算核心的路上。这被称为 **“内存墙（Memory Wall）”** 。英伟达的护城河，很大程度上建立在其无与伦比的HBM互联带宽和NVLink技术上。  
  
Taalas的解法极其暴力： **不搬了。**  
  
通过Mask ROM工艺，Taalas直接把Meta的Llama 3.1 8B模型权重“刻死”在芯片的金属互连层中。计算逻辑和权重数据在物理层面上合二为一（存算一体）。  
  
* **结果惊人：** 单用户17,000 tokens/s输出，功耗仅250W（标准风冷），构建成本是同等GPU的1/20。  
* **代价惨烈：** 灵活性归零。这块芯片只能跑Llama 3.1 8B，想换模型？对不起，重新流片。  
  
这就像英伟达造的是一把无所不能的“瑞士军刀”，而Taalas造的是一把只能切特定尺寸牛排的“极其锋利、极其便宜的定制手术刀”。  
  
  
  
## 二、 投资逻辑的生死线：前提条件假设  
  
作为分析师，我们不能被纸面参数忽悠。Taalas的商业模式能否跑通，**完全建立在一个极其严苛的“第一性原理前提”之上：AI模型将迅速走向商品化与固化（Stabilization）。**  
  
**权威历史案例支撑：加密货币挖矿的算力演进史。**  
回看比特币挖矿的历史，算力经历了 `CPU -> GPU -> FPGA -> ASIC (专用集成电路)` 的必然演进。当算法（SHA-256）彻底固定后，毫无灵活性的ASIC矿机凭借百倍的能效比，将GPU彻底赶出了挖矿市场。  
  
**Taalas押注的前提是：AI推理市场也将迎来“ASIC时刻”。**  
他们赌的是，在未来海量的AI落地场景中（如客服、基础文案生成、固定逻辑的NPC），企业不需要每天更新模型。一个Llama 3.1 8B级别的开源模型，足以在长达1-2年的时间里满足80%的业务需求。在这个前提下，客户为了极低的推理成本（7.6美分/百万token）和功耗，愿意牺牲灵活性，并给出一年的商业承诺。  
  
  
## 三、 逻辑推演：如果前提条件崩塌？  
  
但任何严谨的投资分析，都必须做压力测试。**如果上述假设崩塌，市场会走向何方？**  
  
如果AI行业的演进速度在未来三年内依然保持“狂飙突进”，新架构（如MoE、非Transformer架构的Mamba/RWKV）层出不穷，模型迭代周期以“周”计算；或者未来的AI推理高度依赖“持续学习（Continuous Learning）”和动态权重更新——**那么Taalas的极端路线将是一场灾难。**  
  
一旦前提崩塌，我们将得出以下反向结论与趋势预测：  
  
1. **“刻舟求剑”的硬件会变成电子垃圾：** 两个月的芯片定制周期，在模型日新月异的时代依然太慢。芯片刚造出来，模型就已经过时了。  
2. **量化精度的致命伤将被放大：** Taalas目前为了极致性能采用了自定义的3-bit激进量化。在模型能力未完全溢出的当下，3-bit带来的精度损失在复杂逻辑推理场景中往往是不可接受的（这也是为什么他们急于在HC2推出4-bit）。如果高精度是刚需，这种硬连线的架构优势会大打折扣。  
3. **灵活性依然是王道（GPU与FPGA的长期霸权）：** 如果算法不收敛，英伟达的通用CUDA生态和极高的带宽冗余，依然是所有云厂商唯一安全的避风港。  
  
  
  
## 四、 商业洞察：赢家通吃的终局，还是长尾市场的狂欢？  
  
目前Taalas副总裁Paresh Kharya提出了API、卖芯片、定制合作三种模式。但在我看来，**其最合理的商业归宿，是成为“云厂商底层基建的定制代工厂”。**  
  
当DeepSeek R1（671B）这样颠覆性的开源模型证明了其长期的使用价值后，AWS、微软Azure或者阿里云，完全有动力采购类似Taalas的方案，用30颗芯片组成的集群来承载那些最高频、最标准化的API调用，从而将极其昂贵的GPU算力释放出来，去投入到下一代模型的Training（训练）和高复杂度Inference（推理）中。  
  
**总结：**  
Taalas不是在造一颗芯片，而是在做一次豪赌。他们赌的是AI产业的成熟度已经到了可以“固化基建”的临界点。不管他们最终是成为下一个时代的霸主，还是成为先烈，这种**用零灵活性换取极致性能**的设计思路，都已经为疯狂的AI算力竞赛，提供了一个极具参考价值的降本终局答案。  
  
个人认为这条路一定会走通, 未来将利好具身智能行业(机器人、大黄蜂等), 而公有云如果还单纯仅靠卖tokens(API调用)的话利润可能会受较大影响, 未来一定是能解决复杂问题的AI产品的天下.    
  
## 参考
https://taalas.com/  
    
---  
  
Prompt:  
````  
你是资深投资人和行业分析师, 阅读以下新闻, 用爆款文章的风格写一篇文章, 要观点犀利, 逻辑清晰, 有理有据，有权威数据支撑，有权威案例支撑，不能用个例以偏概全，要有符合第一性原理的前提条件假设来支撑你的观点，如果条件崩塌，引出其他观点。  
```  
初创公司Taalas押注极端专用化：3000万美元造AI芯片 性能碾压GPU   
2026-02-22 04:16 发布于：山东省  
在AI芯片领域，一场颠覆性的变革正在悄然发生。一家名为Taalas的初创公司，以一种近乎“激进”的方式，将专用化芯片设计推向了新的高度。该公司推出的首款推理芯片HC1，将meta的Llama 3.1 8B大语言模型几乎完整地“刻入”了硅片，实现了单用户场景下高达17,000 tokens/s的输出速度，这一性能是当前市场上最快竞品Cerebras的近9倍，更是Nvidia Blackwell架构GPU的近50倍。与此同时，HC1的构建成本仅为同等GPU方案的二十分之一，功耗更是低了一个数量级。  
  
HC1的突破性设计，源于其对传统GPU架构的彻底颠覆。在GPU中，计算单元与存储单元是分离的，模型参数存储在HBM中，计算核心每次运算都需要从HBM搬运数据，这一过程不仅消耗大量能量，还增加了时间成本。而Taalas则采用了全面专用化、存算合一的设计思路，通过Mask ROM工艺将模型权重直接编码在芯片的金属互连层中，与计算逻辑共存于同一块硅片上，从而彻底消除了数据搬运的瓶颈。  
  
这种设计虽然带来了极高的性能提升，但也意味着芯片的灵活性几乎为零。HC1只能运行Llama 3.1 8B模型，若要更换模型，则需重新设计并制造芯片。这种极端专用化的策略，无疑是对AI芯片行业传统设计思路的一次大胆挑战。然而，Taalas的CEO Ljubisa Bajic却对此充满信心。他认为，随着AI模型的成熟和稳定，总有一些模型会在实际业务中被长期使用，对于这些模型，专用化芯片将具有无可比拟的优势。  
  
Bajic的信心并非空穴来风。HC1基于台积电N6工艺制造，芯片面积815 mm²，单颗芯片即可容纳完整的8B参数模型。其功耗约250W，10块HC1板卡装进一台服务器总功耗约2.5 kW，可在标准风冷机架中运行，这与动辄数十千瓦、必须依赖液冷的GPU服务器形成了鲜明对比。Taalas还借鉴了结构化ASIC的设计思路，通过固化门阵列和硬化IP模块，只修改互连层来适配不同模型，从而大大缩短了芯片定制周期。据Bajic透露，从拿到一个新模型到生成RTL，大约只需要一周的工程工作量，整个从模型到芯片的周期目标为两个月。  
  
这种快速周转的能力，使得Taalas能够在模型被验证有效且用户粘性足够高时，迅速为其制造专用硅片，以远低于GPU的成本和功耗提供推理服务。然而，这种模式也要求客户对某个特定模型做出至少一年的承诺。对于这一要求，Bajic认为，虽然会有很多人不愿意，但总会有一些人愿意为了性能和成本的优势而接受。  
  
除了Llama 3.1 8B模型外，Taalas还展示了其对更大模型的支持能力。据模拟数据显示，671B参数的DeepSeek R1模型需要大约30颗HC1芯片协同工作，每颗芯片承载约20B参数。这套30芯片系统在DeepSeek R1上可以达到约12,000 tokens/s/user的输出速度，而当前GPU的最优水平大约在200 tokens/s/user。同时，推理成本约7.6美分/百万token，不到GPU吞吐优化方案的一半。  
  
然而，这些数字目前还停留在模拟阶段。实际多芯片系统面临的互联、同步、良率等工程挑战不容小觑。HC1使用了自定义的3-bit基础数据类型进行激进量化，这可能会带来相对于标准量化模型的质量损失。对此，Taalas并未回避，并表示其第二代硅平台HC2将采用标准4-bit浮点格式以改善这一问题。  
  
在商业模式上，Taalas仍在摸索之中。公司副总裁Paresh Kharya透露了几种可能的方向：自建基础设施运行开源模型并提供API推理服务；直接向客户出售芯片；或者与模型开发者合作，为他们的模型定制专用芯片供其自有推理基础设施使用。哪种模式最终能跑通，将取决于市场对这种极端专用化方案的接受程度。  
  
尽管面临诸多挑战和不确定性，但Taalas的方案无疑触及了一个被主流路线忽略的设计空间。通过将权重以Mask ROM形式与计算逻辑同层集成，Taalas从根本上消除了存算分离带来的带宽墙问题。虽然这种设计以灵活性的彻底丧失为代价，但在允许这种刚性的应用场景中，其换来的性能和成本优势却是实打实的。硬接线芯片还带来了软件栈的极度简化，进一步降低了系统的复杂性和成本  
```  
````  
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
