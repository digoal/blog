<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">Oracle I/O and Operation System Cache</h2>
	<h5 id="">2010-11-13 14:12:05&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201010132125889/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;">[转]<br><h4>Direct or Async I/O</h4>It’s pretty well documented that the most efficient configuration for Oracle is direct asynchronous I/O. <p><em>Direct I/O</em> means to avoid unnecessarily copying bits from  one memory location to another between the hardware and the SGA.  Every  flavor of Unix implements some form of caching for block devices and  filesystems; in Linux the <a rel="nofollow" href="http://tldp.org/LDP/tlk/fs/filesystem.html#tth_sEc9.2"  >buffer cache</a> (for block devices) or <a rel="nofollow" href="http://www.linux-security.cn/ebooks/ulk3-html/0596005652/understandlk-CHP-15-SECT-2.html#understandlk-CHP-15-FIG-2"  >page cache</a>  (for filesystem data) can cache database blocks.  Most applications  benefit from this caching C however Oracle does not.  There are two  primary reasons this caching is bad for Oracle:</p> <ol><li> <p>Oracle caches this data itself in the SGA and it does a far better  job of predicting what blocks will be best retained in memory.  When the  OS caches data you essentially end up with two copies of every data  block C one that’s really unused.  You waste half the memory in your  server with almost no performance benefit.</p> </li><li> <p>The data has to be copied extra times before arriving in the SGA C  and these extra “copy” operations slow down your read operations.</p> </li></ol> <p><em>Asynchronous I/O</em> means to multitask your read and write  operations.  Remember how in MSDOS you could only run one program at a  time?  Believe it or not, this is how most Unix filesystems do I/O by  default C one thing at a time.  Using Asynchronous is equivalent to  upgrading to an Operating System that allows you to run more than one  program at a time.  It allows Oracle to issue multiple read and write  operations concurrently C this is obviously more efficient.</p> <p>There are different ways of implementing Direct I/O and Asynchronous  I/O in different environments (CIO, QIO, ODM, etc) C but it’s always  best to enable them and then shift memory from the OS caches to the SGA.</p> <p><br></p> <h4>The Linux I/O Path</h4> <p>I should start out by saying that I’m not a kernel expert.  After  some digging through source code and mailing lists I think that I’ve got  a handle on the main concepts here C but I’m open to correction since  this is complicated stuff and I easily could miss some details of how  it’s all implemented!  Let me know if you have anything to add!</p> <p>On Linux, asynchronous I/O requests are submitted through the <a rel="nofollow" href="http://www.linuxmanpages.com/man2/io_submit.2.php"  >io_submit()</a> call.  (Kevin Closson once <a rel="nofollow" href="http://kevinclosson.wordpress.com/2006/12/05/analyzing-oracle-database-10g-writer-io-activity-on-linux/"  >wrote about tracing this call</a>.)  This function essentially puts the I/O request into a queue which is managed by the Linux kernel.  I think that this <em>workqueue</em>  is a generic kernel object which you can’t and shouldn’t need to tweak.   The I/O request is subsequently picked up by one of several kernel AIO  background processes and serviced.</p> <p>What the kernel thread does depends on the underlying device.  FC  cards support asynchronous requests to the target (it’s part of the SCSI  spec) C so in this case the kernel thread will issue a number of SCSI  read or write requests in parallel.  This is where a <em>second</em> queue comes in!  And this queue, unlike the kernel workqueues, can be tweaked.</p> <p>The target device (Symetrix, DSxxxx, 3Par, etc) also has an FC port  which has its own queue for I/O requests that are being serviced.  On a  3par device these ports only have between 500 and 2000 slots for  concurrent I/O requests depending on the model.  This means that all the  servers accessing any LUNs over that port cannot issue more than  500-2000 concurrent I/O requests C or you will start to see “QUEUE FULL  SCSI” errors.</p> <p>My best guess: this means that the default limit set by the Linux  qla2xxx driver for concurrent I/O requests on QLogic cards (32 per LUN)  is conservative.  The driver sets this limit by <em>limiting the size of the queue</em>.  So naturally…  my next question was if I could increase performance by increasing this limit…</p> <h4>Tweaking the HBA Queue Depth</h4> <p>I did run a few tests C but I think I’ll save the results for a  second post since this one has already gotten pretty long.  Any guesses  about what the results were?</p><div><img title="Oracle I/O and Operation System Cache - 德哥@Digoal - The Heart,The World."  alt="Oracle I/O and Operation System Cache - 德哥@Digoal - The Heart,The World."  style="margin: 0pt 10px 0pt 0pt;"  src="http://img623.ph.126.net/EK9FRwn1eeV-nNuJ1hzYpQ==/1682657410778357540.gif"  ></div></div>
	</div>
</div>
</body>
</html>