<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">multicast for distribyted storage object data delivering like ceph replication</h2>
	<h5 id="">2014-12-24 17:46:19&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/1638770402014112451349619/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div><div>在看ceph文档时看到如下一段</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >PEERING</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >When Ceph is Peering a placement group, Ceph is bringing the OSDs that store the replicas of the placement group into agreement about the state of the objects and metadata in the placement group. When Ceph completes peering, this means that the OSDs that store the placement group agree about the current state of the placement group. However, completion of the peering process does NOT mean that each replica has the latest contents.</font></div></div><div><font size="2"   ><br></font></div><div><font size="2"   >Authoratative History</font></div><div><font size="2"   >Ceph will NOT acknowledge a write operation to a client, until all OSDs of the acting set persist the write operation. This practice ensures that at least one member of the acting set will have a record of every acknowledged write operation since the last successful peering operation.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >With an accurate record of each acknowledged write operation, Ceph can construct and disseminate a new authoritative history of the placement group C a complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.</font></div><p></p></pre></div></div><div>这段话主要表明用户在往ceph cluster写一份数据时, ceph集群需要确保这份数据已经写入它所在的Placement Group所在的所有acting OSDs中, ceph才返回ACK告诉客户端写入完成.</div><div><br></div><div>那么问题来了, CEPH是怎么处理多份写入的呢, 例如pg放在3份拷贝的POOL, 那么一份数据会放到3个OSD中.</div><div>可能是这样(客户端写一个OSD, 再由ceph通过TCP复制到其他OSD)</div><div>Client -&gt;(write to) OSDa</div><div>OSDa -&gt;(replicate to through tcp) OSDb</div><div><span style="line-height: 28px;"   >OSDa -&gt;(replicate to</span><span style="line-height: 28px;"   >&nbsp;</span><span style="line-height: 28px;"   >through tcp</span><span style="line-height: 28px;"   >) OSDc</span></div><div><span style="line-height: 28px;"   >这种方法的弊端是, 数据需要从OSDa再下发一次到b,c. 并且OSD之间需要3次握手.</span></div><div><span style="line-height: 28px;"   ><br></span></div><div>也可能是这样<span style="line-height: 28px;"   >(客户端写一个OSD, 再由ceph通过UDP(多播)复制到其他OSD)</span></div><div><div style="line-height: 28px;"   >Client -&gt;(write to) OSDa</div><div style="line-height: 28px;"   >OSDa -&gt;(replicate to through UDP mcast) OSDb, OSDc</div></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >这种方法的弊端是, 数据需要从OSDa再下发一次到b,c.&nbsp;</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >相比第一种方法OSD之间不需要3次握手.</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >但是需要CEPH自己处理UDP丢包问题.</span></div><div style="line-height: 28px;"   ><br></div><div>也可能是这样<span style="line-height: 28px;"   >(客户端通过多播一次写多个OSD)</span></div><div><div style="line-height: 28px;"   >Client -&gt;(write to through UDP mcast) OSDa, OSDb, OSDc</div></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >这种方法的好处是数据包只需要在网络上传输一次.</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >弊端</span><span style="line-height: 28px;"   >是需要CEPH自己处理UDP丢包问题, 并且多播组需要动态变化, 例如pg的数据分布发生变化, 不同的PG属于不同的OSD集合, 等等, 使这种方法不切合实际.</span></div><div style="line-height: 28px;"   ><br></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   >也可能是这样<span style="line-height: 28px;"   >(客户端通过单播一次写多个OSD)</span></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   >Client -&gt;(write to through UDP unicast) OSDa, OSDb, OSDc</div><div style="line-height: 28px;"   >这种方法比较切合实际, 同样<span style="line-height: 28px;"   >需要CEPH自己处理UDP丢包问题.</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >但是避免了多播老变化的问题.</span></div><div style="line-height: 28px;"   ><br></div></div></div><div>效率最高的应该是最后一种, 只是软件设计会更加复杂. (例如OSD退出, backfill, pg分布的OSD可能发生变化, 等)</div><div><br></div><div>目前CEPH好像不支持UDP, 所以应该第一种的可能性较大, 暂时先了解到这里.</div><div>当然还可以使用SDN来提升网络传输体验.</div><div><br></div><div>使用tcpdump在一台OSD抓包, eth1是cluster network</div><div>[root@osd2 ~]# tcpdump tcp -i eth1</div><div>可以看到在往rbd写数据时, 有大量的包在cluster network传输.</div><div><div style="line-height: 28px;"   >[root@osd2 ~]# cat /etc/ceph/ceph.conf&nbsp;</div><div style="line-height: 28px;"   >[global]</div><div style="line-height: 28px;"   >fsid = f649b128-963c-4802-ae17-5a76f36c4c76</div><div style="line-height: 28px;"   >mon initial members = mon1, mon2, mon3</div><div style="line-height: 28px;"   >mon host = 172.17.0.2, 172.17.0.3, 172.17.0.4</div><div style="line-height: 28px;"   >public network = 172.17.0.0/16</div><div style="line-height: 28px;"   >cluster network = 172.18.0.0/16, 172.19.0.0/16</div><div style="line-height: 28px;"   >auth cluster required = cephx</div><div style="line-height: 28px;"   >auth service required = cephx</div><div style="line-height: 28px;"   >auth client required = cephx</div><div style="line-height: 28px;"   >osd journal size = 1024</div><div style="line-height: 28px;"   >filestore xattr use omap = true</div><div style="line-height: 28px;"   >osd pool default size = 2</div><div style="line-height: 28px;"   >osd pool default min size = 1</div><div style="line-height: 28px;"   >osd pool default pg num = 333</div><div style="line-height: 28px;"   >osd pool default pgp num = 333</div><div style="line-height: 28px;"   >osd crush chooseleaf type = 1</div><div style="line-height: 28px;"   >ms_bind_ipv6 = false</div></div><div><br></div><div>17:43:35.677205 IP 172.18.0.6.60150 &gt; 172.18.0.7.6800: Flags [P.], seq 14750505:14750776, ack 10006930, win 24576, options [nop,nop,TS val 95339819 ecr 95339763], length 271</div><div><br></div><div><span style="line-height: 28px;"   >[参考]</span></div><wbr><div>1.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://www.google.com/patents/US20140204940"   >http://www.google.com/patents/US20140204940</a></div><div>2.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="https://github.com/OpenHFT/Chronicle-Map#tcp--udp-replication"   >https://github.com/OpenHFT/Chronicle-Map#tcp--udp-replication</a></div><div>3.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://openhft.net/products/chronicle-map/"   >http://openhft.net/products/chronicle-map/</a></div><div>4.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://www.arcserve.com/ap/for-it-professionals/~/media/files/industryanalystreports/arcserve/ca-arcserve-udp-vs-veeam-backup-and-replication-v7-0.pdf"   >http://www.arcserve.com/ap/for-it-professionals/~/media/files/industryanalystreports/arcserve/ca-arcserve-udp-vs-veeam-backup-and-replication-v7-0.pdf</a></div><div>5.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://ganglia.sourceforge.net/"   >http://ganglia.sourceforge.net/</a></div><div>6.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://ceph.com/docs/master/rados/operations/monitoring-osd-pg/"   >http://ceph.com/docs/master/rados/operations/monitoring-osd-pg/</a></div><div><br></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="multicast for distribyted storage like ceph - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>