<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ceph osd & placement group's stats introduce</h2>
	<h5 id="">2014-12-25 10:02:54&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/1638770402014111545023393/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div><div>本文主要介绍一下osd和PG的状态监控.</div><div><br></div><div>OSD有4种状态 :&nbsp;</div><div><div><img title="ceph osd  placement groups stats introduce - 德哥@Digoal - PostgreSQL research"   alt="ceph osd  placement groups stats introduce - 德哥@Digoal - PostgreSQL research"   style="margin:0 10px 0 0;"   src="http://img0.ph.126.net/HmvFsk-mqXpbLQ_yF6KSNg==/3098476543648541655.png"   ></div>Up 的OSD可能在集群中(In)或不在集群中(Out), Down的OSD则肯定是不在集群中(Out)的.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd4 ~]# ceph osd stat</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e41: 4 osds: 4 up, 4 in</font></div><p></p></pre></div><div>例如以上命令看到的集群的osd状态, 总共有4个OSD, 4个在集群中.</div><div>如果你发现有up和in数字不一致, 说明有Down的或Out的osd, 可以通过如下命令查看详情.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd4 ~]# ceph osd tree</font></div><div><font size="2"   ># id &nbsp; &nbsp;weight &nbsp;type name &nbsp; &nbsp; &nbsp; up/down reweight</font></div><div><font size="2"   >-1 &nbsp; &nbsp; &nbsp;4 &nbsp; &nbsp; &nbsp; root default</font></div><div><font size="2"   >-2 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd1</font></div><div><font size="2"   >0 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.0 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-3 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd2</font></div><div><font size="2"   >1 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.1 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-4 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd3</font></div><div><font size="2"   >2 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.2 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-5 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd4</font></div><div><font size="2"   >3 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.3 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><p></p></pre></div><div>如果发现有Down的OSD, 启动OSD daemon进程即可.</div><div>例如 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >/usr/bin/ceph-osd -i 3 --pid-file /var/run/ceph/osd.3.pid -c /etc/ceph/ceph.conf --cluster ceph&nbsp;[--osd-data=path] [--osd-journal=path]</font></div><div></div><p></p></pre></div><div>使用ceph pg stat时, 或使用ceph -s时, 可以看到有时CEPH集群的健康状态是<span style="line-height: 28px;"   >HEALTH_OK, 有时是</span><span style="line-height: 28px;"   >HEALTH_WARN的.&nbsp;</span></div><div><span style="line-height: 28px;"   >集群健康状态非HEALTH_OK, 表明pg的状态非active+clean.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e8: 5 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0,mon5=172.17.0.10:6789/0}, election epoch 38, quorum 0,1,2,3,4 mon1,mon2,mon3,mon4,mon5</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v17830: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 42261 MB used, 1595 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div><br></div><div>为了减少用户数据在OSD中的metadata数据量, CEPH在用户数据和OSD之间加了一层Placement Group(实际上是POOL, 逻辑划分), 即用户数据存在PG中(打印pg map时你会发现), 而PG则存在POOL中, POOL则包含了一些OSD, 以及一些对象属性(如复制多少份).</div><div>获取PG状态 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph pg stat</font></div><div><font size="2"   >v17784: 128 pgs: 128 active+clean; 0 bytes data, 42223 MB used, 1595 GB / 1636 GB avail</font></div><p></p></pre></div></div><wbr><div><span style="line-height: 28px;"   >可以看到 pg的状态是active+clean, 获取其他状态.</span></div><div><span style="line-height: 28px;"   ><br></span></div><div>如果ceph不是active+clean的状态, 说明ceph集群有问题,&nbsp;</div><div>如果我们发现集群的<span style="line-height: 28px;"   >健康状态非HEALTH_OK, 例如只是警告, 那么可能是什么原因引起的呢?</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >1. You have just created a pool and placement groups haven’t peered yet. (创建了一个池, 但是pg还未peer结束)</font></div><div><font size="2"   >2. The placement groups are recovering. (PG正在恢复)</font></div><div><font size="2"   >3. You have just added an OSD to or removed an OSD from the cluster. (添加或删除OSD, 并且重分布未结束)</font></div><div><font size="2"   >4. You have just modified your CRUSH map and your placement groups are migrating. (修改了CRUSH map, pg正在迁移数据)</font></div><div><font size="2"   >5. There is inconsistent data in different replicas of a placement group. (PG在不同的副本中存在不同的数据)</font></div><div><font size="2"   >6. Ceph is scrubbing a placement group’s replicas. (ceph正在修复异常的PG)</font></div><div><font size="2"   >7. Ceph doesn’t have enough storage capacity to complete backfilling operations. (ceph没有足够的空间完成backfill操作)</font></div><p></p></pre></div><div><br></div><div>接下来我们看看如何查看PG的状态.</div><div>如何来查看每个PG的状态呢? 要打印最详细的PG状态, 可以使用如下命令 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >ceph pg dump</font></div><div><font size="2"   >&nbsp; &nbsp; You can also format the output in JSON format and save it to a file:</font></div><div><font size="2"   >ceph pg dump -o {filename} --format=json</font></div><p></p></pre></div><div>将输出每个pg的状态, 信息量很大, 例如</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@mon1 ~]# ceph pg dump | less</font></div><div><font size="2"   >dumped all in format plain</font></div><div><font size="2"   >version 17880</font></div><div><font size="2"   >stamp 2014-12-15 17:39:01.282848</font></div><div><font size="2"   >last_osdmap_epoch 29</font></div><div><font size="2"   >last_pg_scan 26</font></div><div><font size="2"   >full_ratio 0.95</font></div><div><font size="2"   >nearfull_ratio 0.85</font></div><div><font size="2"   >pg_stat objects mip &nbsp; &nbsp; degr &nbsp; &nbsp;misp &nbsp; &nbsp;unf &nbsp; &nbsp; bytes &nbsp; log &nbsp; &nbsp; disklog state &nbsp; state_stamp &nbsp; &nbsp; v &nbsp; &nbsp; &nbsp; reported &nbsp; &nbsp; &nbsp; &nbsp;up &nbsp; &nbsp; &nbsp;up_primary &nbsp; &nbsp; &nbsp;acting &nbsp;acting_primary &nbsp;last_scrub &nbsp; &nbsp; &nbsp;scrub_stamp &nbsp; &nbsp; last_deep_scrub deep_scrub_stamp</font></div><div><font size="2"   >0.2d &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; active+clean &nbsp; &nbsp;2014-12-15 16:09:51.973547 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 29:35 &nbsp; [1,3] &nbsp; 1 &nbsp; &nbsp; &nbsp; [1,3] &nbsp; 1 &nbsp; &nbsp; &nbsp; 0'0 &nbsp; &nbsp; 2014-12-15 16:09:51.973484 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 2014-12-09 16:05:52.202755</font></div><div><font size="2"   >0.2c &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; active+clean &nbsp; &nbsp;2014-12-15 16:08:50.041015 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 29:44 &nbsp; [2,0] &nbsp; 2 &nbsp; &nbsp; &nbsp; [2,0] &nbsp; 2 &nbsp; &nbsp; &nbsp; 0'0 &nbsp; &nbsp; 2014-12-15 16:08:50.040959 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 2014-12-09 16:05:52.201643</font></div><div><font size="2"   >0.2b &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; active+clean &nbsp; &nbsp;2014-12-15 16:09:48.255609 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 29:54 &nbsp; [3,1] &nbsp; 3 &nbsp; &nbsp; &nbsp; [3,1] &nbsp; 3 &nbsp; &nbsp; &nbsp; 0'0 &nbsp; &nbsp; 2014-12-15 16:09:48.255543 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 2014-12-09 16:05:52.200547</font></div><div><font size="2"   >0.2a &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; active+clean &nbsp; &nbsp;2014-12-15 16:09:42.253658 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 29:54 &nbsp; [3,1] &nbsp; 3 &nbsp; &nbsp; &nbsp; [3,1] &nbsp; 3 &nbsp; &nbsp; &nbsp; 0'0 &nbsp; &nbsp; 2014-12-15 16:09:42.253596 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 2014-12-09 16:05:52.199419</font></div><div><font size="2"   >0.29 &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; active+clean &nbsp; &nbsp;2014-12-15 16:09:41.252917 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 29:61 &nbsp; [3,0] &nbsp; 3 &nbsp; &nbsp; &nbsp; [3,0] &nbsp; 3 &nbsp; &nbsp; &nbsp; 0'0 &nbsp; &nbsp; 2014-12-15 16:09:41.252854 &nbsp; &nbsp; &nbsp;0'0 &nbsp; &nbsp; 2014-12-09 16:05:52.198286</font></div></div><div><font size="2"   >............</font></div><p></p></pre></div><div>第一列是pollnum+pg-ID的信息.</div><div>例如0.2d 表示 pollnum=0 , pg-id=2d</div><div><br></div><div>因为ceph pg dump输出的pg包含了poolid, 那么如何查看pool的信息呢? 如下 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@localhost rbd0]# ceph osd lspools</font></div><div><font size="2"   >0 rbd,1 pool1,</font></div><p></p></pre></div><div><span style="line-height: 28px;"   >因此</span><span style="line-height: 28px;"   >0.2d这个 placement group是放在0号POOL的, 即rbd.</span></div><div><br></div><div>查看单个pg的信息如下, 输出JSON</div><pre class="prettyprint"   ><p></p><div><font size="2"   >ceph pg {poolnum}.{pg-id} query</font></div><div></div><p></p></pre><div><span style="line-height: 28px;"   >例如 :&nbsp;</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph pg 0.2d query</font></div><div><font size="2"   >{ "state": "active+clean",</font></div><div><font size="2"   >&nbsp; "snap_trimq": "[]",</font></div><div><font size="2"   >&nbsp; "epoch": 29,</font></div><div><font size="2"   >&nbsp; "up": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; "acting": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; "actingbackfill": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; "1",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; "3"],</font></div><div><font size="2"   >&nbsp; "info": { "pgid": "0.2d",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "last_update": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "last_complete": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "log_tail": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "last_user_version": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "last_backfill": "MAX",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "purged_snaps": "[]",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "history": { "epoch_created": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_started": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_clean": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_split": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_up_since": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_interval_since": 26,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_primary_since": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub_stamp": "2014-12-15 16:09:51.973484",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub_stamp": "2014-12-09 16:05:52.202755",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean_scrub_stamp": "2014-12-15 16:09:51.973484"},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "stats": { "version": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "reported_seq": "35",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "reported_epoch": "29",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "state": "active+clean",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_fresh": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_change": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_active": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_became_active": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_unstale": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_undegraded": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_fullsized": "2014-12-15 16:09:51.973547",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "mapping_epoch": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "log_start": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "ondisk_log_start": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "created": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_clean": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "parent": "0.0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "parent_split_bits": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub_stamp": "2014-12-15 16:09:51.973484",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub_stamp": "2014-12-09 16:05:52.202755",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean_scrub_stamp": "2014-12-15 16:09:51.973484",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "log_size": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "ondisk_log_size": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stats_invalid": "0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stat_sum": { "num_bytes": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_object_clones": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_object_copies": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_missing_on_primary": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_degraded": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_misplaced": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_unfound": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_dirty": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_whiteouts": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_read": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_read_kb": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_write": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_write_kb": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_shallow_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_deep_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_bytes_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_keys_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_omap": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_hit_set_archive": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_bytes_hit_set_archive": 0},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stat_cat_sum": {},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "up": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "acting": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "blocked_by": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "up_primary": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "acting_primary": 1},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "empty": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "dne": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "incomplete": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "last_epoch_started": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; "hit_set_history": { "current_last_update": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "current_last_stamp": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "current_info": { "begin": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "end": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "version": "0'0"},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "history": []}},</font></div><div><font size="2"   >&nbsp; "peer_info": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; { "peer": "3",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "pgid": "0.2d",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_update": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_complete": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "log_tail": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_user_version": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_backfill": "MAX",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "purged_snaps": "[]",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "history": { "epoch_created": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_started": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_clean": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_split": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_up_since": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_interval_since": 26,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "same_primary_since": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub_stamp": "2014-12-15 16:09:51.973484",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub_stamp": "2014-12-09 16:05:52.202755",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean_scrub_stamp": "2014-12-15 16:09:51.973484"},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stats": { "version": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "reported_seq": "18",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "reported_epoch": "21",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "state": "peering",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_fresh": "2014-12-09 16:07:23.524448",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_change": "2014-12-09 16:07:22.474261",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_active": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_became_active": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_unstale": "2014-12-09 16:07:23.524448",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_undegraded": "2014-12-09 16:07:23.524448",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_fullsized": "2014-12-09 16:07:23.524448",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "mapping_epoch": 22,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "log_start": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "ondisk_log_start": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "created": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_clean": 21,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "parent": "0.0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "parent_split_bits": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_scrub_stamp": "2014-12-09 16:05:52.202755",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_deep_scrub_stamp": "2014-12-09 16:05:52.202755",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_clean_scrub_stamp": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "log_size": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "ondisk_log_size": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stats_invalid": "1",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stat_sum": { "num_bytes": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_object_clones": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_object_copies": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_missing_on_primary": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_degraded": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_misplaced": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_unfound": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_dirty": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_whiteouts": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_read": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_read_kb": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_write": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_write_kb": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_shallow_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_deep_scrub_errors": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_bytes_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_keys_recovered": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_omap": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_objects_hit_set_archive": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "num_bytes_hit_set_archive": 0},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "stat_cat_sum": {},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "up": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "acting": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "blocked_by": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "up_primary": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "acting_primary": 1},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "empty": 1,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "dne": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "incomplete": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_epoch_started": 27,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "hit_set_history": { "current_last_update": "0'0",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "current_last_stamp": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "current_info": { "begin": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "end": "0.000000",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "version": "0'0"},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "history": []}}],</font></div><div><font size="2"   >&nbsp; "recovery_state": [</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; { "name": "Started\/Primary\/Active",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "enter_time": "2014-12-09 16:08:14.195101",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "might_have_unfound": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "recovery_progress": { "backfill_targets": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "waiting_on_backfill": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "last_backfill_started": "0\/\/0\/\/-1",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "backfill_info": { "begin": "0\/\/0\/\/-1",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "end": "0\/\/0\/\/-1",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "objects": []},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "peer_backfill_info": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "backfills_in_flight": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "recovering": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "pg_backend": { "pull_from_peer": [],</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "pushing": []}},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "scrub": { "scrubber.epoch_start": "26",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "scrubber.active": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "scrubber.block_writes": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "scrubber.waiting_on": 0,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "scrubber.waiting_on_whom": []}},</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; { "name": "Started",</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "enter_time": "2014-12-09 16:08:13.157757"}],</font></div><div><font size="2"   >&nbsp; "agent_state": {}}</font></div><p></p></pre></div><div><br></div><div>正常情况下 , 添加OSD后, 可以看到pg的状态变化如下 :&nbsp;</div><div><div><img title="ceph osd  placement groups stats introduce - 德哥@Digoal - PostgreSQL research"   alt="ceph osd  placement groups stats introduce - 德哥@Digoal - PostgreSQL research"   style="margin:0 10px 0 0;"   src="http://img0.ph.126.net/2neZab9xjcbPV7GVFOB1lw==/6619373360026106844.png"   ></div></div><div>其他情况可能出现其他状态.</div><div>PG所有可能的状态和介绍如下 :&nbsp;</div><div><br></div><div><div>1. PEERING</div><div><pre class="prettyprint"   ><p></p><div><span style="line-height: 28px;"   ><font size="2"   >正在建立对等关系. 确保所有存储PG副本的OSD加入.</font></span></div><div><font size="2"   >When Ceph is Peering a placement group, Ceph is bringing the OSDs that store the replicas of the placement group into agreement about the state of the objects and metadata in the placement group. When Ceph completes peering, this means that the OSDs that store the placement group agree about the current state of the placement group. However, completion of the peering process does NOT mean that each replica has the latest contents.&nbsp;</font></div><div><font size="2"   >注意这里提到, 即使完成了PEERING, 并不代表所有的副本都是最终状态的.&nbsp;</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Authoratative History</font></div><div><span style="line-height: 28px;"   ><font size="2"   >为了确保数据一致性, 用户在CEPH存储数据时, 只有所有的副本(注意是acting set中的osd, 例如进入degrade后, 可能不是所有的osd都参与写)都写结束了, 才返回用户写结束.</font></span></div><div><font size="2"   >Ceph will NOT acknowledge a write operation to a client, until all OSDs of the acting set persist the write operation. This practice ensures that at least one member of the acting set will have a record of every acknowledged write operation since the last successful peering operation.</font></div><div><font size="2"   >With an accurate record of each acknowledged write operation, Ceph can construct and disseminate a new authoritative history of the placement groupCa complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.</font></div><p></p></pre></div><div><br></div><div>2. ACTIVE</div><div>正常状态.</div><div><pre class="prettyprint"   ><p><font size="2"   >Once Ceph completes the peering process, a placement group may become active. The active state means that the data in the placement group is generally available in the primary placement group and the replicas for read and write operations.</font></p></pre></div><div><br></div><div>3. CLEAN</div><div>所有副本一致的状态.</div><div><pre class="prettyprint"   ><p><font size="2"   >When a placement group is in the clean state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the placement group. Ceph replicated all objects in the placement group the correct number of times.</font></p></pre></div><div><br></div><div>4. DEGRADED</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >这里的描述表明, 当用户往CEPH写数据时, 有副本的情况下, 主osd 负责将数据复制到副本OSD.</font></div><div><font size="2"   >在所有副本写入完成前, PG将保持degraded状态, 结束后进入ACTIVE状态</font></div><div><font size="2"   >When a client writes an object to the primary OSD, the primary OSD is responsible for writing the replicas to the replica OSDs. After the primary OSD writes the object to storage, the placement group will remain in a degraded state until the primary OSD has received an acknowledgement from the replica OSDs that Ceph created the replica objects successfully.</font></div><div><font size="2"   >如果OSDdown, ceph将标记这个OSD上所有的PG为degraded状态.</font></div><div><font size="2"   >这个osd再次UP的时候, 将重新peer.</font></div><div><font size="2"   >只要PG是active的, 新对象还是允许写入PG的.</font></div><div><font size="2"   >The reason a placement group can be active+degraded is that an OSD may be active even though it doesn’t hold all of the objects yet. If an OSD goes down, Ceph marks each placement group assigned to the OSD as degraded. The OSDs must peer again when the OSD comes back online. However, a client can still write a new object to a degraded placement group if it is active.</font></div><div><font size="2"   >如果OSD持续DOWN状态超过默认的300秒, 那么CEPH将触发remap, 这个OSD的PG副本将重分布.</font></div><div><font size="2"   >If an OSD is down and the degraded condition persists, Ceph may mark the down OSD as out of the cluster and remap the data from the down OSD to another OSD. The time between being marked down and being marked out is controlled by mon osd down out interval, which is set to 300 seconds by default.</font></div><div><font size="2"   >当PG进入DEGRADED后,PG可以继续提供服务, 但是某些用户数据可能不可用.</font></div><div><font size="2"   >A placement group can also be degraded, because Ceph cannot find one or more objects that Ceph thinks should be in the placement group. While you cannot read or write to unfound objects, you can still access all of the other objects in the degraded placement group.</font></div><p></p></pre></div><div><br></div><div>5. RECOVERING</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >当OSD进入DOWN状态后, 这个OSD的PG副本将落后于其他OSD的同样的PG副本, 所以UP后, 需要RECOVERY.</font></div><div><font size="2"   >Ceph was designed for fault-tolerance at a scale where hardware and software problems are ongoing. When an OSD goes down, its contents may fall behind the current state of other replicas in the placement groups. When the OSD is back up, the contents of the placement groups must be updated to reflect the current state. During that time period, the OSD may reflect a recovering state.</font></div><div><font size="2"   >如果是交换机, 或机柜出问题的话, 可能导致整个机柜或整个交换机下的所有主机上的OSD DOWN, 那么会有大量的OSD需要恢复.</font></div><div><font size="2"   >Recovery isn’t always trivial, because a hardware failure might cause a cascading failure of multiple OSDs. For example, a network switch for a rack or cabinet may fail, which can cause the OSDs of a number of host machines to fall behind the current state of the cluster. Each one of the OSDs must recover once the fault is resolved.</font></div><div><font size="2"   >如果发生上述情况, CEPH提供了机制来均衡大量OSD恢复带来的资源冲突问题.</font></div><div><font size="2"   >Ceph provides a number of settings to balance the resource contention between new service requests and the need to recover data objects and restore the placement groups to the current state. The osd recovery delay start setting allows an OSD to restart, re-peer and even process some replay requests before starting the recovery process. The osd recovery threads setting limits the number of threads for the recovery process (1 thread by default). The osd recovery thread timeout sets a thread timeout, because multiple OSDs may fail, restart and re-peer at staggered rates. The osd recovery max active setting limits the number of recovery requests an OSD will entertain simultaneously to prevent the OSD from failing to serve . The osd recovery max chunk setting limits the size of the recovered data chunks to prevent network congestion.</font></div><p></p></pre></div><div><br></div><div>6. BACK FILLING</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >当新的OSD加入集群时, CRUSH将从已有的OSD重新指派PG组给这个新加入的OSD.</font></div><div><font size="2"   >When a new OSD joins the cluster, CRUSH will reassign placement groups from OSDs in the cluster to the newly added OSD. Forcing the new OSD to accept the reassigned placement groups immediately can put excessive load on the new OSD. Back filling the OSD with the placement groups allows this process to begin in the background. Once backfilling is complete, the new OSD will begin serving requests when it is ready.</font></div><div><font size="2"   ><span style="line-height: 28px;"   >backfill过程中可能出现的状态 : backfill_wait,&nbsp;</span><span style="line-height: 28px;"   >backfill,&nbsp;</span><span style="line-height: 28px;"   >backfill_too_full</span></font></div><div><font size="2"   >During the backfill operations, you may see one of several states: backfill_wait indicates that a backfill operation is pending, but isn’t underway yet; backfill indicates that a backfill operation is underway; and, backfill_too_full indicates that a backfill operation was requested, but couldn’t be completed due to insufficient storage capacity. When a placement group can’t be backfilled, it may be considered incomplete.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Ceph provides a number of settings to manage the load spike associated with reassigning placement groups to an OSD (especially a new OSD). By default, osd_max_backfills sets the maximum number of concurrent backfills to or from an OSD to 10. The osd backfill full ratio enables an OSD to refuse a backfill request if the OSD is approaching its full ratio (85%, by default). If an OSD refuses a backfill request, the osd backfill retry interval enables an OSD to retry the request (after 10 seconds, by default). OSDs can also set osd backfill scan min and osd backfill scan max to manage scan intervals (64 and 512, by default).</font></div><p></p></pre></div><div><br></div><div>7. REMAPPED</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >如果PG组所在的OSD集合改变, 将触发PG的迁移动作, 迁移到新的OSD集合.</font></div><div><font size="2"   >在迁移结束前, 将请求PG的老的OSD集合中的primary OSD进行读写, 直至迁移结束.</font></div><div><font size="2"   >When the Acting Set that services a placement group changes, the data migrates from the old acting set to the new acting set. It may take some time for a new primary OSD to service requests. So it may ask the old primary to continue to service requests until the placement group migration is complete. Once data migration completes, the mapping uses the primary OSD of the new acting set.</font></div><p></p></pre></div><div><br></div><div>8. STALE</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >如果pg所在的OSD的主OSD DOWN, 则PG进入STALE状态.</font></div><div><font size="2"   >While Ceph uses heartbeats to ensure that hosts and daemons are running, the ceph-osd daemons may also get into a stuck state where they aren’t reporting statistics in a timely manner (e.g., a temporary network fault). By default, OSD daemons report their placement group, up thru, boot and failure statistics every half second (i.e., 0.5), which is more frequent than the heartbeat thresholds. If the Primary OSD of a placement group’s acting set fails to report to the monitor or if other OSDs have reported the primary OSD down, the monitors will mark the placement group stale.</font></div><div><font size="2"   >当CEPH集群启动时, peering结束前, PG处于stale状态.</font></div><div><font size="2"   >When you start your cluster, it is common to see the stale state until the peering process completes. After your cluster has been running for awhile, seeing placement groups in the stale state indicates that the primary OSD for those placement groups is down or not reporting placement group statistics to the monitor.</font></div></div><div><font size="2"   >另一种情况是, 当Primary OSD没有定期向MON报告PG的统计信息时, PG也会进入STALE状态.</font></div><p></p></pre></div></div><div><br></div><div><br></div><div>PG其他状态(unclean, inactive, stale) :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@localhost rbd0]# ceph pg stat</font></div><div><font size="2"   >v27135: 256 pgs: 256 active+clean; 10307 MB data, 504 GB used, 1131 GB / 1636 GB avail</font></div></div><div><font size="2"   >其他状态,&nbsp;</font></div><div><div><font size="2"   >Unclean: Placement groups contain objects that are not replicated the desired number of times. They should be recovering.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Inactive: Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back up.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Stale: Placement groups are in an unknown state, because the OSDs that host them have not reported to the monitor cluster in a while (configured by mon osd report timeout).</font></div></div><p></p></pre></div><div><br></div><div>搜索状态不正常的pg.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph pg dump_stuck unclean</font></div><div><font size="2"   >ok</font></div><div><font size="2"   >[root@mon1 ~]# ceph pg dump_stuck inactive</font></div><div><font size="2"   >ok</font></div><div><font size="2"   >[root@mon1 ~]# ceph pg dump_stuck stale</font></div><div><font size="2"   >ok</font></div><p></p></pre></div><div><br></div>[参考]<wbr><div>1.&nbsp;<a target="_blank" rel="nofollow" href="http://ceph.com/docs/master/rados/operations/monitoring-osd-pg/"   >http://ceph.com/docs/master/rados/operations/monitoring-osd-pg/</a></div><div><br></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ceph osd  placement groups stats introduce - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>