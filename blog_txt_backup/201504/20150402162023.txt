PostgreSQL research

use kvm & zfs deploy postgresql block level incremental backup on 2 host's ha based on stream replication

2015-04-02 16:20:23   查看原文>>

基于流复制的HA, 采样如下方案.
https://github.com/digoal/PostgreSQL_HA_with_primary_standby_2vip
本文介绍一下, 在没有其他主机的情况下, 如何实施块级别的增量备份.
(假设主机为了性能, 未直接使用ZFS作为文件系统)

主要用到的是chroot, zfs. 当前, 现在btrfs 也支持snapshot, 所以随便你的选择了. 

实现方法也比较多, 我们可以用容器, 例如docker, 也可用虚拟机.

本文采用KVM虚拟机, 在虚拟机中使用ZFS文件系统, 在虚拟机中创建以宿主机数据库为primary的standby.
然后在zfs上做快照.

那么问题来了, 宿主机的归档如何传递给虚拟机呢, 我这里虚拟机和宿主机用的不同网段, 虚拟机出外网使用NAT转发通讯.

# iptables -L -v -n -t nat
Chain PREROUTING (policy ACCEPT 22449 packets, 1310K bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain POSTROUTING (policy ACCEPT 25464 packets, 1325K bytes)
 pkts bytes target     prot opt in     out     source               destination         
  116  6960 MASQUERADE  tcp  --  *      *       192.168.122.0/24    !192.168.122.0/24    masq ports: 1024-65535 
  337 23214 MASQUERADE  udp  --  *      *       192.168.122.0/24    !192.168.122.0/24    masq ports: 1024-65535 
    0     0 MASQUERADE  all  --  *      *       192.168.122.0/24    !192.168.122.0/24    

Chain OUTPUT (policy ACCEPT 25464 packets, 1325K bytes)
 pkts bytes target     prot opt in     out     source               destination 


这种通讯方式, 要直接挂载远端的NFS是有问题的, 所以需要用其他方式来获取远端归档.

我这里选择的方法是scp, 将虚拟机的postgres 用户下的key拷贝到2台宿主机, 使用scp来拷贝归档.
假设宿主机的物理IP分别为172.16.18.25,172.16.18.26. 

restore_command = 'ping -c 1 -W 1 172.16.18.25 && scp root@172.16.18.25:/data04/pgdata/arch/*/%f %p; ping -c 1 -W 1 172.16.18.26 && scp root@172.16.18.26:/data04/pgdata/arch/*/%f %p'
primary_conninfo = 'host=192.168.122.1 port=1921 user=replica keepalives_idle=60'


好了, 有人要问, 为什么不直接使用primary VIP呢? 假设primary VIP是172.16.18.27.
因为VIP会漂移, 一旦漂移, 虚拟机的~/.ssh/known_hosts里面的条目就会失效, 需要重新编写, 比较麻烦.

最后要配置宿主机
自动启动虚拟机

# cat /etc/rc.local
# kvm
/usr/bin/virsh start centos6_6_x64



配置虚拟机自动启动数据库

# cat /etc/rc.local
su - postgres -c "pg_ctl start"



配置虚拟机自动快照

# crontab -l
8 * * * * /usr/sbin/ntpdate asia.pool.ntp.org && /sbin/hwclock --systohc
1 4 * * * /root/script/zfs_snap.sh

# cat /root/script/zfs_snap.sh 
#!/bin/bash

DATE="`date +%Y%m%d`"
# 注意, 第一个快照的位置是$PGDATA所在的zfs
/sbin/zfs snapshot zp1/data02@$DATE
/sbin/zfs snapshot zp1/data01@$DATE
/sbin/zfs snapshot zp1/data03@$DATE
/sbin/zfs snapshot zp1/data04@$DATE



现在快照有了, 加上宿主机上的归档也在, 完全可以实现基于时间点的恢复 . 
最后, 强烈建议pg_xlog不要放在zfs文件系统中. 宁愿放在虚拟机的系统盘里面也不要放在ZFS里面. 

Flag Counter
