## 列存优化(shard,大小块,归整,块级索引,bitmap scan) - (大量数据)任意列搜索
      
### 作者      
digoal      
      
### 日期      
2017-06-14      
      
### 标签      
PostgreSQL , 列存储 , shard , 切片 , 大块 , 小块 , sort , 块级索引 , bitmap scan , 索引延迟 , 归整        
      
----      
      
## 背景 
数据分析系统，决策系统的数据量通常非常庞大，属性（列）非常多，可能涉及到任意列的组合条件查询，筛选结果、聚合结果、多维分析等。

这种场景如何优化能满足实时的响应需求呢？

PostgreSQL中有一些技术，可以满足此类场景。

1\. 内置bitmapAnd bitmapOr，使用任意字段的索引搜索时，可以快速跳过不满足条件的块，快速的得到组合结果。

实测10亿数据，31个字段任意搜索，约几百毫秒的响应时间。

案例如下：

[《多字段，任意组合条件查询(无需建模) - 毫秒级实时圈人 最佳实践》](../201706/20170607_02.md)  

还有这种方法，要求每个字段都建立索引，对数据写入会有性能影响（特别是与堆存储线性相关性很差的字段，索引分裂会比较严重，导致IO巨大）。

对于数据量非常庞大的场景，建议对表进行分区，可以建立多级分区。

例如UID为一级HASH分区，时间戳为二级范围分区。这样可以控制每个表的大小，通过时间分区我们可以将历史数据静态化，不影响新录入数据的索引。从而提升整体性能。

2\. PostgreSQL支持多种索引接口，针对不同的数据类型，数据的存储风格。在前面的案例中也有介绍。

因此并不是btree一路用到底，对于不同的类型有不同的搜索需求，比如范围类型，数组类型，全文检索类型，通常会根据包含、相交来进行查询。而对于空间数据，通常会根据相交，距离等进行查询。对于一维类型通常会根据=，或者范围进行查询。

这些数据类型都能找到合适的索引来加速扫描，结合bitmapAnd, bitmapOr，实现实时的任意字段搜索。

3\. bitmap 索引(greenplum)

对于选择性较差的列，例如1亿记录，只有1000个唯一值，这种情况下，使用bitmap index就可以达到非常好的效果。

案例如下

[《Greenplum 最佳实践 - 什么时候选择bitmap索引》](../201705/20170512_01.md)  

[《PostgreSQL (varbit, roaring bitmap) VS pilosa(bitmap库)》](../201706/20170612_01.md)  

4\. 列存储, cstore(orc)格式

https://orc.apache.org/

https://github.com/citusdata/cstore_fdw

如果单机已经做了很多优化，例如使用分区表，但是由于实时写入的数据量依旧很庞大，数据录入由于索引过多遇到瓶颈了，还有什么优化手段呢？

## 海量数据 - 实时任意字段查询，实时写入 矛盾优化
要做到实时的海量写入，又要实时的查询，确实是一个比较矛盾的问题。

因为要实时写入，就要尽量少的索引，而要实时查询，就要索引的支持。

那么到底怎么解决这个矛盾问题呢？

1、索引延迟合并

在PostgreSQL中有一个GIN索引，这个索引是针对多值类型的索引，例如数组。

当用户在写入一条记录时，（一个数组往往涉及多个元素），索引的条目可能会设置到多个，因此每写一条记录，IO是巨大的，因此GIN有一个加速数据写入、更新、删除的特性，fastupdate。

这个特性也比较好理解，实际上就是延迟合并，比如累计1000条记录后，合并一次到索引的条目中。从而大幅提升写入性能。

那么我们也可以使用类似的技术来优化以上场景，只是需要对所有的索引接口都做类似的fastupdate功能。

2、lambda批量操作

我们可以将数据分为两个部分，一个部分是实时写入部分，另一部分是数据的合并部分。

数据实时的写入部分（不需要索引），实时写入后，再批量对其建立索引。

也就是先写入，到达分区边界后，写入新的分区，而前一个分区则开始建立索引。PG支持对一张表并行的创建多个索引。

例如

```
每个小时一个分区。

12:00-13:00，数据写入12:00到13:00的分区，在12:00时，可以对11:00-12:00的分区建立索引。

因此实时的数据都是写入没有索引的表，写入吞吐可以做到500万行/s左右。

批量创建索引的速度是非常快的，通常可以跟上节奏。
```

另外有两个类似的场景，有兴趣的话也可以参考一下，与本例的用法不一样。

[《块级(ctid)扫描在IoT(物联网)极限写和消费读并存场景的应用》](../201706/20170607_01.md)  

[《海量数据 "写入、共享、存储、计算" 最佳实践》](../201705/20170509_02.md)  

3、分布式

当单机的容量、写入性能、查询性能可能成为瓶颈时，可以创建多个实例。

多个实例有许多方法可以融合起来。

例如

3\.1 postgres_fdw + inherit

https://www.postgresql.org/docs/9.6/static/tutorial-inheritance.html

https://www.postgresql.org/docs/10/static/postgres-fdw.html

[《PostgreSQL 9.6 单元化,sharding (based on postgres_fdw) - 内核层支持前传》](../201610/20161004_01.md)  

[《PostgreSQL 9.6 sharding + 单元化 (based on postgres_fdw) 最佳实践 - 通用水平分库场景设计与实践》](../201610/20161005_01.md)  

3\.2 postgres_fdw + pg_pathman

https://github.com/postgrespro/pg_pathman

https://www.postgresql.org/docs/10/static/postgres-fdw.html

[《PostgreSQL 9.5+ 高效分区表实现 - pg_pathman》](../201610/20161024_01.md)  

[《PostgreSQL 9.6 sharding based on FDW & pg_pathman》](../201610/20161027_01.md)  

3\.3 plproxy

https://plproxy.github.io/

https://github.com/plproxy/plproxy

[《PostgreSQL 最佳实践 - 水平分库(基于plproxy)》](../201608/20160824_02.md)  

[《阿里云ApsaraDB RDS for PostgreSQL 最佳实践 - 2 教你RDS PG的水平分库》](../201512/20151220_02.md)  

[《阿里云ApsaraDB RDS for PostgreSQL 最佳实践 - 3 水平分库 vs 单机 性能》](../201512/20151220_03.md)  

[《阿里云ApsaraDB RDS for PostgreSQL 最佳实践 - 4 水平分库 之 节点扩展》](../201512/20151220_04.md)  

3\.4 citusdata extension

https://github.com/citusdata/citus

4、干掉索引，列存储优化

列存储除了可以降低单列统计，少量列统计的IO扫描成本，同时还可以提高压缩比。

列存储的方法是本文的重点，实际上可以不需要索引，使用列存，对列存的存储进行优化和规则，使得访问数据就如有索引一样高效。

列存的优化手段是编排，可以参考如下文章。

[《从一维编排到多维编排，从平面存储到3D存储 - 数据存储优化之路》](../201706/20170614_01.md)  

## 列存储编排优化
单列编排比较好实现，多列编排则需要建立列和列之间的映射关系。例如使用一个中间VALUE（PK）进行关联，这样会增加一个成本。

例如

```
PK
pk_val1
pk_val2
pk_val3

column a
a_val1, pk?
a_val2, pk?
a_val3, pk?

column b
b_val1, pk?
b_val2, pk?
b_val3, pk?

column c
c_val1, pk?
c_val2, pk?
c_val3, pk?
```

bitmapAnd, bitmapOr结合

```
where column a ?
and column b ?
or column c ?
```

bitmap扫描方法如下，只是替换成PK。

```
Heap, one square = one page:  
+---------------------------------------------+  
|c____u_____X___u___X_________u___cXcc______u_|  
+---------------------------------------------+  
Rows marked c match customers pkey condition.  
Rows marked u match username condition.  
Rows marked X match both conditions.  
  
  
Bitmap scan from customers_pkey:  
+---------------------------------------------+  
|100000000001000000010000000000000111100000000| bitmap 1  
+---------------------------------------------+  
One bit per heap page, in the same order as the heap  
Bits 1 when condition matches, 0 if not  
  
Bitmap scan from ix_cust_username:  
+---------------------------------------------+  
|000001000001000100010000000001000010000000010| bitmap 2  
+---------------------------------------------+  
Once the bitmaps are created a bitwise AND is performed on them:  
  
+---------------------------------------------+  
|100000000001000000010000000000000111100000000| bitmap 1  
|000001000001000100010000000001000010000000010| bitmap 2  
 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&  
|000000000001000000010000000000000010000000000| Combined bitmap  
+-----------+-------+--------------+----------+  
            |       |              |  
            v       v              v  
Used to scan the heap only for matching pages:  
+---------------------------------------------+  
|___________X_______X______________X__________|  
+---------------------------------------------+  
The bitmap heap scan then seeks to the start of each page and reads the page:  
  
+---------------------------------------------+  
|___________X_______X______________X__________|  
+---------------------------------------------+  
seek------->^seek-->^seek--------->^  
            |       |              |  
            ------------------------  
            only these pages read  
and each read page is then re-checked against the condition since there can be >1 row per page and not all necessarily match the condition.  
```

编排本身和索引类似，也会涉及到块的调整，因此也会导致写延时，为了







列存储带来的好处，压缩。

行列混合编排，

返回较多行的记录时，可以考虑。




shard  
  
  
  
列存编排优化  

  
数据的访问是多样化的，怎么优化？  
  
bitmap  
  
shard  

当需要返回很多行时。


与 shard 异曲同工

## 
